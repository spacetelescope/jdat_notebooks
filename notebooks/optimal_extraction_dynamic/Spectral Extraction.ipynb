{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NIRSpec MOS Optimal Spectral Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Use case:** optimal spectral extraction; method by [Horne (1986)](https://ui.adsabs.harvard.edu/abs/1986PASP...98..609H/abstract).<br>\n",
    "**Data:** JWST simulated NIRSpec MOS data; point sources.<br>\n",
    "**Tools:**  jwst, webbpsf, matplotlib, scipy, custom functions.<br>\n",
    "**Cross-intrument:** any spectrograph. <br>\n",
    "**Documentation:** This notebook is part of a STScI's larger [post-pipeline Data Analysis Tools Ecosystem](https://jwst-docs.stsci.edu/jwst-post-pipeline-data-analysis).<br>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The JWST pipeline produces 1-D and 2-D rectified spectra from combined exposures for each spectroscopic mode. Currently, the 1D products are produced using aperture extraction, with plans to implement optimal extraction via PSF-weighting or fitting. However, there are many situations in which the output will not necessarily be \"optimal\", and fine-tuning the parameters will be needed to improve the results. This notebook is intended to provide a walkthrough of the optimal extraction procedure with example JWST data.\n",
    "\n",
    "### Defining terms\n",
    "__Optimal extraction:__ a method of aperture extraction first defined in [Horne (1986)](https://ui.adsabs.harvard.edu/abs/1986PASP...98..609H/).<br>\n",
    "__S/N:__ Signal-to-noise ratio, a measure of how noisy a spectrum is.<br>\n",
    "__WCS:__ World Coordinate System, used for converting between different reference frames.<br>\n",
    "\n",
    "## Imports\n",
    "We will be using the following libraries to perform optimal spectral extraction.\n",
    "- `glob glob` for collecting filenames\n",
    "- `numpy` to handle array functions, as well as other various and sundry activities\n",
    "- `jwst.datamodels ImageModel, MultiSpecModel` for accessing the datamodels for our example data\n",
    "- `astropy.io fits` for low-level FITS file I/O\n",
    "- `astropy.modeling models, fitting` for the many fitting tasks\n",
    "- `astropy.visualization astropy_mpl_style, simple_norm` for displaying nice images\n",
    "- `scipy.interpolate interp1d, RegularGridInterpolator` for all our interpolation needs\n",
    "- `matplotlib.pyplot` for plotting data\n",
    "- `matplotlib.patches Rectangle` for plotting rectangles on our data\n",
    "- `ipywidgets` to create interactive widgets for adjusting fit parameters\n",
    "- `webbpsf NIRSpec` to generate and visualize a PSF from the instrument model (see Appendix B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from jwst.datamodels import ImageModel, MultiSpecModel\n",
    "from astropy.io import fits\n",
    "from astropy.modeling import models, fitting\n",
    "from astropy.visualization import astropy_mpl_style, simple_norm\n",
    "from scipy.interpolate import interp1d, RegularGridInterpolator\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "plt.style.use(astropy_mpl_style) #use the style we imported for matplotlib displays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading data\n",
    "We will be using simulated level 3 MOS data provided by James Muzerolle. These files come from a simulated visit with many point sources, and we will begin with the products of the `resample` step, which have the file extension `s2d.fits`. We will also compare the results of our optimal extraction with the products of the `extract1d` step, with the `x1d.fits` extension. See [the science data products specification](https://jwst-pipeline.readthedocs.io/en/stable/jwst/data_products/product_types.html#stage-3-data-products) and links therein for details on structure and format of these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal extraction procedure laid out below can be repeated for each `'SCI'` extension in each `s2d` file. For the purposes of this notebook, we will assume that the `resample` step has produced optimal output, so those are the only extensions we need to access. (Rectifying and combining the input spectra is a complicated process on its own, and is far beyond the scope of this notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# If the example dataset has already been downloaded, comment out these lines:\n",
    "import zipfile\n",
    "import urllib.request\n",
    "boxlink = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/optimal_extraction/optimal_extraction.zip'\n",
    "boxfile = './optimal_extraction.zip'\n",
    "urllib.request.urlretrieve(boxlink, boxfile)\n",
    "zf = zipfile.ZipFile(boxfile, 'r')\n",
    "zf.extractall()\n",
    "# ...to here\n",
    "\n",
    "example_file = 'F170LP-G235M_MOS_observation-6_mod_correctedWCS_noflat_nooutlierdet_combined_s30263_'\n",
    "s2d_file = os.path.join('s2d_files', example_file+'s2d.fits')\n",
    "x1d_file = os.path.join('x1d_files', example_file+'x1d.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_model = ImageModel(s2d_file)\n",
    "resampled_2d_image = data_model.data # if multiple SCI extensions, also specify EXTVER\n",
    "weights_2d_image = data_model.wht # we will use this to estimate the per-pixel variance later\n",
    "\n",
    "image_shape = resampled_2d_image.shape\n",
    "print(image_shape) #note the swap of x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to view 2d spectra, we'll generally need to stretch the pixels vertically to get a useful image. We can do this by setting the plot aspect ratio explicitly (we'll try to retain a measure of rectangularity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = simple_norm(resampled_2d_image, stretch='power')\n",
    "aspect_ratio = image_shape[1] / (2 * image_shape[0])\n",
    "fig1 = plt.figure() # we save these in dummy variables to avoid spurious Jupyter Notebook output\n",
    "img1 = plt.imshow(resampled_2d_image, cmap='gray', aspect=aspect_ratio, \n",
    "                  norm=norm, interpolation='none')\n",
    "clb1 = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Extraction algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an outline of the steps we'll be following:\n",
    "1. [Define an extraction region on the 2D image](#Define-an-extraction-region)\n",
    "1. [Identify a high S/N cross-dispersion (binned & coadded) slice to use for the initial kernel fit](#Create-kernel-slice)\n",
    "3. [Define the extraction kernel](#Define-the-extraction-kernel)\n",
    "    1. Single or composite PSF\n",
    "    1. Polynomial fit to background\n",
    "4. [Fit extraction kernel to initial slice](#Fit-extraction-kernel)\n",
    "5. ***Skipped:*** [*Fit geometric distortion*](#Fit-geometric-distortion-(skipped))\n",
    "    1. *Determine cross-dispersion bins for trace fitting*\n",
    "    1. *First-pass fit of kernel to each bin to find trace center*\n",
    "    1. *Polynomial fit of trace centers*\n",
    "6. [Combine composite model (kernel | trace) with 2D image to create output 1D spectrum](#Construct-final-1D-spectrum)\n",
    "7. Compare output spectrum with catalog photometry for flux calibration (not sure how to do this yet)\n",
    "\n",
    "Appendices:\n",
    "- [Appendix A: Batch Processing](#Appendix-A:-Batch-Processing)\n",
    "- [Appendix B: WebbPSF](#Appendix-B:-WebbPSF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Developer Note:*\n",
    "\n",
    "This sort of functionality is desired by many, and as of yet, no general-purpose optimal extraction Python packages exist. While this notebook can provide optimal extraction for 2D resampled JWST pipeline products, and could be adapted for use with other data, it is a far cry from a widely-applicable, maintained and updated spectral extraction codebase. It would be very nice if such a thing existed...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define an extraction region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by identifying the region in the 2D resampled image which contains the spectral trace we want to extract. For a simple case with only a single source, we can theoretically use the entire image. However, we may still want to exclude large systematic fluctuations in the background which might complicate the fit, or part of the trace with essentially no signal which will make fitting the trace centers difficult. In addition, when working with background nod-subtracted data, the images will contain negative traces, which we will want to exclude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attempt to do this interactively, using sliders to define the bounding box. \n",
    "\n",
    "(Note that sliders with large ranges will jump more than one value at a time; for finer control, select a slider with the cursor and then use the up and down arrow keys to increment or decrement by one pixel.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(9,9)) # we want the largest figure that will fit in the notebook\n",
    "img2 = plt.imshow(resampled_2d_image, cmap='gray', aspect=aspect_ratio, \n",
    "                  norm=norm, interpolation='none') # reuse norm from earlier\n",
    "\n",
    "# create region box and slider\n",
    "region_x = region_y = 0\n",
    "region_h, region_w = image_shape\n",
    "region_rectangle = Rectangle((region_x, region_y), region_w, region_h, \n",
    "                             facecolor='none', edgecolor='b', linestyle='--')\n",
    "current_axis = plt.gca()\n",
    "current_axis.add_patch(region_rectangle)\n",
    "\n",
    "# interactive widget controls\n",
    "def region(x1=0, y1=0, x2=region_w-1, y2=region_h-1):\n",
    "    region_rectangle.set_bounds(x1, y1, x2-x1, y2-y1)\n",
    "    plt.draw()\n",
    "    \n",
    "interact1 = interact(region, x1=(0, region_w-2, 1), y1=(0, region_h-2, 1), \n",
    "                    x2=(1, region_w-1, 1), y2=(1, region_h-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the region coordinates from the bounding rectangle -- in this case, setting the coordinates to `x1=51, y1=3, x2=1268, y2=9` seems fine -- or, we can set them directly. Finally, we create a new array containing only our extraction region (so that we don't need to continually index our original array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment these lines out if interativity is not desired\n",
    "x, y = region_rectangle.xy\n",
    "w = region_rectangle.get_width() \n",
    "h = region_rectangle.get_height()\n",
    "\n",
    "#uncomment and set these to your desired extraction region if interativity is not desired\n",
    "# x = y = 0\n",
    "# h, w = image_shape\n",
    "\n",
    "print(x, y, x+w, y+h)\n",
    "\n",
    "er_y, er_x = np.mgrid[y:y+h, x:x+w]\n",
    "extraction_region = resampled_2d_image[er_y, er_x]\n",
    "weights_region = weights_2d_image[er_y, er_x]\n",
    "er_ny, er_nx = extraction_region.shape\n",
    "\n",
    "aspect_ratio = er_nx / (3. * er_ny)\n",
    "\n",
    "er_norm = simple_norm(extraction_region, stretch='power')\n",
    "fig3 = plt.figure()\n",
    "img3 = plt.imshow(extraction_region, cmap='gray', aspect=aspect_ratio, \n",
    "                  norm=er_norm, interpolation='none')\n",
    "clb3 = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(To adjust the region at this point, re-run *both* of the previous cells - the sliders need to be reset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create kernel slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a cross-dispersion slice of our extraction region with which to fit our initial extraction kernel. As an initial guess, we'll coadd the 30 columns centered on the middle of the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_width = 30\n",
    "initial_column = er_nx // 2\n",
    "\n",
    "def kernel_slice_coadd(width, column_idx):\n",
    "    \"\"\"\n",
    "    Coadd a number of columns (= width) of the extraction region,\n",
    "    centered on column_idx.\n",
    "    \"\"\"\n",
    "    \n",
    "    half_width = width // 2\n",
    "    to_coadd = np.arange(max(0, column_idx - half_width), \n",
    "                         min(er_nx-1, column_idx + half_width))\n",
    "    return extraction_region[:, to_coadd].sum(axis=1) / width\n",
    "\n",
    "slice_0 = kernel_slice_coadd(slice_width, initial_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll plot the resulting slice, and (interactively) adjust the width and center of the coadd region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, (iax4, pax4) = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))\n",
    "plt.subplots_adjust(hspace=0.15, top=0.95, bottom=0.05)\n",
    "img4 = iax4.imshow(extraction_region, cmap='gray', aspect=aspect_ratio, \n",
    "                  norm=er_norm, interpolation='none')\n",
    "\n",
    "#create slice box\n",
    "def make_slice(width, column_idx):\n",
    "    sy, sh, sw = 0, er_ny, width\n",
    "    sx = column_idx - width // 2\n",
    "    return sx, sy, sw, sh\n",
    "\n",
    "*sxy, sw, sh = make_slice(slice_width, initial_column)\n",
    "slice_rectangle = Rectangle(sxy, sw, sh, facecolor='none', \n",
    "                            edgecolor='b', linestyle='--')\n",
    "iax4.add_patch(slice_rectangle)\n",
    "\n",
    "#plot the coadded slice\n",
    "xd_pixels = np.arange(er_ny)\n",
    "lin4, = pax4.plot(xd_pixels, slice_0, 'k-')\n",
    "pax4.set_xlabel('Cross-dispersion pixel')\n",
    "pax4.axes.set_ylabel('Coadded signal')\n",
    "\n",
    "column_slider = widgets.IntSlider(initial_column, 0, er_nx-1, 1)\n",
    "width_slider = widgets.IntSlider(slice_width, 1, er_nx-1, 1)\n",
    "\n",
    "#interactive controls\n",
    "def slice_update(column_idx, width):\n",
    "    #update rectangle\n",
    "    new_slice_box = make_slice(width, column_idx)\n",
    "    slice_rectangle.set_bounds(*new_slice_box)\n",
    "    #update line plot\n",
    "    lin4.set_ydata(kernel_slice_coadd(width, column_idx))\n",
    "    #update the axis limits\n",
    "    pax4.relim()\n",
    "    pax4.autoscale_view()\n",
    "    plt.draw()\n",
    "\n",
    "interact2 = interact(slice_update, column_idx=column_slider, width=width_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column index of 670 and width 50 seem to work reasonably well for this file, so we can now generate the final slice for kernel fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_slice = kernel_slice_coadd(width_slider.value, column_slider.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the extraction kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define an extraction kernel which will be used to fit our trace at each pixel in the dispersion direction. This kernel will be made of 2 parts:\n",
    "- a PSF template (or a composite of multiple PSFs, for deblending purposes)\n",
    "- a polynomial for background fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a PSF template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many options for PSF template that we could consider for our kernel, but a full comparison is outside the scope of this notebook. We will be demonstrating only Gaussian and Moffat profiles.\n",
    "\n",
    "There are two things to note:\n",
    "1. The methods shown here are only applicable to a true point source. Extended sources require a different methodology.\n",
    "2. The `WebbPSF` package can be used to directly construct a composite PSF from the instrument model; however, this process is far more arduous than fitting a 1D profile using the `astropy.modeling` tools, and has thus been banished to Appendix B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by plotting the two profiles against the kernel slice, with a naive normalization so that we can ignore scaling for the time being, centered on the pixel with the kernel's maximum value. (We will perform a true fit later, don't worry!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pixel = np.argmax(kernel_slice)\n",
    "fwhm = 1.\n",
    "\n",
    "moffat_profile = models.Moffat1D(amplitude=1, gamma=fwhm, x_0=max_pixel, alpha=1)\n",
    "gauss_profile = models.Gaussian1D(amplitude=1, mean=max_pixel, stddev=fwhm)\n",
    "\n",
    "fig5 = plt.figure()\n",
    "kern5 = plt.plot(xd_pixels, kernel_slice / kernel_slice[max_pixel], label='Kernel Slice')\n",
    "moff5 = plt.plot(xd_pixels, moffat_profile(xd_pixels), label='Moffat Profile')\n",
    "gaus5 = plt.plot(xd_pixels, gauss_profile(xd_pixels), label='Gaussian Profile')\n",
    "lgd5 = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian profile looks like a better approximation, so that's the profile we'll use for this spectrum. In the cell below, we could add more PSF templates using [model operations](https://docs.astropy.org/en/stable/modeling/compound-models.html); this is left as an exercise for the reader.\n",
    "\n",
    "We need to de-normalize our amplitude, so we'll set it to the maximum pixel value of the slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_template = gauss_profile\n",
    "psf_template.amplitude = kernel_slice[max_pixel]\n",
    "print(psf_template)\n",
    "# If deblending multiple sources, add more PSF templates here:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the background with a polynomial. Some experimentation is recommended to find the polynomial degree which best fits the data; for this example, we'll use a 2nd-degree polynomial.\n",
    "\n",
    "For nod-subtracted data, there may not be enough pixels in the extraction region to accurately fit a residual. In such cases, use a 0th-order polynomial or a `Const1D` model for the background; to avoid fitting the background at all, set the parameter to `fixed = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_poly = models.Polynomial1D(2)\n",
    "print(background_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to combine the PSF(s) and the background to create our compound model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_kernel = psf_template + background_poly\n",
    "print(extraction_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit extraction kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an extraction kernel, we want to fit it to our kernel slice, so as to have the best tool for fitting trace centers in the next step. We also plot the fit components, as well as the fit vs the kernel slice, as visual checks; if they are unacceptable, we can go back to the previous section, tweak parameters, and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = fitting.LevMarLSQFitter()\n",
    "fit_extraction_kernel = fitter(extraction_kernel, xd_pixels, kernel_slice)\n",
    "print(fit_extraction_kernel)\n",
    "\n",
    "fit_line = fit_extraction_kernel(xd_pixels)\n",
    "\n",
    "fig6, (fax6, fln6) = plt.subplots(nrows=2, ncols=1, figsize=(8, 12))\n",
    "plt.subplots_adjust(hspace=0.15, top=0.95, bottom=0.05)\n",
    "psf6 = fax6.plot(xd_pixels, fit_extraction_kernel[0](xd_pixels), label=\"PSF\")\n",
    "poly6 = fax6.plot(xd_pixels, fit_extraction_kernel[1](xd_pixels), label=\"Background\")\n",
    "sum6 = fax6.plot(xd_pixels, fit_line, label=\"Composite Kernel\")\n",
    "lgd6a = fax6.legend()\n",
    "lin6 = fln6.plot(xd_pixels, kernel_slice, label='Kernel Slice')\n",
    "fit6 = fln6.plot(xd_pixels, fit_line, 'o', label='Extraction Kernel')\n",
    "lgd6b = fln6.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelength-varying FWHM (skipped)\n",
    "\n",
    "The NIRSpec PSF width changes with wavelength, and so for science data, it may be beneficial to fit multiple locations along the spectral trace. Below is a demonstration of the process; note, however, for this example dataset, the (not-yet-optimized) resampling and combining of the dithered input spectra introduces a width variation artifact, so we will not actually be using the results of this step for the extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to account for a varying FWHM, we can bin the 2D spectrum in the dispersion direction and fit each bin. The kernel we defined above can act as our initial estimate, which can be helpful in very faint regions of the spectrum, since `astropy.modeling` fitting routines can be sensitive to initial estimates.\n",
    "\n",
    "(Once the binned kernel FWHMs have been calculated and plotted, the next step would be to find an appropriate model and fit the FWHM as a function of bin center. The fit model would then be included in the final 1D extraction below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.stats import sigma_clip\n",
    "\n",
    "n_bin = 100\n",
    "bin_width = er_nx // n_bin\n",
    "bin_centers = np.arange(0, er_nx, bin_width+1, dtype=float) + bin_width // 2\n",
    "binned_spectrum = np.hstack([extraction_region[:, i:i+bin_width+1].sum(axis=1)[:, None] \n",
    "                                 for i in range(0, er_nx, bin_width+1)])\n",
    "bin_fwhms = np.zeros_like(bin_centers, dtype=float)\n",
    "\n",
    "for y in range(bin_centers.size):\n",
    "    bin_fit = fitter(fit_extraction_kernel, xd_pixels, binned_spectrum[:, y])\n",
    "    bin_fwhms[y] = bin_fit.stddev_0.value\n",
    "    \n",
    "bin_ny, bin_nx = binned_spectrum.shape\n",
    "bin_ar = bin_nx / (3 * bin_ny)\n",
    "\n",
    "fig_fwhm, ax_fwhm = plt.subplots(nrows=2, ncols=1, figsize=(6, 10))\n",
    "plt.subplots_adjust(hspace=0.05)\n",
    "fwhm_img = ax_fwhm[0].imshow(binned_spectrum, aspect=bin_ar, interpolation='none',\n",
    "                             cmap='gray')\n",
    "fwhm_plot = ax_fwhm[1].plot(bin_centers, bin_fwhms)\n",
    "xlbl_fwhm = ax_fwhm[1].set_xlabel(\"Bin center (px)\")\n",
    "ylbl_fwhm = ax_fwhm[1].set_ylabel(\"FWHM (arcsec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit geometric distortion *(skipped)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline `resample` step drizzles all input 2d spectra onto a rectified grid, so this particular step of our optimal extraction process is not typically necessary. A brief discussion of the procedure is included here as a guideline for extracting unrectified spectra (with the suffix `_cal.fits`), where the trace can have significant curvature and the trace dispersion is not column-aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define bins for trace fitting\n",
    "\n",
    "Depending on how noisy the 2D resampled spectrum is, it may be beneficial to define bins in the dispersion direction. These can be evenly- or unevenly-spaced, and once they're defined, coadd the columns in each bin (possibly using the `WHT` extension in the `s2d` file) and create an array of bin center locations.\n",
    "\n",
    "If the 2D spectrum has high S/N, this may not be necessary, and each cross-dispersed column can be fit individually in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit each bin with a modified extraction kernel\n",
    "\n",
    "We want to fit each of the defined bins with our extraction kernel, but we don't want any other artifacts or noise to confuse the trace. So, we copy the extraction kernel, then set each parameter other than the profile center (`mean_0` in the example above) to `fixed = True`. Starting at one end of the trace, iterate over each bin to fit the slice with the extraction kernel, and store the resulting trace centers in an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the trace centers with a 1D polynomial\n",
    "\n",
    "This step is straightforward: create a `Polynomial1D` model, then fit it to the trace centers from the previous step.\n",
    "\n",
    "Since we won't be fitting, instead we'll create a placeholder trace center model: a 0th-order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_center_model = models.Polynomial1D(0) #we use a constant because the spectrum has already been rectified\n",
    "trace_center_model.c0 = fit_extraction_kernel.mean_0.value # use the parameter for center of the PSF profile\n",
    "print(trace_center_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct final 1D spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the final 1D spectrum as a weighted sum in the cross-dispersion direction the 2D spectrum, using our composite model (the extraction kernel centered on the trace) for the weights. We also need to incorporate the variance for each pixel, which we'll estimate from the `WHT` extension output by the resample step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a variance image\n",
    "\n",
    "Horne's algorithm requires the variance for each pixel. Errors are not currently propagated through the resample step; however, as per the [DrizzlePac Handbook](https://www.stsci.edu/files/live/sites/www/files/home/scientific-community/software/drizzlepac/_documents/drizzlepac-handbook.pdf), we can estimate the variance from the drizzle weights image: $ Var \\approx 1 / (W \\times s^4) $, where $s$ is the pixel scale. Currently, the NIRSpec drizzle parameters are set to `PIXFRAC = 1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1.0 # adjust this if and when the NIRSpec PIXFRAC changes\n",
    "\n",
    "# We want any pixel with 0 weight to be excluded from the calculation\n",
    "# in the next step, so we'll use masked array operations.\n",
    "bad_pixels = weights_region == 0\n",
    "masked_wht = np.ma.array(weights_region, mask=bad_pixels)\n",
    "variance_image = np.ma.divide(1., weights_region * scale**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the variance image to see if there are any regions of the extraction region which will not be included in the spectrum (indicated in red below). For this particular example spectrum, every pixel has a nonzero weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "fig_var = plt.figure()\n",
    "palette = copy(plt.cm.gray)\n",
    "palette.set_bad('r', alpha=0.7)\n",
    "var_norm = simple_norm(variance_image, stretch='log', min_cut=0.006, max_cut=0.1)\n",
    "img_var = plt.imshow(variance_image, interpolation='none', aspect=aspect_ratio, norm=var_norm, cmap=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the 1D spectrum\n",
    "\n",
    "Now, we finally calculate our 1D spectrum, summing over cross-dispersed columns:\n",
    "$$S_x = \\frac{1}{G_x}\\sum_{y} \\frac{I_{xy}\\cdot K_y(x)}{V_{xy}}$$\n",
    "where $I$ is the pixel value in the 2D resampled image, $K$ is our extraction kernel set to the column's trace center, $V$ is the pixel value in the variance image, and $G$ is the kernel normalization given by:\n",
    "$$G_x = \\sum_y \\frac{K_y^2(x)}{V_{xy}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = np.zeros(er_nx, dtype=float) #initialize our spectrum with zeros\n",
    "column_pixels = np.arange(er_nx)\n",
    "trace_centers = trace_center_model(column_pixels) # calculate our trace centers array\n",
    "\n",
    "# Loop over columns\n",
    "for x in column_pixels:\n",
    "    # create the kernel for this column, using the fit trace centers\n",
    "    kernel_column = fit_extraction_kernel.copy()\n",
    "    kernel_column.mean_0 = trace_centers[x]\n",
    "    # kernel_column.stddev_0 = fwhm_fit(x) # if accounting for a varying FWHM, uncomment this line.\n",
    "    kernel_values = kernel_column(xd_pixels)\n",
    "    \n",
    "    # isolate the relevant column in the spectrum and variance images\n",
    "    variance_column = variance_image[:, x] # remember that numpy arrays are row, column\n",
    "    image_pixels = extraction_region[:, x]\n",
    "    \n",
    "    # calculate the kernal normalization\n",
    "    g_x = np.ma.sum(kernel_values**2 / variance_column)\n",
    "    if np.ma.is_masked(g_x): #this column isn't valid, so we'll skip it\n",
    "        continue\n",
    "    \n",
    "    # and now sum the weighted column\n",
    "    weighted_column = np.ma.divide(image_pixels * kernel_values, variance_column)\n",
    "    spectrum[x] = np.ma.sum(weighted_column) / g_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a wavelength array to display the spectrum, which we can create from the WCS object stored in the data model's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcs = data_model.meta.wcs\n",
    "print(wcs.__repr__())\n",
    "alpha_C, delta_C, y = wcs(er_x, er_y)\n",
    "wavelength = y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7 = plt.figure()\n",
    "spec7 = plt.plot(wavelength, spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the extracted spectrum out to a file\n",
    "# This is left as an exercise for the reader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to compare our optimally-extracted spectrum with the `x1d` pipeline product. We'll normalize the spectra so we can plot them on the same axes.\n",
    "\n",
    "(Note that the `x1d` spectrum includes negative traces from the background subtraction step, which usually results in a negative flux calculation. We need to correct for that when comparing with our optimally-extracted version.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1d_model = MultiSpecModel(x1d_file)\n",
    "# For a file with multiple spectra, the index to .spec is EXTVAR-1\n",
    "x1d_wave = x1d_model.spec[0].spec_table.WAVELENGTH\n",
    "x1d_flux = x1d_model.spec[0].spec_table.FLUX \n",
    "if x1d_flux.sum() <= 0:\n",
    "    x1d_flux = -x1d_flux\n",
    "fig8 = plt.figure()\n",
    "x1d8 = plt.plot(x1d_wave, x1d_flux / x1d_flux.max(), label=\"Pipeline\")\n",
    "opt8 = plt.plot(wavelength, spectrum / spectrum.max(), label=\"Optimal\", alpha=0.7)\n",
    "lgd8 = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When optimal extraction is desired for a large number of spectra, going step-by-step through the process laid out above for each spectrum may not be practical. In such cases, we can initially use those interactive methods on one or two spectra to make decisions about some of our extraction parameters (e.g., what PSF template profile to use, or what degree polynonmial to fit the background with), then use those parameters to process all of the spectra non-interactively. Afterwards, we can examine the output from each extracted spectrum and revisit any which need more individualized handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a large number of spectra non-interactively by defining functions for each of the steps above, and a single master function to iterate over all the spectra in a single directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an extraction region\n",
    "\n",
    "There's no way to perform this step non-interactively, so we'll skip it here. However, there are two good ways (and one bad way) to deal with this for a real dataset:\n",
    "1. Define an extraction region for each 2D spectrum before batch processing. You can save the region bounding boxes to a python dictionary (or write them to a file, then read it in during iteration).\n",
    "1. Visually examine the 2D spectra, and only batch process those spectra for which a specific extraction region (i.e., smaller than the full 2D spectrum) doesn't need to be defined. The remainder of the spectra can be extracted individually.\n",
    "1. Skip this step, and assume that any spectra for which a specific extraction region would need to be defined will need individualized reprocessing anyway. This step is not recommended, but it is the one we will be using here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Kernel Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_kernel_slice(extraction_region, slice_width=30, column_idx=None):\n",
    "    \"\"\"\n",
    "    Create a slice in the cross-dispersion direction out of the \n",
    "    2D array `extraction_region`, centered on `column_idx` and \n",
    "    `slice_width` pixels wide. If `column_idx` is not given, use\n",
    "    the column with the largest total signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    if column_idx is None:\n",
    "        column_idx = np.argmax(extraction_region.sum(axis=0))\n",
    "        \n",
    "    ny, nx = extraction_region.shape\n",
    "    half_width = slice_width // 2\n",
    "    \n",
    "    #make sure we don't go past the edges of the extraction region\n",
    "    to_coadd = np.arange(max(0, column_idx - half_width), \n",
    "                         min(nx-1, column_idx + half_width))\n",
    "    \n",
    "    return extraction_region[:, to_coadd].sum(axis=1) / slice_width\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit the extraction kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_fit_extraction_kernel(xd_slice, psf_profile=models.Gaussian1D, \n",
    "                          height_param_name='amplitude', height_param_value=None,\n",
    "                          width_param_name='stddev', width_param_value=1.,\n",
    "                          center_param_name='mean', center_param_value=None,\n",
    "                          other_psf_args=[], other_psf_kw={},\n",
    "                          bg_model=models.Polynomial1D,\n",
    "                          bg_args=[3], bg_kw={}):\n",
    "    \"\"\"\n",
    "    Initialize a composite extraction kernel, then fit it to \n",
    "    the 1D array `xd_slice`, which has been nominally\n",
    "    generated via the `kernel_slice` function defined above. \n",
    "    \n",
    "    To allow for PSF template models with different parameter \n",
    "    names, we use the `height_param_*`, `width_param_*`, and\n",
    "    `center_param_*` keyword arguments. We collect any other\n",
    "    positional or keyword arguments for the PSF model in \n",
    "    `other_psf_*`. If the height or center values are `None`, \n",
    "    they will be calculated from the data.\n",
    "    \n",
    "    Similarly, any desired positional or keyword arguments to\n",
    "    the background fit model (default `Polynomial1D`) are\n",
    "    accepted via `bg_args` and `bg_kw`.\n",
    "    \n",
    "    Note that this function can not handle cases which involve\n",
    "    multiple PSFs for deblending. It is recommended to process\n",
    "    such spectra individually, using the interactive procedure\n",
    "    above.\n",
    "    \"\"\"\n",
    "    xd_pixels = np.arange(xd_slice.size)\n",
    "    \n",
    "    if center_param_value is None:\n",
    "        center_param_value = np.argmax(xd_slice)\n",
    "    \n",
    "    if height_param_value is None:\n",
    "        # In case of non-integer values passed via center_param_value,\n",
    "        # we need to interpolate.\n",
    "        slice_interp = interp1d(xd_pixels, xd_slice)\n",
    "        height_param_value = slice_interp(center_param_value)\n",
    "    \n",
    "    # Create the PSF and the background models\n",
    "    psf_kw = dict([(height_param_name, height_param_value), \n",
    "                   (width_param_name, width_param_value),\n",
    "                   (center_param_name, center_param_value)])\n",
    "    psf_kw.update(other_psf_kw)\n",
    "    psf = psf_profile(*other_psf_args, **psf_kw)\n",
    "    \n",
    "    bg = bg_model(*bg_args, **bg_kw)\n",
    "    \n",
    "    composite_kernel = psf + bg\n",
    "    fitter = fitting.LevMarLSQFitter()\n",
    "    return fitter(composite_kernel, xd_pixels, xd_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account for varying FWHM\n",
    "\n",
    "This is left as an exercise for the user, as per the process shown [here](#Wavelength-varying-FWHM). Note that `batch_extract_spectrum` and `batch_optimal_extraction` below will also need to be modified to incorporate this function, if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_vary_fwhm(extraction_region, kernel):\n",
    "    pass # implement a function which fits a wavelength-varying FWHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the trace centers\n",
    "\n",
    "If this is required, replace this with a real function that does the fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_fit_trace_centers(extraction_region, kernel,\n",
    "                      trace_model=models.Polynomial1D,\n",
    "                      trace_args=[0], trace_kw={}):\n",
    "    \"\"\"\n",
    "    Fit the geometric distortion of the trace with\n",
    "    a model. Currently this is a placeholder function,\n",
    "    since geometric distortion is typically removed\n",
    "    during the `resample` step. However, if this\n",
    "    functionality is necesary, use this function\n",
    "    signature to remain compatible with the rest of\n",
    "    this Appendix.\n",
    "    \"\"\"\n",
    "    \n",
    "    trace_centers = trace_model(*trace_args, **trace_kw)\n",
    "    trace_centers.c0 = kernel.mean_0\n",
    "    return trace_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the 1D spectrum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract_spectrum(extraction_region, trace, kernel, \n",
    "                     weights_image, \n",
    "                     trace_center_param='mean',\n",
    "                     scale=1.0):\n",
    "    \"\"\"\n",
    "    Optimally extract the 1D spectrum from the extraction \n",
    "    region.\n",
    "    \n",
    "    A variance image is created from `weights_image` (which \n",
    "    should have the same dimensions as `extraction_region`).\n",
    "    Then, for each column of the spectrum, we sum the aperture\n",
    "    as per the equations defined above, masking pixels with\n",
    "    zero weights. \n",
    "    \n",
    "    Note that unlike the interactive, step-by-step method, \n",
    "    here we will vectorize for speed. This requires using\n",
    "    a model set for the kernel, but this is allowed since\n",
    "    we are not fitting anything.\n",
    "    \n",
    "    `trace_center_param` is the name of the parameter which \n",
    "    will defines the trace centers, *without the model number\n",
    "    subscript* (since we will be dealing with the components\n",
    "    individually).\n",
    "    \n",
    "    `scale` is the size ratio of input to output pixels when\n",
    "    drizzling, equivalent to PIXFRAC in the drizzle parameters\n",
    "    from the `resample` step.\n",
    "    \"\"\"\n",
    "    \n",
    "    bad_pixels = weights_image == 0.\n",
    "    masked_wht = np.ma.array(weights_image, mask=bad_pixels)\n",
    "    variance_image = np.ma.divide(1., masked_wht * scale**4)\n",
    "    \n",
    "    ny, nx = extraction_region.shape\n",
    "    trace_pixels = np.arange(nx)\n",
    "    xd_pixels = np.arange(ny)\n",
    "    trace_centers = trace(trace_pixels) # calculate our trace centers array\n",
    "    \n",
    "    # Create kernel image for vectorizing, which requires some gymnastics...\n",
    "    # ******************************************************************\n",
    "    # * IMPORTANT:                                                     *\n",
    "    # * ----------                                                     *\n",
    "    # * Note that because of the way model sets are implemented, it is *\n",
    "    # * not feasible to alter an existing model instance to use them.  *\n",
    "    # * Instead we'll create a new kernel instance, using the fitted   *\n",
    "    # * parameters from the original kernel.                           *\n",
    "    # *                                                                *\n",
    "    # * Caveat: this assumes that the PSF is the first element, and    *\n",
    "    # * the background is the second. If you change that when creating *\n",
    "    # * your composite kernel, make sure you update this section       *\n",
    "    # * similarly, or it will not work!                                *\n",
    "    # ******************************************************************\n",
    "    psf0, bg0 = kernel\n",
    "    psf_params = {}\n",
    "    for pname, pvalue in zip(psf0.param_names, psf0.parameters):\n",
    "        if pname == trace_center_param:\n",
    "            psf_params[pname] = trace_centers\n",
    "        else:\n",
    "            psf_params[pname] = np.full(nx, pvalue)\n",
    "    psf_set = psf0.__class__(n_models=nx, **psf_params)\n",
    "    #if not using Polynomial1D for background model, edit this:\n",
    "    bg_set = bg0.__class__(len(bg0.param_names)-1, n_models=nx)\n",
    "    for pname, pvalue in zip(bg0.param_names, bg0.parameters):\n",
    "        setattr(bg_set, pname, np.full(nx, pvalue))\n",
    "    kernel_set = psf_set + bg_set\n",
    "    # We pass model_set_axis=False so that every model in the set \n",
    "    # uses the same input, and we transpose the result to fix the\n",
    "    # orientation.\n",
    "    kernel_image = kernel_set(xd_pixels, model_set_axis=False).T\n",
    "    \n",
    "    # Now we perform our weighted sum, using numpy.ma routines\n",
    "    # to preserve our masks\n",
    "    g = np.ma.sum(kernel_image**2 / variance_image, axis=0)\n",
    "    weighted_spectrum = np.ma.divide(kernel_image * extraction_region, variance_image)\n",
    "    spectrum1d = np.ma.sum(weighted_spectrum, axis=0) / g\n",
    "    \n",
    "    # Any masked values we set to 0.\n",
    "    return spectrum1d.filled(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_wavelength_from_wcs(datamodel, pix_x, pix_y):\n",
    "    \"\"\"\n",
    "    Convenience function to grab the WCS object from the\n",
    "    datamodel's metadata, generate world coordinates from\n",
    "    the given pixel coordinates, and return the 1D \n",
    "    wavelength.\n",
    "    \"\"\"\n",
    "    \n",
    "    wcs = datamodel.meta.wcs\n",
    "    aC, dC, y = wcs(pix_x, pix_y)\n",
    "    return y[0]\n",
    "\n",
    "def batch_save_extracted_spectrum(filename, wavelength, spectrum):\n",
    "    \"\"\"\n",
    "    Quick & dirty fits dump of an extracted spectrum.\n",
    "    Replace with your preferred output format & function.\n",
    "    \"\"\"\n",
    "    \n",
    "    wcol = fits.Column(name='wavelength', format='E', \n",
    "                       array=wavelength)\n",
    "    scol = fits.Column(name='spectrum', format='E',\n",
    "                       array=spectrum)\n",
    "    cols = fits.ColDefs([wcol, scol])\n",
    "    hdu = fits.BinTableHDU.from_columns(cols)\n",
    "    hdu.writeto(filename, overwrite=True)\n",
    "\n",
    "def batch_plot_output(resampled_image, extraction_bbox, \n",
    "                kernel_slice, kernel_model,\n",
    "                wavelength, spectrum, filename):\n",
    "    \"\"\"\n",
    "    Convenience function for summary output figures,\n",
    "    allowing visual inspection of the results from \n",
    "    each file being processed.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, \n",
    "                                        figsize=(8,12))\n",
    "    fig.suptitle(filename)\n",
    "    \n",
    "    ny, nx = resampled_image.shape\n",
    "    aspect = nx / (2 * ny)\n",
    "    \n",
    "    # Subplot 1: Extraction region\n",
    "    power_norm = simple_norm(resampled_image, 'power')\n",
    "    er_img = ax1.imshow(resampled_image, interpolation='none',\n",
    "               aspect=aspect, norm=power_norm, cmap='gray')\n",
    "    rx, ry, rw, rh = extraction_bbox\n",
    "    region = Rectangle((rx, ry), rw, rh, facecolor='none', \n",
    "                       edgecolor='b', linestyle='--')\n",
    "    er_ptch = ax1.add_patch(region)\n",
    "    \n",
    "    # Subplot 2: Kernel fit\n",
    "    xd_pixels = np.arange(kernel_slice.size)\n",
    "    fit_line = kernel_model(xd_pixels)\n",
    "    ks_line = ax2.plot(xd_pixels, kernel_slice, label='Kernel Slice')\n",
    "    kf_line = ax2.plot(xd_pixels, fit_line, 'o', label='Extraction Kernel')\n",
    "    k_lgd = ax2.legend()\n",
    "    \n",
    "    # Subplot 3: Extracted spectrum\n",
    "    spec_line = ax3.plot(wavelength, spectrum)\n",
    "    \n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over the desired files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_optimal_extraction(file_list):\n",
    "    \"\"\"\n",
    "    Iterate over a list of fits file paths, optimally extract\n",
    "    the SCI extension in each file, generate an output summary\n",
    "    image, and then save the resulting spectrum.\n",
    "    \n",
    "    Note that in the example dataset, there is only one SCI\n",
    "    extension in each file. For data with multiple SCI \n",
    "    extensions, a second loop over those extensions is\n",
    "    required.\n",
    "    \"\"\"\n",
    "    \n",
    "    # For this example data, we'll just use the default values\n",
    "    # for all the functions\n",
    "    for i, fitsfile in enumerate(file_list):\n",
    "        print(\"Processing file {} of {}: {}\".format(i+1, len(file_list), fitsfile))\n",
    "        dmodel = ImageModel(fitsfile)\n",
    "        spec2d = dmodel.data\n",
    "        wht2d = dmodel.wht\n",
    "        \n",
    "        k_slice = batch_kernel_slice(spec2d)\n",
    "        k_model = batch_fit_extraction_kernel(k_slice)\n",
    "        trace = batch_fit_trace_centers(spec2d, k_model)\n",
    "        spectrum = batch_extract_spectrum(spec2d, trace, k_model, wht2d)\n",
    "        \n",
    "        ny, nx = spec2d.shape\n",
    "        y2d, x2d = np.mgrid[:ny, :nx]\n",
    "        wavelength = batch_wavelength_from_wcs(dmodel, x2d, y2d)\n",
    "        \n",
    "        bbox = [0, 0, nx-1, ny-1]\n",
    "        \n",
    "        outfile = fitsfile.replace('s2d.fits', 'x1d_optimal')\n",
    "        \n",
    "        batch_plot_output(spec2d, bbox, k_slice, k_model,\n",
    "                    wavelength, spectrum, \n",
    "                    outfile+'.png')\n",
    "        batch_save_extracted_spectrum(outfile+'.fits', wavelength, spectrum)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on example dataset\n",
    "\n",
    "Take particular note of any spectrum which produces a warning during fitting - these are likely to be good candidates for interactive reprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Developer Note:*\n",
    "\n",
    "It would be great if there was a way to do this without spawning invisible plots from the creation of matplotlib figures, so that the `ioff` and `ion` calls could be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff() # if we don't turn this off, then matplotlib tries to display an (invisible) plot for each spectrum\n",
    "s2d_files = glob(os.path.join('s2d_files', '*s2d.fits'))\n",
    "batch_optimal_extraction(s2d_files)\n",
    "plt.ion() # now we turn it back on so everything else plots as it should!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: WebbPSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a PSF template, we can generate a PSF directly from the instrument model with [WebbPSF](https://webbpsf.readthedocs.io/en/stable/index.html). Currently, only the F110W and F140X imaging filters are supported, but we'll walk through the process anyway for whenever more filters become available.\n",
    "\n",
    "The primary function of WebbPSF is to produce imaging PSFs; however, it *can* generate a set of monochromatic PSFs, which we can combine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`webbpsf` is only needed here so we import it at the start of this appendix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webbpsf import NIRSpec, display_psf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WebbPSF has a number of data files which are required to run, so we'll begin by verifying that they can be accessed (and downloading them if necessary).\n",
    "\n",
    "Note that you will see a big red error message if you have not yet downloaded the data files. Don't worry, as long as you see \"Downloading WebbPSF data files.\" everything is still proceeding as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Developer Note:*\n",
    "\n",
    "WebbPSF should be updated so that the red error doesn't appear.  See https://github.com/spacetelescope/webbpsf/issues/380"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    instrument = NIRSpec()\n",
    "except OSError:\n",
    "    # assume that WebbPSF data files have not been downloaded\n",
    "    import tarfile, sys\n",
    "    print(\"Downloading WebbPSF data files.\")\n",
    "    webb_url = \"https://stsci.box.com/shared/static/qcptcokkbx7fgi3c00w2732yezkxzb99.gz\"\n",
    "    webb_file = os.path.join('.', \"webbpsf-data-0.9.0.tar.gz\")\n",
    "    urllib.request.urlretrieve(webb_url, webb_file)\n",
    "    print(\"Extracting into ./webbpsf-data ...\")\n",
    "    tar = tarfile.open(webb_file)\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    os.environ[\"WEBBPSF_PATH\"] = os.path.join(\".\",\"webbpsf-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument properties\n",
    "See the WebbPSF documentation for a full list of instrument settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument = NIRSpec()\n",
    "print(instrument.filter_list)\n",
    "\n",
    "# For reference:\n",
    "allowed_masks = ('S200A1','S200A2','S400A1','S1600A1','S200B1', \n",
    "                 'MSA all open', 'Single MSA open shutter', \n",
    "                 'Three adjacent MSA open shutters')\n",
    "\n",
    "# Edit these as necessary\n",
    "instrument.filter = 'F110W' \n",
    "instrument.image_mask = 'Three adjacent MSA open shutters'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monochromatic PSFs\n",
    "\n",
    "The most rigorous method we could use is to generate a PSF for each wavelength in the 2D spectrum and combine all of them. However, the computation time and memory required for this method is generally very large unless the spectra are quite short (in the dispersion direction). A more reasonable method (which is what we will be doing here) is to create a subset of monochromatic PSFs spaced evenly across the wavelength range, and interpolate between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "psf_wavelengths = np.linspace(wavelength[0], wavelength[-1], num=10) * 1.0e-6 # wavelengths must be in meters\n",
    "\n",
    "cube_hdul = instrument.calc_datacube(psf_wavelengths) #the output is a HDUList\n",
    "psf_cube = cube_hdul[1].data\n",
    "psf_cube.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the contents of the data cube\n",
    "fig9, ax9 = plt.subplots(nrows=5, ncols=2, figsize=(8,12))\n",
    "plt.subplots_adjust(hspace=0.15, wspace=0.01, left=0.06, \n",
    "                   right=0.94, bottom=0.05, top=0.95)\n",
    "for row in range(5):\n",
    "    for col in range(2):\n",
    "        ax = ax9[row, col]\n",
    "        w = row * 2 + col\n",
    "        wl = psf_wavelengths[w]\n",
    "        \n",
    "        display_psf(cube_hdul, ax=ax, cube_slice=w,\n",
    "                    title=\"$\\lambda$ = {:.3f} $\\mu$m\".format(wl*1e6),\n",
    "                    vmax=.2, vmin=1e-4, ext=1, colorbar=False)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation Methods\n",
    "\n",
    "The method of interpolation we choose depends strongly on how the PSF varies with wavelength. For evaluating the different methods, we'll create another monochromatic PSF for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_psf_hdul = instrument.calc_psf(monochromatic=3.0e-6)\n",
    "reference_psf = reference_psf_hdul[1].data\n",
    "ref_norm = simple_norm(reference_psf, stretch='log', min_cut=1e-4, max_cut=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way is a 3D linear interpolation, so let's see how it does. In the figure below, the top-left image is the reference PSF, the top-right is the linearly-interpolated PSF, the bottom left is a difference image, and the bottom right is a log-log plot of the pixel values in the reference (X) and interpolated (Y) PSFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_pix = reference_psf >= 1e-4\n",
    "psf_x = psf_y = np.arange(48)\n",
    "out_x, out_y = np.meshgrid(psf_x, psf_y, indexing='ij')\n",
    "interpolator = RegularGridInterpolator((psf_wavelengths, psf_x, psf_y), psf_cube, method='linear')\n",
    "linear_psf = interpolator((3.0e-6, out_x, out_y))\n",
    "\n",
    "diff_lin_psf = reference_psf - linear_psf\n",
    "\n",
    "print(\"Reference: min {:.3e}, max {:.3e}\".format(reference_psf.min(), reference_psf.max()))\n",
    "print(\"Linear: min {:.3e}, max {:.3e}\".format(linear_psf.min(), linear_psf.max()))\n",
    "print(\"Diff: min {:.3e}, max {:.3e}\".format(diff_lin_psf.min(), diff_lin_psf.max()))\n",
    "print(\"Total error: {:.5e}\".format(np.sqrt((diff_lin_psf**2).sum())))\n",
    "\n",
    "figA, axA = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.01, left=0.05, right=0.95)\n",
    "axA[0, 0].imshow(reference_psf, interpolation='none', norm=ref_norm)\n",
    "axA[0, 0].xaxis.set_visible(False)\n",
    "axA[0, 0].yaxis.set_visible(False)\n",
    "axA[0, 1].imshow(linear_psf, interpolation='none', norm=ref_norm)\n",
    "axA[0, 1].xaxis.set_visible(False)\n",
    "axA[0, 1].yaxis.set_visible(False)\n",
    "axA[1, 0].imshow(diff_lin_psf, interpolation='none', vmin=-5e-4, vmax=5e-4)\n",
    "axA[1, 0].xaxis.set_visible(False)\n",
    "axA[1, 0].yaxis.set_visible(False)\n",
    "axA[1, 1].loglog(reference_psf[ref_pix], linear_psf[ref_pix], 'k+')\n",
    "axA[1, 1].set_aspect('equal', 'box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method is more calculation-intensive, but could be more accurate. We go pixel-by-pixel through the PSF cube and interpolate with a 1D cubic spline along the wavelength axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic_psf = np.zeros_like(psf_cube[0])\n",
    "for row in np.arange(48):\n",
    "    for col in np.arange(48):\n",
    "        spline = interp1d(psf_wavelengths, psf_cube[:, row, col], kind='cubic')\n",
    "        cubic_psf[row, col] = spline(3.0e-6)\n",
    "        \n",
    "diff_cub_psf = reference_psf - cubic_psf\n",
    "\n",
    "print(\"Reference: min {:.3e}, max {:.3e}\".format(reference_psf.min(), reference_psf.max()))\n",
    "print(\"Cubic: min {:.3e}, max {:.3e}\".format(cubic_psf.min(), cubic_psf.max()))\n",
    "print(\"Diff: min {:.3e}, max {:.3e}\".format(diff_cub_psf.min(), diff_cub_psf.max()))\n",
    "print(\"Total error: {:.5e}\".format(np.sqrt((diff_cub_psf**2).sum())))\n",
    "\n",
    "figB, axB = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.01, left=0.05, right=0.95)\n",
    "axB[0, 0].imshow(reference_psf, interpolation='none', norm=ref_norm)\n",
    "axB[0, 0].xaxis.set_visible(False)\n",
    "axB[0, 0].yaxis.set_visible(False)\n",
    "axB[0, 1].imshow(cubic_psf, interpolation='none', norm=ref_norm)\n",
    "axB[0, 1].xaxis.set_visible(False)\n",
    "axB[0, 1].yaxis.set_visible(False)\n",
    "axB[1, 0].imshow(diff_cub_psf, interpolation='none', vmin=-5e-4, vmax=5e-4)\n",
    "axB[1, 0].xaxis.set_visible(False)\n",
    "axB[1, 0].yaxis.set_visible(False)\n",
    "axB[1, 1].loglog(reference_psf[ref_pix], cubic_psf[ref_pix], 'k+')\n",
    "axB[1, 1].set_aspect('equal', 'box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the log-log plot looks virtually identical to the linear case, the difference image in the spline case shows slightly larger errors in some of the central pixels. This is consistent with the \"total error\" statistic (the sum of squares of the difference image), which is larger in this second case.\n",
    "\n",
    "We can see in the plot below that the difference between the two methods is very slight, but the linearly-interpolated PSF is more accurate by about a factor of ~3 in total error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figC = plt.figure()\n",
    "plt.loglog(linear_psf[ref_pix], cubic_psf[ref_pix], 'k+')\n",
    "plt.xlabel('Linear interpolation')\n",
    "plt.ylabel('Cubic interpolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full trace PSF\n",
    "\n",
    "Now we can generate a full PSF for the spectral trace. Note that the PSF at each wavelength is going to be a linear combination of the overlapping adjacent monochromatic PSFs. If geometric distortion is present, it may be beneficial to create this PSF *after* the trace centers have been fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_w, cube_x, cube_y = np.meshgrid(wavelength * 1e-6, psf_x, psf_y, indexing='ij')\n",
    "full_psf_cube = interpolator((cube_w, cube_x, cube_y))\n",
    "nw, ny, nx = full_psf_cube.shape\n",
    "half = ny // 2\n",
    "trace = np.zeros((ny, nw), dtype=float)\n",
    "\n",
    "for wl, psf in enumerate(full_psf_cube):\n",
    "    lo = wl - half\n",
    "    lo_w = max(lo, 0)\n",
    "    lo_x = lo_w - lo\n",
    "    hi = wl + half\n",
    "    hi_w = min(hi, nw)\n",
    "    hi_x = nx - (hi - hi_w)\n",
    "    trace[:, lo_w:hi_w] += psf[:, lo_x:hi_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpsf_aspect = nw / (2. * ny)\n",
    "figD = plt.figure(figsize=(10, 8))\n",
    "trace_norm = simple_norm(trace, stretch='log', min_cut=1e-4, max_cut=0.2)\n",
    "plt.imshow(trace, interpolation='none', aspect=wpsf_aspect, norm=trace_norm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the trace\n",
    "\n",
    "Currently, our PSF array is not the same size or position as the trace in the extraction region. While we could shift and trim to the correct size, the spectrum will rarely be centered on a pixel, and is sufficiently under-sampled that fractional pixel shifts in the PSF could cause significant errors in the final extraction. Thus, we will perform a final resampling to the location of the spectrum in the extraction region. To do this, we can use our old friend `RegularGridInterpolator`. We set the center of the WebbPSF trace (originally at row 23) to our fit trace center, and resample appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_row = np.arange(ny)\n",
    "trace_interpolator = RegularGridInterpolator((trace_row, wavelength), trace)\n",
    "center_0 = 23\n",
    "center_1 = fit_extraction_kernel.mean_0\n",
    "\n",
    "out_lo = center_0 - center_1\n",
    "out_hi = out_lo + er_ny\n",
    "\n",
    "resample_row = np.linspace(out_lo, out_hi, er_ny)\n",
    "resample_y, resample_w = np.meshgrid(resample_row, wavelength, indexing='ij')\n",
    "\n",
    "resampled_trace = trace_interpolator((resample_y, resample_w))\n",
    "\n",
    "figE, axE = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
    "plt.subplots_adjust(hspace=0.1)\n",
    "trace_renorm = simple_norm(resampled_trace, stretch='log')\n",
    "axE[0].imshow(resampled_trace, interpolation='none', aspect=aspect_ratio, norm=trace_renorm)\n",
    "axE[1].imshow(extraction_region, cmap='gray', aspect=aspect_ratio, \n",
    "                  norm=er_norm, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this notebook\n",
    "**Author:** Graham Kanarek, Staff Scientist, Science Support  \n",
    "**Updated On:** 2020-07-13\n",
    "\n",
    "Optimal extraction algorithm adapted from [Horne (1986)](https://ui.adsabs.harvard.edu/abs/1986PASP...98..609H/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
