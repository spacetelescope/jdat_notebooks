{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOTS Time Series Observations\n",
    "\n",
    " **Use case:** Bright Object Time Series; extracting exoplanet spectra.<br>\n",
    "**Data:** JWST simulated NIRSpec data from ground-based campaign; GJ436b spectra from the Goyal et al. (2018).<br>\n",
    "**Tools:**  scikit, lmfit, scipy, matplotlip, astropy, pandas.<br>\n",
    "**Cross-intrument:** . <br>\n",
    "**Documentation:** This notebook is part of a STScI's larger [post-pipeline Data Analysis Tools Ecosystem](https://jwst-docs.stsci.edu/jwst-post-pipeline-data-analysis).<br>\n",
    "**Author:** David K. Sing (dsing@jhu.edu)<br>\n",
    "**Last updated:** 2 July 2020\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook uses time series JWST NIRSpec data taken during a ground-based campaign to illustrate extracting exoplanet spectra from time-series observations.  \n",
    "\n",
    "The data are derived from the ISIM-CV3, the cryovacuum campaign of the JWST Integrated Science Instrument Module (ISIM), that took place at Goddard Space Flight Center during the winter 2015-2016 (Kimble et al. 2016). The data can be found at https://www.cosmos.esa.int/web/jwst-nirspec/test-data, and detailed and insightful report of the data by G. Giardino, S. Birkmann, P. Ferruit, B. Dorner, B. Rauscher can be found here: ftp://ftp.cosmos.esa.int/jwstlib/ReleasedCV3dataTimeSeries/CV3_TimeSeries_PRM.tgz\n",
    "\n",
    "This NIRSpec time series dataset has had a transit light curve injected at the pixel-level, which closely mimics a bright object time series (BOTS) observation of a transiting exoplanet. In this case, a GJ436b spectra from the \n",
    "__[Goyal et al. (2018)](https://ui.adsabs.harvard.edu/abs/2018MNRAS.474.5158G/)__ \n",
    "exoplanet grid was selected (clear atmosphere at solar metallicity).  With an actual NIRSpec dataset, the noise properties of the detector, jitter, and the effects on extracting exoplanet spectra from time-series observations can more accurately simulated.\n",
    "\n",
    "Broadly the aim of this notebook is to work with these time series observations to:\n",
    "\n",
    " 1) Extract 1D spectra from the 2D spectral images. \n",
    "    \n",
    " 2) Define a time series model to fit to the wavelength dependent transit light curve.\n",
    "    \n",
    " 3) Fit each time series wavelength bin of the 1D spectra, measuring the desired quantity $R_{pl}(\\lambda)/R_{star}$.\n",
    " \n",
    " 4) Produce a measured transmission spectrum that can then be compared to models.\n",
    " \n",
    "The example outputs the fit light curves for each spectral bin, along with fitting statistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages\n",
    "\n",
    "This notebook uses packages (matplotlib, astropy, scipy, glob, lmfit, pickle, os, sklearn) which can all be installed in a standard fashion through pip.\n",
    "\n",
    "Several routines to calculate limb-darkening and a transit model were extracted from ExoTiC-ISm \n",
    "(__[Laginja & Wakeford 2020](https://ui.adsabs.harvard.edu/abs/2020JOSS....5.2281L/)__ ;\n",
    "https://github.com/hrwakeford/ExoTiC-ISM), and slightly adapted. The full set of stellar models used for the limb-darkening calculation can also be downloaded from ExoTiC-ISM, as this notebook only downloads and loads the single stellar model used to generate the limb darkening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from astropy.utils.data import download_file\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits, ascii\n",
    "from astropy.modeling.models import custom_model\n",
    "from astropy.modeling.fitting import LevMarLSQFitter\n",
    "from scipy.interpolate import interp1d, splev, splrep\n",
    "from scipy.io import readsav\n",
    "from scipy import stats\n",
    "import glob\n",
    "import lmfit\n",
    "import pickle\n",
    "from os import path, mkdir\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameters\n",
    "\n",
    "Parameters of the fit include directories where the data and limb darkening stellar models are held, along with properties of the planet and star. The stellar and planet values that have been entered here (modeled after GJ436) are the same as was used to model the injected transit.  Note, the 4500K stellar model used to inject the transit was hotter than GJ436A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP  ----------------------------------------------\n",
    "# Setup directories\n",
    "save_directory = './notebookrun2/'          # Local directory to save files to\n",
    "data_directory = './'                       # Local data to work with fits files if desired\n",
    "\n",
    "# Setup Detector Properties & Rednoise measurement timescale \n",
    "gain = 1.0   # 2D spectra has already converted to counts, gain of detector is 1.0\n",
    "binmeasure = 256   # Binning technique to measure rednoise, choose bin size to evaluate sigma_r\n",
    "number_of_images = 8192  # Number of images in the dataset\n",
    "\n",
    "# Setup Planet Properties\n",
    "grating = 'NIRSpecPrism'\n",
    "ld_model = '3D'      # 3D/1D stellar model choice (transit was injected with the 3D model)\n",
    "\n",
    "# Setup Stellar Properties for Limb-Darkening Calculation\n",
    "Teff = 4500           # Effective Temperature (K)\n",
    "logg = 4.5            # Surface Gravity\n",
    "M_H = 0.0            # Stellar Metallicity log_10[M/H]\n",
    "Rstar = 0.455          # Planet radius (in units of solar radii Run)\n",
    "\n",
    "# Setup Transit parameters (can get from NASA exoplanet archive)\n",
    "t0 = 2454865.084034              # bjd time of inferior conjunction \n",
    "per = 2.64389803                  # orbital period (days) BJD_TDB\n",
    "rp = 0.0804                      # Planet radius (in units of stellar radii)\n",
    "a_Rs = 14.54                       # Semi-major axis (input a/Rstar so units of stellar radii)\n",
    "inc = 86.858 * (2*np.pi/360)      # Orbital inclination (in degrees->radians)\n",
    "ecc = 0.0                         # Eccentricity\n",
    "omega = 0.0 * (2*np.pi/360)         # Longitude of periastron (in degrees->radians)\n",
    "\n",
    "rho_star = (3*np.pi)/(6.67259E-8*(per*86400)**2)*(a_Rs)**3     # Stellar Density (g/cm^3) from a/Rs\n",
    "# a_Rs=(rho_star*6.67259E-8*per_sec*per_sec/(3*np.pi))**(1/3)  # a/Rs from Stellar Density (g/cm^3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local directories\n",
    "if not path.exists(save_directory):\n",
    "    mkdir(save_directory)      # Create a new directory to save outputs to if needed\n",
    "if not path.exists(save_directory+'3DGrid'): \n",
    "    mkdir(save_directory+'3DGrid')      # Create new directory to save\n",
    "limb_dark_directory = save_directory    # Point to limb darkeing directory contaning 3DGrid/ directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Load NIRSpec data\n",
    "\n",
    "The fits images are loaded, and information including the image date and science spectra are saved.\n",
    "\n",
    "A default flux offset value BZERO is also taken from the header and subtracted from every science frame.\n",
    "\n",
    "Reading in the 2^13 fits files is slow.  To speed things up, we created a pickle file of the for first instance the fits images are loaded.  This 1GB pickle file is loaded instead of reading the fits files if found.\n",
    "\n",
    "Alternatively, the fits files can be downloaded here:\n",
    "https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/transit_spectroscopy_notebook/Archive.Trace_SLIT_A_1600_SRAD-PRM-PS-6007102143_37803_JLAB88_injected.tar.gz.  The images are in a tar.gz archvie, which needs to be un-archived and data_directory variable set to the directory in the SETUP cell above.\n",
    "\n",
    "The cell below downloads the 1GB JWST data pickle file, and several other files needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 1GB NIRSpec Data\n",
    "fn_jw = save_directory+'jwst_data.pickle'\n",
    "if not path.exists(fn_jw):\n",
    "    fn = download_file('https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/transit_spectroscopy_notebook/jwst_data.pickle')\n",
    "    dest = shutil.move(fn, save_directory+'jwst_data.pickle')  \n",
    "    print('JWST Data Download Complete')\n",
    "\n",
    "# Download further files needed, move to local directory for easier repeated access\n",
    "fn_sens = download_file('https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/transit_spectroscopy_notebook/NIRSpec.prism.sensitivity.sav')\n",
    "dest = shutil.move(fn_sens, save_directory+'NIRSpec.prism.sensitivity.sav')  # Move files to save_directory\n",
    "\n",
    "fn_ld = download_file('https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/transit_spectroscopy_notebook/3DGrid/mmu_t45g45m00v05.flx')\n",
    "destld = shutil.move(fn_ld, save_directory+'3DGrid/mmu_t45g45m00v05.flx')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the Pickle File data.  Alternatly, the data can be read from the original fits files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.exists(fn_jw):\n",
    "    dbfile = open(fn_jw, 'rb') # for reading also binary mode is important\n",
    "    jwst_data = pickle.load(dbfile)\n",
    "    print('Loading JWST data from Pickle File')\n",
    "    bjd = jwst_data['bjd']\n",
    "    wsdata_all = jwst_data['wsdata_all']\n",
    "    shx = jwst_data['shx']\n",
    "    shy = jwst_data['shy']\n",
    "    common_mode = jwst_data['common_mode']\n",
    "    all_spec = jwst_data['all_spec']\n",
    "    exposure_length = jwst_data['exposure_length']\n",
    "    dbfile.close()    \n",
    "    print('Done')\n",
    "elif not path.exists(fn_jw):\n",
    "    # Load all fits images\n",
    "    # Arrays created for BJD time, and the white light curve total_counts\n",
    "    list = glob.glob(data_directory+\"*.fits\")\n",
    "    index_of_images = np.arange(number_of_images) \n",
    "    bjd = np.zeros((number_of_images))\n",
    "    exposure_length = np.zeros((number_of_images))\n",
    "    all_spec = np.zeros((32, 512, number_of_images))\n",
    "    for i in index_of_images:\n",
    "        img = list[i]\n",
    "        print(img)\n",
    "        hdul = fits.open(img)\n",
    "        # hdul.info()\n",
    "        bjd_image = hdul[0].header['BJD_TDB']\n",
    "        BZERO = hdul[0].header['BZERO']        # flux value offset\n",
    "        bjd[i] = bjd_image\n",
    "        expleng = hdul[0].header['INTTIME']    # Total integration time for one MULTIACCUM (seconds)\n",
    "        exposure_length[i] = expleng/86400.    # Total integration time for one MULTIACCUM (days)\n",
    "        print(bjd[i])\n",
    "        data = hdul[0].data\n",
    "        # total counts in image\n",
    "        # total_counts[i]=gain*np.sum(data[11:18,170:200]-BZERO) # total counts in 12 pix wide aperture around pixel 60 image\n",
    "        all_spec[:, :, i] = gain * (data-BZERO)     # Load all spectra into an array subtract flux value offset\n",
    "        hdul.close()\n",
    "    # Sort data\n",
    "    srt = np.argsort(bjd) # index to sort\n",
    "    bjd = bjd[srt]\n",
    "    # total_counts=total_counts[srt]\n",
    "    exposure_length = exposure_length[srt]\n",
    "    all_spec[:, :, :] = all_spec[:, :, srt]\n",
    "\n",
    "    # Get Wavelength of Data\n",
    "    file_wave = download_file('https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/transit_spectroscopy_notebook/JWST_NIRSpec_wavelength_microns.txt')\n",
    "    f = open(file_wave, 'r')\n",
    "    wsdata_all = np.genfromtxt(f)\n",
    "    \n",
    "    print('wsdata size :', wsdata_all.shape)\n",
    "    print('Data wavelength Loaded :', wsdata_all)\n",
    "    print('wsdata new size :', wsdata_all.shape)\n",
    "    \n",
    "    # Read in Detrending parameters\n",
    "    # Mean of parameter must be 0.0 to be properly normalized\n",
    "    # Idealy standard deviation of parameter = 1.0\n",
    "    file_xy = download_file('https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/transit_spectroscopy_notebook/JWST_NIRSpec_Xposs_Yposs_CM_detrending.txt')\n",
    "    f = open(file_xy, 'r')\n",
    "    data = np.genfromtxt(f, delimiter=',')\n",
    "    shx = data[:, 0]\n",
    "    shy = data[:, 1]\n",
    "    common_mode = data[:, 2]\n",
    "    \n",
    "    # Store Data in a pickle file\n",
    "    jwst_data = {'bjd': bjd, 'wsdata_all': wsdata_all, 'shx': shx, 'shy': shy, 'common_mode': common_mode, 'all_spec': all_spec, 'exposure_length': exposure_length}\n",
    "    dbfile = open('jwst_data.pickle', 'ab') # Its important to use binary mode\n",
    "    pickle.dump(jwst_data, dbfile)\n",
    "    dbfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the 2D spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expnum = 2                                           # Choose Exposure number to view\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10.0, 3.0]           # Figure dimensions\n",
    "plt.rcParams['figure.dpi'] = 200                   # Resolution\n",
    "plt.rcParams['savefig.dpi'] = 200\n",
    "plt.rcParams['image.aspect'] = 5                     # Aspect ratio (the CCD is quite long!!!)\n",
    "plt.cmap = plt.cm.magma\n",
    "plt.cmap.set_bad('k', 1.)\n",
    "plt.rcParams['image.cmap'] = 'magma'                   # Colormap\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "plt.rcParams['image.origin'] = 'lower'\n",
    "plt.rcParams['font.family'] = \"monospace\"\n",
    "plt.rcParams['font.monospace'] = 'DejaVu Sans Mono'\n",
    "\n",
    "img = all_spec[:, :, expnum]\n",
    "zeros = np.where(img <= 0)     # Plot on a log scale, so set zero or negative values to a small number \n",
    "img[zeros] = 1E-10\n",
    "fig, axs = plt.subplots()\n",
    "f = axs.imshow(np.log10(img), vmin=0) # Plot image\n",
    "plt.xlabel('x-pixel')\n",
    "plt.ylabel('y-pixel')\n",
    "axs.yaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "axs.yaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "axs.xaxis.set_major_locator(ticker.MultipleLocator(50))\n",
    "axs.xaxis.set_minor_locator(ticker.MultipleLocator(10))\n",
    "plt.title('2D NIRSpec Image of Exposure ' + str(expnum))\n",
    "fig.colorbar(f, label='Log$_{10}$ Electron counts', ax=axs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract 1D spectra from 2D array of images\n",
    "\n",
    "Ideally, extracting 1D spectra from the 2D images would use optimal aperture extraction along a fit \n",
    "trace with routines equivalent to IRAF/apall. This functionality is not yet available in astro-py.\n",
    "\n",
    "Several processing steps have already been applied.  The 2D spectra here have been flat field corrected, and 1/f noise has been removed from each pixel by subtracting the median count rate from the un-illuminated pixels along each column (see __[Giardino et al.](ftp://ftp.cosmos.esa.int/jwstlib/ReleasedCV3dataTimeSeries/CV3_TimeSeries_PRM.tgz)__\n",
    "for more information about 1/f noise).  Each 2D image has also been aligned in the X and Y directions, such that each pixel corresponds to the same wavelength.  As the CV3 test had no requirements for flux stability, the ~1% flux variations from the LED have also been removed.  \n",
    "    \n",
    "For spectral extraction, the example here simply uses a simple summed box.  The 8192 2D spectra have been\n",
    "pre-loaded into a numpy array.  The spectra peaks at pixel Y=16. For each column, an aperature sum is taken over Y-axis pixels 11 to 18, which\n",
    "contains most of the spectrum counts. Wider aperture would add more counts, but also introduces more noise.\n",
    "\n",
    "**Further cleaning steps are not done here** \n",
    "\n",
    "1) Ideally, the pixels flagged as bad for various reasons should be cleaned.  \n",
    "\n",
    "2) Cosmic rays should be identified and removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spec.shape\n",
    "y_lower = 11                                             # Lower extraction aperture\n",
    "y_upper = 18                                             # Upper extraction aperture\n",
    "all_spec_1D = np.sum(all_spec[y_lower:y_upper, :, :], axis=0) # Sum along Y-axis from pixels 11 to 18\n",
    "# Plot \n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10.0, 3.0]           # Figure dimensions\n",
    "plt.rcParams['figure.dpi'] = 200                   # Resolution\n",
    "plt.rcParams['savefig.dpi'] = 200\n",
    "plt.rcParams['image.aspect'] = 5                     # Aspect ratio (the CCD is quite long!!!)\n",
    "plt.cmap = plt.cm.magma\n",
    "plt.cmap.set_bad('k', 1.)\n",
    "plt.rcParams['image.cmap'] = 'magma'                   # Colormap\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "plt.rcParams['image.origin'] = 'lower'\n",
    "plt.rcParams['font.family'] = \"monospace\"\n",
    "plt.rcParams['font.monospace'] = 'DejaVu Sans Mono'\n",
    "\n",
    "img = all_spec[:, :, expnum]\n",
    "zeros = np.where(img <= 0)     # Plot on a log scale, so set zero or negative values to a small number \n",
    "img[zeros] = 1E-10\n",
    "fig, axs = plt.subplots()\n",
    "f = axs.imshow(np.log10(img), vmin=0)  # Plot image\n",
    "plt.xlabel('x-pixel')\n",
    "plt.ylabel('y-pixel')\n",
    "axs.yaxis.set_major_locator(ticker.MultipleLocator(5))\n",
    "axs.yaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "axs.xaxis.set_major_locator(ticker.MultipleLocator(50))\n",
    "axs.xaxis.set_minor_locator(ticker.MultipleLocator(10))\n",
    "plt.axhline(y_lower, color='w', ls='dashed')\n",
    "plt.axhline(y_upper, color='w', ls='dashed')\n",
    "plt.title('2D NIRSpec Image of Exposure '+str(expnum))\n",
    "fig.colorbar(f, label='Log$_{10}$ Electron counts', ax=axs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the 1D spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "f = plt.plot(wsdata_all, all_spec_1D[:, 0], linewidth=2, zorder=0)  # overplot Transit model at data\n",
    "plt.xlabel(r'Wavelength ($\\mu$m)')\n",
    "plt.ylabel('Flux (e-)')\n",
    "axs.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\n",
    "axs.xaxis.set_minor_locator(ticker.MultipleLocator(0.1))\n",
    "plt.annotate('H$_2$O', xy=(3.0, 42000))\n",
    "plt.annotate('CO$_2$', xy=(4.2, 42000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CV3 test observed a lamp with a similar PSF as JWST will have, and has significant counts from \n",
    "about 1.5 to 4.5 $\\mu$m.\n",
    "\n",
    "The cryogenic test chamber had CO$_2$ and H$_2$O ice buildup on the window, which can be seen as spectral absorption features in the 2D spectra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Orbital Phase and a separate fine grid model used for plotting purposes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Orbital Phase\n",
    "phase = (bjd-t0) / (per)  # phase in days relative to T0 ephemeris\n",
    "phase = phase - np.fix(phase[number_of_images-1]) # Have current phase occur at value 0.0\n",
    "\n",
    "t_fine = np.linspace(np.min(bjd), np.max(bjd), 1000) # times at which to calculate light curve\n",
    "phase_fine = (t_fine-t0)/(per)  # phase in days relative to T0 ephemeris\n",
    "phase_fine = phase_fine-np.fix(phase[number_of_images-1]) # Have current phase occur at value 0.0\n",
    "\n",
    "b0 = a_Rs * np.sqrt((np.sin(phase * 2 * np.pi)) ** 2 + (np.cos(inc) * np.cos(phase * 2 * np.pi)) ** 2)\n",
    "intransit = (b0-rp < 1.0E0).nonzero()  # Select indicies between first and fourth contact\n",
    "outtransit = (b0-rp > 1.0E0).nonzero() # Select indicies out of transit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Systematic Drift On the Detector\n",
    "\n",
    "The CV3 test assessed the stability of the instrument by introducing a large spatial jitter and drift. This resulted in a significant X,Y movement of the spectra on the 2D detector. While this bulk shift has been removed which aligns the spectra, intra- and inter- pixel sensitivities introduce flux variations which need to be removed. The jitter from the CV3 test was more than 30 mas, which is ~4X larger than the JWST stability requirement. Thus, in orbit these detector effects are expected to be significantly smaller, but they will still be present and will need to be modeled and removed from time series observations.\n",
    "\n",
    "The detector X, Y positions here were measured from cross-correlation of the 2D images (collapsing the spectra along one dimension first), and are saved in arrays $shx$ and $shy$. These detending vectors would ideally be measured using the trace position values from the spectral extraction of each integration, as that could also accurately measure integration-to-integration how the spectra spatially changed on the detector.\n",
    "\n",
    "The detector shifts have original amplitudes near 0.2 pixels, though the vectors have had initial normalization. For detrending purposes, these arrays should have a mean of 0 and standard deviation of 1.0.\n",
    "\n",
    "A residual color-dependent trend with the LED lamp can also been seen in the CV3 data, which can be partly removed by scaling original common-mode lamp trend, which was measured using the CV3 white light curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shx_tmp = shx / np.mean(shx) - 1.0E0       # Set Mean around 0.0\n",
    "shx_detrend = shx_tmp/np.std(shx_tmp)  # Set standard deviation to 1.0\n",
    "shy_tmp = shy / np.mean(shy) - 1.0E0       # Set Mean around 0.0\n",
    "shy_detrend = shy_tmp/np.std(shy_tmp)  # Set standard deviation to 1.0\n",
    "\n",
    "cm = common_mode / np.mean(common_mode) - 1.0E0\n",
    "cm_detrend = cm/np.std(cm)\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "plt.plot(shx_detrend, label='X-possition')\n",
    "plt.plot(shy_detrend, label='Y-possition')\n",
    "plt.xlabel('Image Sequence Number')\n",
    "plt.ylabel('Relative Detector Possition')\n",
    "plt.title('Time-series Detrending Vectors')\n",
    "axs.xaxis.set_major_locator(ticker.MultipleLocator(1000))\n",
    "axs.xaxis.set_minor_locator(ticker.MultipleLocator(100))\n",
    "axs.yaxis.set_major_locator(ticker.MultipleLocator(0.5))\n",
    "axs.yaxis.set_minor_locator(ticker.MultipleLocator(0.1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create arrays of the vectors used for detrending.  \n",
    "\n",
    "From Sing et al. 2019:  Systematic errors are often removed by a parameterized deterministic model, where the non-transit photometric trends are found to correlate with a number $n$ of external parameters (or optical state vectors, $X$). These parameters describe changes in the instrument or other external factors as a function of time during the observations, and are fit with a coefficient for each optical state parameter, $p_n$, to model and remove (or detrend) the photometric light curves.\n",
    "\n",
    "When including systematic trends, the total parameterized model of the flux measurements over time, $f(t)$, can be modeled as a combination of the theoretical transit model, $T(t,\\theta)$ (which depends upon the transit parameters $\\theta$), the total baseline flux detected from the star, $F_0$, and the systematics error model $S(x)$ giving,\n",
    "\n",
    "$f(t) = T(t,\\theta)\\times F_0 \\times S(x)$.\n",
    "\n",
    "We will use a linear model for the instrument systematic effects.\n",
    "\n",
    "$S(x)= p_1 x + p_2 y + p_3 x^2 + p_4 y^2 + p_5 x y + p_6 cm + p_7 \\phi $\n",
    "\n",
    "$cm$ is the common_mode trend, and $\\phi$ is a linear time trend which helps remove changing H$_2$O ice within the H$_2$O spectral feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shx = shx_detrend\n",
    "shy = shy_detrend\n",
    "common_mode = cm_detrend\n",
    "\n",
    "XX = np.array([shx, shy, shx**2, shy**2, shx*shy, common_mode, np.ones(number_of_images)])  # Detrending array without linear time trend\n",
    "XX = np.transpose(XX)\n",
    "XXX = np.array([shx, shy, shx**2, shy**2, shx*shy, common_mode, phase, np.ones(number_of_images)])  # Detrending array with with linear time trend\n",
    "XXX = np.transpose(XXX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression** can be used to quickly determine the parameters $p_n$ using the out-of-transit data.\n",
    "\n",
    "Here, we take a wavelength bin of the data (pixels 170 to 200) to make a time series.  The out-of-transit points are selected and a linear regression of $S(x)$ is done to determine the optical state parameters $p_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix1 = 170       # wavelength bin lower range\n",
    "pix2 = 200       # wavelength bin upper range\n",
    "y = np.sum(all_spec_1D[pix1:pix2, :], axis=0)    # flux over a selected wavelength bin\n",
    "\n",
    "msize = plt.rcParams['lines.markersize'] ** 2.           # default marker size\n",
    "plt.rcParams['figure.figsize'] = [10.0, 3.0]           # Figure dimensions\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "f = plt.plot(wsdata_all, all_spec_1D[:, 0], linewidth=2, zorder=0)  # Plot Region of wavelength bin\n",
    "plt.fill_between(wsdata_all[pix1:pix2], 0, all_spec_1D[pix1:pix2, 0], alpha=0.5)\n",
    "plt.xlabel(r'Wavelength ($\\mu$m)')\n",
    "plt.ylabel('Flux (e-)')\n",
    "plt.title('1D Extracted Spectrum')\n",
    "axs.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\n",
    "axs.xaxis.set_minor_locator(ticker.MultipleLocator(0.1))\n",
    "plt.annotate('H$_2$O', xy=(3.0, 42000))\n",
    "plt.annotate('CO$_2$', xy=(4.2, 42000))\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "plt.scatter(bjd, y/np.mean(y[outtransit]), label='$f(t)$ Data', zorder=1, s=msize*0.75, linewidth=1, alpha=0.4, marker='+', edgecolors='blue')\n",
    "plt.xlabel('Barycentric Julian Date (days)')\n",
    "plt.ylabel('Relative Flux')\n",
    "plt.title(r'Time-series Transit Light Curve  $\\lambda=$['+str(wsdata_all[pix1])+':'+str(wsdata_all[pix2]) + r'] $\\mu$m')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(XX[outtransit], y[outtransit]/np.mean(y[outtransit]))\n",
    "print('Linear Regression Coefficients:')\n",
    "print(regressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are on the order of ~10$^{-4}$ so the trends have an amplitude on the order of 100's of ppm.\n",
    "\n",
    "Visualize the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfit = regressor.predict(XX)                         # Project the fit over the whole time series\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10.0, 7.0]           # Figure dimensions\n",
    "msize = plt.rcParams['lines.markersize'] ** 2.           # default marker size\n",
    "plt.scatter(bjd, y/np.mean(y[outtransit]), label='$f(t)$ Data', zorder=1, s=msize*0.75, linewidth=1, alpha=0.5, marker='+', edgecolors='blue')\n",
    "f = plt.plot(bjd, yfit, label='$S(x)$ Regression fit ', linewidth=2, color='orange', zorder=2, alpha=0.85)\n",
    "plt.xlabel('Barycentric Julian Date (days)')\n",
    "plt.ylabel('Relative Flux')\n",
    "plt.title(r'Time-series Transit Light Curve  $\\lambda=$['+str(wsdata_all[pix1])+':'+str(wsdata_all[pix2]) + r'] $\\mu$m')\n",
    "axs.xaxis.set_major_locator(ticker.MultipleLocator(0.01))\n",
    "axs.xaxis.set_minor_locator(ticker.MultipleLocator(0.005))\n",
    "axs.yaxis.set_major_locator(ticker.MultipleLocator(0.002))\n",
    "axs.yaxis.set_minor_locator(ticker.MultipleLocator(0.001))\n",
    "yplot = y / np.mean(y[outtransit])\n",
    "plt.ylim(yplot.min() * 0.999, yplot.max()*1.001)\n",
    "plt.xlim(bjd.min()-0.001, bjd.max()+0.001)\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transit and Limb-Darkening Model Functions\n",
    "\n",
    "Define a functions used by the fitting routines. \n",
    "These which will take the transit and systematic parameters and create our full transit light curve model\n",
    "\n",
    "$model = T(t,\\theta)\\times F_0 \\times S(x)$\n",
    "\n",
    "compares it to the data\n",
    "\n",
    "$y = f(t)$\n",
    "\n",
    "by returning the residuals \n",
    "\n",
    "$(y-model)/(\\sigma_y)$\n",
    "\n",
    "To calculate the transit model, here we use  __[Mandel and Agol (2002)](https://ui.adsabs.harvard.edu/abs/2002ApJ...580L.171M/abstract)__ as coded in python by H. Wakeford (__[ExoTiC-ISM](https://github.com/hrwakeford/ExoTiC-ISM)__).\n",
    "\n",
    "To calculate the stellar limb-darkening, we use the procedure from Sing et al. (2010) which uses stellar models and fits for non-linear limb darkening coefficients, with a module as coded in python by H. Wakeford (__[ExoTiC-ISM](https://github.com/hrwakeford/ExoTiC-ISM)__).\n",
    "\n",
    "A new orbit is first calculated based on the system parameters of $a/R_{star}$, the cosine of the inclination $cos(i)$, and the orbital phase $\\phi$. \n",
    "The inputs are the orbit distance between the planet-star center $b$ at each phase, limb-darkening parameters ($c_1,c_2,c_3,c_4$), and the planet-to-star radius ratio $R_p/R_{star}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@custom_model\n",
    "def nonlinear_limb_darkening(x, c0=0.0, c1=0.0, c2=0.0, c3=0.0):\n",
    "    \"\"\"\n",
    "    Define non-linear limb darkening model with four parameters c0, c1, c2, c3.\n",
    "    \"\"\"\n",
    "    model = (1. - (c0 * (1. - x ** (1. / 2)) + c1 * (1. - x ** (2. / 2)) + c2 * (1. - x ** (3. / 2)) + c3 *\n",
    "                   (1. - x ** (4. / 2))))\n",
    "    return model\n",
    "\n",
    "\n",
    "@custom_model\n",
    "def quadratic_limb_darkening(x, aLD=0.0, bLD=0.0):\n",
    "    \"\"\"\n",
    "    Define linear limb darkening model with parameters aLD and bLD.\n",
    "    \"\"\"\n",
    "    model = 1. - aLD * (1. - x) - bLD * (1. - x) ** (4. / 2.)\n",
    "    return model\n",
    "\n",
    "\n",
    "def limb_dark_fit(grating, wsdata, M_H, Teff, logg, dirsen, ld_model='1D'):\n",
    "    \"\"\"\n",
    "    Calculates stellar limb-darkening coefficients for a given wavelength bin.\n",
    "\n",
    "    Currently supports:\n",
    "    HST STIS G750L, G750M, G430L gratings\n",
    "    HST WFC3 UVIS/G280, IR/G102, IR/G141 grisms\n",
    "\n",
    "    What is used for 1D models - Kurucz (?)\n",
    "    Procedure from Sing et al. (2010, A&A, 510, A21).\n",
    "    Uses 3D limb darkening from Magic et al. (2015, A&A, 573, 90).\n",
    "    Uses photon FLUX Sum over (lambda*dlamba).\n",
    "    :param grating: string; grating to use ('G430L','G750L','G750M', 'G280', 'G102', 'G141')\n",
    "    :param wsdata: array; data wavelength solution\n",
    "    :param M_H: float; stellar metallicity\n",
    "    :param Teff: float; stellar effective temperature (K)\n",
    "    :param logg: float; stellar gravity\n",
    "    :param dirsen: string; path to main limb darkening directory\n",
    "    :param ld_model: string; '1D' or '3D', makes choice between limb darkening models; default is 1D\n",
    "    :return: uLD: float; linear limb darkening coefficient\n",
    "    aLD, bLD: float; quadratic limb darkening coefficients\n",
    "    cp1, cp2, cp3, cp4: float; three-parameter limb darkening coefficients\n",
    "    c1, c2, c3, c4: float; non-linear limb-darkening coefficients\n",
    "    \"\"\"\n",
    "\n",
    "    print('You are using the', str(ld_model), 'limb darkening models.')\n",
    "\n",
    "    if ld_model == '1D':\n",
    "\n",
    "        direc = os.path.join(dirsen, 'Kurucz')\n",
    "\n",
    "        print('Current Directories Entered:')\n",
    "        print('  ' + dirsen)\n",
    "        print('  ' + direc)\n",
    "\n",
    "        # Select metallicity\n",
    "        M_H_Grid = np.array([-0.1, -0.2, -0.3, -0.5, -1.0, -1.5, -2.0, -2.5, -3.0, -3.5, -4.0, -4.5, -5.0, 0.0, 0.1, 0.2, 0.3, 0.5, 1.0])\n",
    "        M_H_Grid_load = np.array([0, 1, 2, 3, 5, 7, 8, 9, 10, 11, 12, 13, 14, 17, 20, 21, 22, 23, 24])\n",
    "        optM = (abs(M_H - M_H_Grid)).argmin()\n",
    "        MH_ind = M_H_Grid_load[optM]\n",
    "\n",
    "        # Determine which model is to be used, by using the input metallicity M_H to figure out the file name we need\n",
    "        direc = 'Kurucz'\n",
    "        file_list = 'kuruczlist.sav'\n",
    "        sav1 = readsav(os.path.join(dirsen, file_list))\n",
    "        model = bytes.decode(sav1['li'][MH_ind])  # Convert object of type \"byte\" to \"string\"\n",
    "\n",
    "        # Select Teff and subsequently logg\n",
    "        Teff_Grid = np.array([3500, 3750, 4000, 4250, 4500, 4750, 5000, 5250, 5500, 5750, 6000, 6250, 6500])\n",
    "        optT = (abs(Teff - Teff_Grid)).argmin()\n",
    "\n",
    "        logg_Grid = np.array([4.0, 4.5, 5.0])\n",
    "        optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        if logg_Grid[optG] == 4.0:\n",
    "            Teff_Grid_load = np.array([8, 19, 30, 41, 52, 63, 74, 85, 96, 107, 118, 129, 138])\n",
    "\n",
    "        elif logg_Grid[optG] == 4.5:\n",
    "            Teff_Grid_load = np.array([9, 20, 31, 42, 53, 64, 75, 86, 97, 108, 119, 129, 139])\n",
    "\n",
    "        elif logg_Grid[optG] == 5.0:\n",
    "            Teff_Grid_load = np.array([10, 21, 32, 43, 54, 65, 76, 87, 98, 109, 120, 130, 140])\n",
    "\n",
    "        # Where in the model file is the section for the Teff we want? Index T_ind tells us that.\n",
    "        T_ind = Teff_Grid_load[optT]\n",
    "        header_rows = 3    # How many rows in each section we ignore for the data reading\n",
    "        data_rows = 1221   # How  many rows of data we read\n",
    "        line_skip_data = (T_ind + 1) * header_rows + T_ind * data_rows   # Calculate how many lines in the model file we need to skip in order to get to the part we need (for the Teff we want).\n",
    "        line_skip_header = T_ind * (data_rows + header_rows)\n",
    "\n",
    "        # Read the header, in case we want to have the actual Teff, logg and M_H info.\n",
    "        # headerinfo is a pandas object.\n",
    "        headerinfo = pd.read_csv(os.path.join(dirsen, direc, model), delim_whitespace=True, header=None,\n",
    "                                 skiprows=line_skip_header, nrows=1)\n",
    "\n",
    "        Teff_model = headerinfo[1].values[0]\n",
    "        logg_model = headerinfo[3].values[0]\n",
    "        MH_model = headerinfo[6].values[0]\n",
    "        MH_model = float(MH_model[1:-1])\n",
    "\n",
    "        print('\\nClosest values to your inputs:')\n",
    "        print('Teff: ', Teff_model)\n",
    "        print('M_H: ', MH_model)\n",
    "        print('log(g): ', logg_model)\n",
    "\n",
    "        # Read the data; data is a pandas object.\n",
    "        data = pd.read_csv(os.path.join(dirsen, direc, model), delim_whitespace=True, header=None,\n",
    "                           skiprows=line_skip_data, nrows=data_rows)\n",
    "\n",
    "        # Unpack the data\n",
    "        ws = data[0].values * 10   # Import wavelength data\n",
    "        f0 = data[1].values / (ws * ws)\n",
    "        f1 = data[2].values * f0 / 100000.\n",
    "        f2 = data[3].values * f0 / 100000.\n",
    "        f3 = data[4].values * f0 / 100000.\n",
    "        f4 = data[5].values * f0 / 100000.\n",
    "        f5 = data[6].values * f0 / 100000.\n",
    "        f6 = data[7].values * f0 / 100000.\n",
    "        f7 = data[8].values * f0 / 100000.\n",
    "        f8 = data[9].values * f0 / 100000.\n",
    "        f9 = data[10].values * f0 / 100000.\n",
    "        f10 = data[11].values * f0 / 100000.\n",
    "        f11 = data[12].values * f0 / 100000.\n",
    "        f12 = data[13].values * f0 / 100000.\n",
    "        f13 = data[14].values * f0 / 100000.\n",
    "        f14 = data[15].values * f0 / 100000.\n",
    "        f15 = data[16].values * f0 / 100000.\n",
    "        f16 = data[17].values * f0 / 100000.\n",
    "\n",
    "        # Make single big array of them\n",
    "        fcalc = np.array([f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16])\n",
    "        phot1 = np.zeros(fcalc.shape[0])\n",
    "\n",
    "        # Define mu\n",
    "        mu = np.array([1.000, .900, .800, .700, .600, .500, .400, .300, .250, .200, .150, .125, .100, .075, .050, .025, .010])\n",
    "\n",
    "        # Passed on to main body of function are: ws, fcalc, phot1, mu\n",
    "\n",
    "    elif ld_model == '3D':\n",
    "\n",
    "        direc = os.path.join(dirsen, '3DGrid')\n",
    "\n",
    "        print('Current Directories Entered:')\n",
    "        print('  ' + dirsen)\n",
    "        print('  ' + direc)\n",
    "\n",
    "        # Select metallicity\n",
    "        M_H_Grid = np.array([-3.0, -2.0, -1.0, 0.0])  # Available metallicity values in 3D models\n",
    "        M_H_Grid_load = ['30', '20', '10', '00']  # The according identifiers to individual available M_H values\n",
    "        optM = (abs(M_H - M_H_Grid)).argmin()  # Find index at which the closes M_H values from available values is to the input M_H.\n",
    "\n",
    "        # Select Teff\n",
    "        Teff_Grid = np.array([4000, 4500, 5000, 5500, 5777, 6000, 6500, 7000])  # Available Teff values in 3D models\n",
    "        optT = (abs(Teff - Teff_Grid)).argmin()  # Find index at which the Teff values is, that is closest to input Teff.\n",
    "\n",
    "        # Select logg, depending on Teff. If several logg possibilities are given for one Teff, pick the one that is\n",
    "        # closest to user input (logg).\n",
    "\n",
    "        if Teff_Grid[optT] == 4000:\n",
    "            logg_Grid = np.array([1.5, 2.0, 2.5])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 4500:\n",
    "            logg_Grid = np.array([2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 5000:\n",
    "            logg_Grid = np.array([2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 5500:\n",
    "            logg_Grid = np.array([3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 5777:\n",
    "            logg_Grid = np.array([4.4])\n",
    "            optG = 0\n",
    "\n",
    "        elif Teff_Grid[optT] == 6000:\n",
    "            logg_Grid = np.array([3.5, 4.0, 4.5])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 6500:\n",
    "            logg_Grid = np.array([4.0, 4.5])\n",
    "            optG = (abs(logg - logg_Grid)).argmin()\n",
    "\n",
    "        elif Teff_Grid[optT] == 7000:\n",
    "            logg_Grid = np.array([4.5])\n",
    "            optG = 0\n",
    "\n",
    "        # Select Teff and Log g. Mtxt, Ttxt and Gtxt are then put together as string to load correct files.\n",
    "        Mtxt = M_H_Grid_load[optM]\n",
    "        Ttxt = \"{:2.0f}\".format(Teff_Grid[optT] / 100)\n",
    "        if Teff_Grid[optT] == 5777:\n",
    "            Ttxt = \"{:4.0f}\".format(Teff_Grid[optT])\n",
    "        Gtxt = \"{:2.0f}\".format(logg_Grid[optG] * 10)\n",
    "\n",
    "        #\n",
    "        file = 'mmu_t' + Ttxt + 'g' + Gtxt + 'm' + Mtxt + 'v05.flx'\n",
    "        print('Filename:', file)\n",
    "\n",
    "        # Read data from IDL .sav file\n",
    "        sav = readsav(os.path.join(direc, file))  # readsav reads an IDL .sav file\n",
    "        ws = sav['mmd'].lam[0]  # read in wavelength\n",
    "        flux = sav['mmd'].flx  # read in flux\n",
    "        Teff_model = Teff_Grid[optT]\n",
    "        logg_model = logg_Grid[optG]\n",
    "        MH_model = str(M_H_Grid[optM])\n",
    "\n",
    "        print('\\nClosest values to your inputs:')\n",
    "        print('Teff  : ', Teff_model)\n",
    "        print('M_H   : ', MH_model)\n",
    "        print('log(g): ', logg_model)\n",
    "\n",
    "        f0 = flux[0]\n",
    "        f1 = flux[1]\n",
    "        f2 = flux[2]\n",
    "        f3 = flux[3]\n",
    "        f4 = flux[4]\n",
    "        f5 = flux[5]\n",
    "        f6 = flux[6]\n",
    "        f7 = flux[7]\n",
    "        f8 = flux[8]\n",
    "        f9 = flux[9]\n",
    "        f10 = flux[10]\n",
    "\n",
    "        # Make single big array of them\n",
    "        fcalc = np.array([f0, f1, f2, f3, f4, f5, f6, f7, f8, f9, f10])\n",
    "        phot1 = np.zeros(fcalc.shape[0])\n",
    "\n",
    "        # Mu from grid\n",
    "        # 0.00000    0.0100000    0.0500000     0.100000     0.200000     0.300000   0.500000     0.700000     0.800000     0.900000      1.00000\n",
    "        mu = sav['mmd'].mu\n",
    "\n",
    "        # Passed on to main body of function are: ws, fcalc, phot1, mu\n",
    "\n",
    "    # Load response function and interpolate onto kurucz model grid\n",
    "\n",
    "    # FOR STIS\n",
    "    if grating == 'G430L':\n",
    "        sav = readsav(os.path.join(dirsen, 'G430L.STIS.sensitivity.sav'))  # wssens,sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 3\n",
    "\n",
    "    if grating == 'G750M':\n",
    "        sav = readsav(os.path.join(dirsen, 'G750M.STIS.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 0.554\n",
    "\n",
    "    if grating == 'G750L':\n",
    "        sav = readsav(os.path.join(dirsen, 'G750L.STIS.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 4.882\n",
    "\n",
    "    # FOR WFC3\n",
    "    if grating == 'G141':  # http://www.stsci.edu/hst/acs/analysis/reference_files/synphot_tables.html\n",
    "        sav = readsav(os.path.join(dirsen, 'G141.WFC3.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 1\n",
    "\n",
    "    if grating == 'G102':  # http://www.stsci.edu/hst/acs/analysis/reference_files/synphot_tables.html\n",
    "        sav = readsav(os.path.join(dirsen, 'G141.WFC3.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 1\n",
    "\n",
    "    if grating == 'G280':  # http://www.stsci.edu/hst/acs/analysis/reference_files/synphot_tables.html\n",
    "        sav = readsav(os.path.join(dirsen, 'G280.WFC3.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 1\n",
    "\n",
    "    # FOR JWST\n",
    "    if grating == 'NIRSpecPrism':  # http://www.stsci.edu/hst/acs/analysis/reference_files/synphot_tables.html\n",
    "        sav = readsav(os.path.join(dirsen, 'NIRSpec.prism.sensitivity.sav'))  # wssens, sensitivity\n",
    "        wssens = sav['wssens']\n",
    "        sensitivity = sav['sensitivity']\n",
    "        wdel = 12\n",
    "\n",
    "    widek = np.arange(len(wsdata))\n",
    "    wsHST = wssens\n",
    "    wsHST = np.concatenate((np.array([wsHST[0] - wdel - wdel, wsHST[0] - wdel]),\n",
    "                            wsHST,\n",
    "                            np.array([wsHST[len(wsHST) - 1] + wdel,\n",
    "                                      wsHST[len(wsHST) - 1] + wdel + wdel])))\n",
    "\n",
    "    respoutHST = sensitivity / np.max(sensitivity)\n",
    "    respoutHST = np.concatenate((np.zeros(2), respoutHST, np.zeros(2)))\n",
    "    inter_resp = interp1d(wsHST, respoutHST, bounds_error=False, fill_value=0)\n",
    "    respout = inter_resp(ws)  # interpolate sensitivity curve onto model wavelength grid\n",
    "\n",
    "    wsdata = np.concatenate((np.array([wsdata[0] - wdel - wdel, wsdata[0] - wdel]), wsdata,\n",
    "                             np.array([wsdata[len(wsdata) - 1] + wdel, wsdata[len(wsdata) - 1] + wdel + wdel])))\n",
    "    respwavebin = wsdata / wsdata * 0.0\n",
    "    widek = widek + 2  # need to add two indicies to compensate for padding with 2 zeros\n",
    "    respwavebin[widek] = 1.0\n",
    "    data_resp = interp1d(wsdata, respwavebin, bounds_error=False, fill_value=0)\n",
    "    reswavebinout = data_resp(ws)  # interpolate data onto model wavelength grid\n",
    "\n",
    "    # Integrate over the spectra to make synthetic photometric points.\n",
    "    for i in range(fcalc.shape[0]):  # Loop over spectra at diff angles\n",
    "        fcal = fcalc[i, :]\n",
    "        Tot = int_tabulated(ws, ws * respout * reswavebinout)\n",
    "        phot1[i] = (int_tabulated(ws, ws * respout * reswavebinout * fcal, sort=True)) / Tot\n",
    "\n",
    "    if ld_model == '1D':\n",
    "        yall = phot1 / phot1[0]\n",
    "    elif ld_model == '3D':\n",
    "        yall = phot1 / phot1[10]\n",
    "\n",
    "    # Co = np.zeros((6, 4))   # NOT-REUSED\n",
    "\n",
    "    # A = [0.0, 0.0, 0.0, 0.0]  # c1, c2, c3, c4      # NOT-REUSED\n",
    "    x = mu[1:]     # wavelength\n",
    "    y = yall[1:]   # flux\n",
    "    # weights = x / x   # NOT-REUSED\n",
    "\n",
    "    # Start fitting the different models\n",
    "    fitter = LevMarLSQFitter()\n",
    "\n",
    "    # Fit a four parameter non-linear limb darkening model and get fitted variables, c1, c2, c3, c4.\n",
    "    corot_4_param = nonlinear_limb_darkening()\n",
    "    corot_4_param = fitter(corot_4_param, x, y)\n",
    "    c1, c2, c3, c4 = corot_4_param.parameters\n",
    "\n",
    "    # Fit a three parameter non-linear limb darkening model and get fitted variables, cp2, cp3, cp4 (cp1 = 0).\n",
    "    corot_3_param = nonlinear_limb_darkening()\n",
    "    corot_3_param.c0.fixed = True  # 3 param is just 4 param with c0 = 0.0\n",
    "    corot_3_param = fitter(corot_3_param, x, y)\n",
    "    cp1, cp2, cp3, cp4 = corot_3_param.parameters\n",
    "\n",
    "    # Fit a quadratic limb darkening model and get fitted parameters aLD and bLD.\n",
    "    quadratic = quadratic_limb_darkening()\n",
    "    quadratic = fitter(quadratic, x, y)\n",
    "    aLD, bLD = quadratic.parameters\n",
    "\n",
    "    # Fit a linear limb darkening model and get fitted variable uLD.\n",
    "    linear = nonlinear_limb_darkening()\n",
    "    linear.c0.fixed = True\n",
    "    linear.c2.fixed = True\n",
    "    linear.c3.fixed = True\n",
    "    linear = fitter(linear, x, y)\n",
    "    uLD = linear.c1.value\n",
    "\n",
    "    print('\\nLimb darkening parameters:')\n",
    "    print(\"4param \\t{:0.8f}\\t{:0.8f}\\t{:0.8f}\\t{:0.8f}\".format(c1, c2, c3, c4))\n",
    "    print(\"3param \\t{:0.8f}\\t{:0.8f}\\t{:0.8f}\".format(cp2, cp3, cp4))\n",
    "    print(\"Quad \\t{:0.8f}\\t{:0.8f}\".format(aLD, bLD))\n",
    "    print(\"Linear \\t{:0.8f}\".format(uLD))\n",
    "\n",
    "    return uLD, c1, c2, c3, c4, cp1, cp2, cp3, cp4, aLD, bLD\n",
    "\n",
    "\n",
    "def int_tabulated(X, F, sort=False):\n",
    "    Xsegments = len(X) - 1\n",
    "\n",
    "    # Sort vectors into ascending order.\n",
    "    if not sort:\n",
    "        ii = np.argsort(X)\n",
    "        X = X[ii]\n",
    "        F = F[ii]\n",
    "\n",
    "    while (Xsegments % 4) != 0:\n",
    "        Xsegments = Xsegments + 1\n",
    "\n",
    "    Xmin = np.min(X)\n",
    "    Xmax = np.max(X)\n",
    "\n",
    "    # Uniform step size.\n",
    "    h = (Xmax + 0.0 - Xmin) / Xsegments\n",
    "    # Compute the interpolates at Xgrid.\n",
    "    # x values of interpolates >> Xgrid = h * FINDGEN(Xsegments + 1L) + Xmin\n",
    "    z = splev(h * np.arange(Xsegments + 1) + Xmin, splrep(X, F))\n",
    "\n",
    "    # Compute the integral using the 5-point Newton-Cotes formula.\n",
    "    ii = (np.arange((len(z) - 1) / 4, dtype=int) + 1) * 4\n",
    "\n",
    "    return np.sum(2.0 * h * (7.0 * (z[ii - 4] + z[ii]) + 32.0 * (z[ii - 3] + z[ii - 1]) + 12.0 * z[ii - 2]) / 45.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now define the transit model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occultnl(rl, c1, c2, c3, c4, b0):\n",
    "    \"\"\"\n",
    "    MANDEL & AGOL (2002) transit model.\n",
    "    :param rl: float, transit depth (Rp/R*)\n",
    "    :param c1: float, limb darkening parameter 1\n",
    "    :param c2: float, limb darkening parameter 2\n",
    "    :param c3: float, limb darkening parameter 3\n",
    "    :param c4: float, limb darkening parameter 4\n",
    "    :param b0: impact parameter in stellar radii\n",
    "    :return: mulimb0: limb-darkened transit model, mulimbf: lightcurves for each component that you put in the model\n",
    "    \"\"\"\n",
    "    mulimb0 = occultuniform(b0, rl)\n",
    "    bt0 = b0\n",
    "    fac = np.max(np.abs(mulimb0 - 1))\n",
    "    if fac == 0:\n",
    "        fac = 1e-6  # DKS edit\n",
    "\n",
    "    omega = 4 * ((1 - c1 - c2 - c3 - c4) / 4 + c1 / 5 + c2 / 6 + c3 / 7 + c4 / 8)\n",
    "    nb = len(b0)\n",
    "    indx = np.where(mulimb0 != 1.0)[0]\n",
    "    if len(indx) == 0:\n",
    "        indx = -1\n",
    "    mulimb = mulimb0[indx]\n",
    "    mulimbf = np.zeros((5, nb))\n",
    "    mulimbf[0, :] = mulimbf[0, :] + 1.\n",
    "    mulimbf[1, :] = mulimbf[1, :] + 0.8\n",
    "    mulimbf[2, :] = mulimbf[2, :] + 2 / 3\n",
    "    mulimbf[3, :] = mulimbf[3, :] + 4 / 7\n",
    "    mulimbf[4, :] = mulimbf[4, :] + 0.5\n",
    "    nr = np.int64(2)\n",
    "    dmumax = 1.0\n",
    "\n",
    "    while (dmumax > fac * 1.e-3) and (nr <= 131072):\n",
    "        # print(nr)\n",
    "        mulimbp = mulimb\n",
    "        nr = nr * 2\n",
    "        dt = 0.5 * np.pi / nr\n",
    "        t = dt * np.arange(nr + 1)\n",
    "        th = t + 0.5 * dt\n",
    "        r = np.sin(t)\n",
    "        sig = np.sqrt(np.cos(th[nr - 1]))\n",
    "        mulimbhalf = sig ** 3 * mulimb0[indx] / (1 - r[nr - 1])\n",
    "        mulimb1 = sig ** 4 * mulimb0[indx] / (1 - r[nr - 1])\n",
    "        mulimb3half = sig ** 5 * mulimb0[indx] / (1 - r[nr - 1])\n",
    "        mulimb2 = sig ** 6 * mulimb0[indx] / (1 - r[nr - 1])\n",
    "        for i in range(1, nr):\n",
    "            mu = occultuniform(b0[indx] / r[i], rl / r[i])\n",
    "            sig1 = np.sqrt(np.cos(th[i - 1]))\n",
    "            sig2 = np.sqrt(np.cos(th[i]))\n",
    "            mulimbhalf = mulimbhalf + r[i] ** 2 * mu * (sig1 ** 3 / (r[i] - r[i - 1]) - sig2 ** 3 / (r[i + 1] - r[i]))\n",
    "            mulimb1 = mulimb1 + r[i] ** 2 * mu * (sig1 ** 4 / (r[i] - r[i - 1]) - sig2 ** 4 / (r[i + 1] - r[i]))\n",
    "            mulimb3half = mulimb3half + r[i] ** 2 * mu * (sig1 ** 5 / (r[i] - r[i - 1]) - sig2 ** 5 / (r[i + 1] - r[i]))\n",
    "            mulimb2 = mulimb2 + r[i] ** 2 * mu * (sig1 ** 6 / (r[i] - r[i - 1]) - sig2 ** 6 / (r[i + 1] - r[i]))\n",
    "\n",
    "        mulimb = ((1 - c1 - c2 - c3 - c4) * mulimb0[\n",
    "            indx] + c1 * mulimbhalf * dt + c2 * mulimb1 * dt + c3 * mulimb3half * dt + c4 * mulimb2 * dt) / omega\n",
    "        ix1 = np.where(mulimb + mulimbp != 0.)[0]\n",
    "        if len(ix1) == 0:\n",
    "            ix1 = -1\n",
    "\n",
    "        # print(ix1)\n",
    "        # python cannot index on single values so you need to use atlest_1d for the below to work when mulimb is a single value\n",
    "        dmumax = np.max(np.abs(np.atleast_1d(mulimb)[ix1] - np.atleast_1d(mulimbp)[ix1]) / (\n",
    "                np.atleast_1d(mulimb)[ix1] + np.atleast_1d(mulimbp)[ix1]))\n",
    "\n",
    "    mulimbf[0, indx] = np.atleast_1d(mulimb0)[indx]\n",
    "    mulimbf[1, indx] = mulimbhalf * dt\n",
    "    mulimbf[2, indx] = mulimb1 * dt\n",
    "    mulimbf[3, indx] = mulimb3half * dt\n",
    "    mulimbf[4, indx] = mulimb2 * dt\n",
    "    np.atleast_1d(mulimb0)[indx] = mulimb\n",
    "    b0 = bt0\n",
    "\n",
    "    return mulimb0, mulimbf\n",
    "\n",
    "\n",
    "def occultuniform(b0, w):\n",
    "    \"\"\"\n",
    "    Compute the lightcurve for occultation of a uniform source without microlensing (Mandel & Agol 2002).\n",
    "\n",
    "    :param b0: array; impact parameter in units of stellar radii\n",
    "    :param w: array; occulting star size in units of stellar radius\n",
    "    :return: muo1: float; fraction of flux at each b0 for a uniform source\n",
    "    \"\"\"\n",
    "\n",
    "    if np.abs(w - 0.5) < 1.0e-3:\n",
    "        w = 0.5\n",
    "\n",
    "    nb = len(np.atleast_1d(b0))\n",
    "    muo1 = np.zeros(nb)\n",
    "\n",
    "    for i in range(nb):\n",
    "        # substitute z=b0(i) to shorten expressions\n",
    "        z = np.atleast_1d(b0)[i]\n",
    "        # z = z.value    # stripping it of astropy units\n",
    "        if z >= 1+w:\n",
    "            muo1[i] = 1.0\n",
    "            continue\n",
    "\n",
    "        if w >= 1 and z <= w-1:\n",
    "            muo1[i] = 0.0\n",
    "            continue\n",
    "\n",
    "        if z >= np.abs(1-w) and z <= 1+w:\n",
    "            kap1 = np.arccos(np.min(np.append((1 - w ** 2 + z ** 2) / 2 / z, 1.)))\n",
    "            kap0 = np.arccos(np.min(np.append((w ** 2 + z ** 2 - 1) / 2 / w / z, 1.)))\n",
    "            lambdae = w ** 2 * kap0 + kap1\n",
    "            lambdae = (lambdae - 0.5 * np.sqrt(np.max(np.append(4. * z ** 2 - (1 + z ** 2 - w ** 2) ** 2, 0.)))) / np.pi\n",
    "            muo1[i] = 1 - lambdae\n",
    "\n",
    "        if z <= 1-w:\n",
    "            muo1[i] = 1 - w ** 2\n",
    "            continue\n",
    "\n",
    "    return muo1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now define the function to generate the transit light curve and compare it to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to call and calculate models\n",
    "def residual(p, phase, x, y, err, c1, c2, c3, c4):\n",
    "    # calculate new orbit\n",
    "    b0 = p['a_Rs'].value * np.sqrt((np.sin(phase * 2 * np.pi)) ** 2 + (p['cosinc'].value * np.cos(phase * 2 * np.pi)) ** 2)\n",
    "    # Select indicies between first and fourth contact\n",
    "    intransit = (b0-p['rprs'].value < 1.0E0).nonzero()\n",
    "    # Make light curve model, set all values initially to 1.0\n",
    "    light_curve = b0/b0\n",
    "    mulimb0, mulimbf = occultnl(p['rprs'].value, c1, c2, c3, c4, b0[intransit])  # Madel and Agol\n",
    "    light_curve[intransit] = mulimb0\n",
    "    model = (light_curve) * p['f0'].value * (p['Fslope'].value * phase + p['xsh'].value * shx + p['x2sh'].value * shx**2. + p['ysh'].value * shy + p['y2sh'].value * shy**2. + p['xysh'].value * shy * shx + p['comm'].value * common_mode + 1.0) # transit model is baseline flux X transit model X systematics model\n",
    "    chi2now = np.sum((y-model)**2/err**2)\n",
    "    res = np.std((y-model)/p['f0'].value)\n",
    "    print(\"rprs: \", p['rprs'].value, \"current chi^2=\", chi2now, ' scatter ', res, end=\"\\r\")\n",
    "    return (y-model)/err\n",
    "    # return np.sum((y-model)**2/err**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is also defined to return just the transit model $T(t,\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fine(p):  # Make Transit model with a fine grid for plotting purposes\n",
    "    b0 = p['a_Rs'].value * np.sqrt((np.sin(phase_fine * 2 * np.pi)) ** 2 + (p['cosinc'].value * np.cos(phase_fine * 2 * np.pi)) ** 2)\n",
    "    mulimb0, mulimbf = occultnl(p['rprs'].value, c1, c2, c3, c4, b0)  # Madel and Agol\n",
    "    model_fine = mulimb0\n",
    "    return model_fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add a transit model to the Example Light curve.  Here, we've compute the limb darkening coefficients, then use them in the transit light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave1 = wsdata_all[pix1]\n",
    "wave2 = wsdata_all[pix2]\n",
    "bin_wave_index = ((wsdata_all > wave1) & (wsdata_all <= wave2)).nonzero()\n",
    "wsdata = wsdata_all[bin_wave_index]*1E4 # Select wavelength bin values (um=> angstroms)\n",
    "\n",
    "_uLD, c1, c2, c3, c4, _cp1, _cp2, _cp3, _cp4, aLD, bLD = limb_dark_fit(grating, wsdata, M_H, Teff, logg, limb_dark_directory, ld_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the transit model.\n",
    "\n",
    "The transit parameters such as inclination and $a/R_{star}$ have been setup at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Transit Model\n",
    "rl = 0.0825     # Planet-to-star Radius Ratio\n",
    "\n",
    "b0 = a_Rs * np.sqrt((np.sin(phase * 2 * np.pi)) ** 2 + (np.cos(inc) * np.cos(phase * 2 * np.pi)) ** 2)\n",
    "intransit = (b0-rl < 1.0E0).nonzero()  # Select indicies between first and fourth contact\n",
    "\n",
    "mulimb0, mulimbf = occultnl(rl, c1, c2, c3, c4, b0)  # Mandel & Agol non-linear limb darkened transit model\n",
    "model = mulimb0*yfit \n",
    "\n",
    "# plot\n",
    "plt.rcParams['figure.figsize'] = [10.0, 7.0]           # Figure dimensions\n",
    "msize = plt.rcParams['lines.markersize'] ** 2.           # default marker size\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "gs = fig.add_gridspec(3, 1, hspace=0.00, wspace=0.00)\n",
    "ax1 = fig.add_subplot(gs[0:2, :])\n",
    "ax1.scatter(bjd, y/np.mean(y[outtransit]), label='$f(t)$ Data', zorder=1, s=msize*0.75, linewidth=1, alpha=0.5, marker='+', edgecolors='blue')\n",
    "ax1.plot(bjd, model, label='$S(x)$ Regression fit ', linewidth=2, color='orange', zorder=2, alpha=0.85)\n",
    "ax1.xaxis.set_ticklabels([])\n",
    "plt.ylabel('Relative Flux')\n",
    "plt.title(r'Time-series Transit Light Curve  $\\lambda=$['+str(wsdata_all[pix1])+':'+str(wsdata_all[pix2]) + r'] $\\mu$m')\n",
    "ax1.xaxis.set_major_locator(ticker.MultipleLocator(0.01))\n",
    "ax1.xaxis.set_minor_locator(ticker.MultipleLocator(0.005))\n",
    "ax1.yaxis.set_major_locator(ticker.MultipleLocator(0.002))\n",
    "ax1.yaxis.set_minor_locator(ticker.MultipleLocator(0.001))\n",
    "yplot = y/np.mean(y[outtransit])\n",
    "plt.ylim(yplot.min()*0.999, yplot.max()*1.001)\n",
    "plt.xlim(bjd.min()-0.001, bjd.max()+0.001)\n",
    "plt.legend()\n",
    "fig.add_subplot(ax1)\n",
    "# Residual\n",
    "ax2 = fig.add_subplot(gs[2, :])\n",
    "ax2.scatter(bjd, 1E6*(y/np.mean(y[outtransit])-model), label='$f(t)$ Data', zorder=1, s=msize*0.75, linewidth=1, alpha=0.5, marker='+', edgecolors='blue')\n",
    "wsb, wsb_bin_edges, binnumber = stats.binned_statistic(bjd, 1E6*(y/np.mean(y[outtransit])-model), bins=256)\n",
    "plt.scatter(wsb_bin_edges[1:], wsb, linewidth=2, alpha=0.75, facecolors='orange', edgecolors='none', marker='o', zorder=25)\n",
    "plt.xlabel('Barycentric Julian Date (days)')\n",
    "plt.ylabel('Residual (ppm)')\n",
    "ax2.xaxis.set_major_locator(ticker.MultipleLocator(0.01))\n",
    "ax2.xaxis.set_minor_locator(ticker.MultipleLocator(0.005))\n",
    "yplot = y / np.mean(y[outtransit])\n",
    "plt.xlim(bjd.min()-0.001, bjd.max()+0.001)\n",
    "fig.add_subplot(ax2)\n",
    "plt.show()\n",
    "\n",
    "# print chi^2 value\n",
    "err = np.sqrt(y) / np.mean(y[outtransit])\n",
    "print('Chi^2 = '+str(np.sum((y/np.mean(y[outtransit])-model)**2/err**2)))\n",
    "\n",
    "print('Residual Standard Deviation : '+str(1E6*np.std((y/np.mean(y[outtransit])-model)))+' ppm')\n",
    "print('256 Bin Standard Deviation  :'+str(np.std(wsb))+' ppm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model transit depth is a little too deep compared to the data. The planet radius needs to be smaller, and the parameter $rl$ is closer to 0.08.  As an exercise you can re-run the above cell changing the planet radius to $rl$=0.0805 and compare the $\\chi^2$ value to the previous default value ($\\chi^2$=9265.4 at $rl$ = 0.0825)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT Transit Light Curves\n",
    "\n",
    "Now we can fit each light curve, optimizing the fit parameters with a least-squares fit. Here a Levenberg-Marquart fit is used to find a $\\chi^2$ minimum and estimate uncertainties using the lmfit package (https://lmfit.github.io/lmfit-py/fitting.html).\n",
    "\n",
    "In practice, first we would fit the white light curve, which consists of summing over all of the wavelengths in the entire 1D spectra.  This can then be used to fit for the system parameters such as inclination and transit time, and then the spectroscopic channels are then fixed to these values as they are wavelength-independent.  However, the CV3 data required the overall variations of the lamp to be removed, which prevents our use of using this data for a white light curve analysis.  Here, we proceed to fitting for the spectroscopic light curve bins.\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "    1) Wavelength Bin is selected\n",
    "    \n",
    "    2) Limb-darkening coefficients are calculated from a stellar model for each bin. \n",
    "    \n",
    "    3) An initial linear regression is performed on the out-of-transit data to start the \n",
    "    systematic fit parameters, this greatly speeds up the fit as those parameters start \n",
    "    near their global minimum.\n",
    "    \n",
    "    4) The fit is started, and some statistics are output during the minimization\n",
    "    \n",
    "    5) Once the best-fit is found, a number of statistics are displayed \n",
    "    \n",
    "    6) Finally, several plots are generated which are stored as PDFs and the next bin is started.\n",
    "\n",
    "These steps are performed for each spectral bin.\n",
    "\n",
    "In this example, the planet radius is set to vary in the fit along with the baseline flux and instrument systematic parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Wavelengths to fit over\n",
    "The spectra must be binned in wavelength to get sufficient counts to reach ~100 ppm levels needed.\n",
    "The spectra has significant counts from about pixel 100 to 400, we start at pixel $k0$ and bin the spectra by $wk$ pixels.\n",
    "\n",
    "Several arrays are also defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k0 = 113 # 98   #100\n",
    "kend = 392 # 422\n",
    "wk = 15\n",
    "number_of_bins = int((kend-k0)/wk)\n",
    "wsd = np.zeros((number_of_bins))\n",
    "werr = np.zeros((number_of_bins))\n",
    "rprs = np.zeros((number_of_bins))\n",
    "rerr = np.zeros((number_of_bins))\n",
    "sig_r = np.zeros((number_of_bins))\n",
    "sig_w = np.zeros((number_of_bins))\n",
    "beta = np.zeros((number_of_bins))\n",
    "depth = np.zeros((number_of_bins))\n",
    "depth_err = np.zeros((number_of_bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Over Wavelength Bins Fitting Each Lightcurve\n",
    "### Note this step takes considerable time to complete (~20 min, few minutes/bin)\n",
    "Each wavelength bin is fit for the transit+systematics model. Various outputs including plots are saved.\n",
    "Can skip to the next cells to load pre-computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k0   # wavelength to start\n",
    "# --------------------------------------------------------------------------\n",
    "# Loop over wavelength bins and fit for each one\n",
    "for bin in range(0, number_of_bins):\n",
    "    \n",
    "    # Select wavelength bin\n",
    "    wave1 = wsdata_all[k]\n",
    "    wave2 = wsdata_all[k+wk]\n",
    "\n",
    "    # Indicies to select for wavelgth bin\n",
    "    bin_wave_index = ((wsdata_all > wave1) & (wsdata_all <= wave2)).nonzero()\n",
    "\n",
    "    # make light curve bin\n",
    "    wave_bin_counts = np.sum(all_spec_1D[k+1:k+wk, :], axis=0)  # Sum Wavelength pixels\n",
    "    wave_bin_counts_err = np.sqrt(wave_bin_counts)            # adopt photon noise for errors\n",
    "\n",
    "    # Calculate Limb Darkening\n",
    "\n",
    "    wsdata = wsdata_all[bin_wave_index]*1E4 # Select wavelength bin values (um=> angstroms)\n",
    "    _uLD, c1, c2, c3, c4, _cp1, _cp2, _cp3, _cp4, aLD, bLD = limb_dark_fit(grating, wsdata, M_H, Teff, logg, limb_dark_directory, ld_model)\n",
    "\n",
    "    print('\\nc1 = {}'.format(c1))\n",
    "    print('c2 = {}'.format(c2))\n",
    "    print('c3 = {}'.format(c3))\n",
    "    print('c4 = {}'.format(c4))\n",
    "    print('')\n",
    "    # u   = [c1,c2,c3,c4]      # limb darkening coefficients\n",
    "    # u = [aLD, bLD]\n",
    "\n",
    "    # Make initial model\n",
    "    \n",
    "    # Setup LMFIT\n",
    "    x = bjd                    # X data\n",
    "    y = wave_bin_counts        # Y data\n",
    "    err = wave_bin_counts_err  # Y Error\n",
    "\n",
    "    # Perform Quick Linear regression on out-of-transit data to obtain accurate starting Detector fit values\n",
    "    if wave1 > 2.7 and wave1 < 3.45:\n",
    "        regressor.fit(XXX[outtransit], y[outtransit]/np.mean(y[outtransit]))\n",
    "    else:\n",
    "        regressor.fit(XX[outtransit], y[outtransit]/np.mean(y[outtransit]))\n",
    "\n",
    "    # create a set of Parameters for LMFIT https://lmfit.github.io/lmfit-py/parameters.html\n",
    "    # class Parameter(name, value=None, vary=True, min=- inf, max=inf, expr=None, brute_step=None, user_data=None)¶\n",
    "    # Set vary=0 to fix\n",
    "    # Set vary=1 to fit\n",
    "    p = lmfit.Parameters()  # object to store L-M fit Parameters           # Parameter Name\n",
    "    p.add('cosinc', value=np.cos(inc), vary=0)                # inclination, vary cos(inclin)\n",
    "    p.add('rho_star', value=rho_star, vary=0)                # stellar density\n",
    "    p.add('a_Rs', value=a_Rs, vary=0)                # a/Rstar\n",
    "    p.add('rprs', value=rp, vary=1, min=0, max=1)  # planet-to-star radius ratio\n",
    "    p.add('t0', value=t0, vary=0)                # Transit T0\n",
    "    p.add('f0', value=np.mean(y[outtransit]), vary=1, min=0)         # Baseline Flux\n",
    "    p.add('ecc', value=ecc, vary=0, min=0, max=1) # eccentricity\n",
    "    p.add('omega', value=omega, vary=0)                # arguments of periatron\n",
    "    # Turn on a linear slope in water feature to account for presumably changing H2O ice builtup on widow during cryogenic test\n",
    "    if wave1 > 2.7 and wave1 < 3.45:\n",
    "        p.add('Fslope', value=regressor.coef_[6], vary=1)                # Orbital phase\n",
    "    else:\n",
    "        p.add('Fslope', value=0, vary=0)                          # Orbital phase\n",
    "    p.add('xsh', value=regressor.coef_[0], vary=1)                 # Detector X-shift detrending\n",
    "    p.add('ysh', value=regressor.coef_[1], vary=1)                 # Detector X-shift detrending\n",
    "    p.add('x2sh', value=regressor.coef_[2], vary=1)                # Detector X^2-shift detrending\n",
    "    p.add('y2sh', value=regressor.coef_[3], vary=1)                # Detector Y^2-shift detrending\n",
    "    p.add('xysh', value=regressor.coef_[4], vary=1)                # Detector X*Y detrending\n",
    "    p.add('comm', value=regressor.coef_[5], vary=1)                # Common-Mode detrending\n",
    "    \n",
    "    # Perform Minimization https://lmfit.github.io/lmfit-py/fitting.html\n",
    "    # create Minimizer\n",
    "    # mini = lmfit.Minimizer(residual, p, nan_policy='omit',fcn_args=(phase,x,y,err)\n",
    "    print('')\n",
    "    print('Fitting Bin', bin, ' Wavelength =', np.mean(wsdata)/1E4, '  Range= [', wave1, ':', wave2, ']')\n",
    "\n",
    "    #  solve with Levenberg-Marquardt using the\n",
    "    result = lmfit.minimize(residual, params=p, args=(phase, x, y, err, c1, c2, c3, c4))\n",
    "    # result = mini.minimize(method='emcee')\n",
    "\n",
    "    print('')\n",
    "    print('Re-Fitting Bin', bin, ' Wavelength =', np.mean(wsdata)/1E4, '  Range= [', wave1, ':', wave2, ']')\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\")\n",
    "    print(\"redchi\", result.redchi)\n",
    "    print(\"chi2\", result.chisqr)\n",
    "    print(\"nfree\", result.nfree)\n",
    "    print(\"bic\", result.bic)\n",
    "    print(\"aic\", result.aic)\n",
    "    print(\"L-M FIT Variable\")\n",
    "    print(lmfit.fit_report(result.params))\n",
    "    text_file = open(save_directory+'JWST_NIRSpec_Prism_fit_light_curve_bin'+str(bin)+'_statistics.txt', \"w\")\n",
    "    n = text_file.write(\"\\nredchi \"+str(result.redchi))\n",
    "    n = text_file.write(\"\\nchi2   \"+str(result.chisqr))\n",
    "    n = text_file.write(\"\\nnfree  \"+str(result.nfree))\n",
    "    n = text_file.write(\"\\nbic    \"+str(result.bic))\n",
    "    n = text_file.write(\"\\naic    \"+str(result.aic))\n",
    "    n = text_file.write(lmfit.fit_report(result.params))\n",
    "    # file-output.py\n",
    "\n",
    "    # Update with best-fit parameters\n",
    "    p['rho_star'].value = result.params['rho_star'].value\n",
    "    p['cosinc'].value = result.params['cosinc'].value\n",
    "    p['rprs'].value = result.params['rprs'].value\n",
    "    p['t0'].value = result.params['t0'].value\n",
    "    p['f0'].value = result.params['f0'].value\n",
    "    p['Fslope'].value = result.params['Fslope'].value\n",
    "    p['xsh'].value = result.params['xsh'].value\n",
    "    p['ysh'].value = result.params['ysh'].value\n",
    "    p['x2sh'].value = result.params['x2sh'].value\n",
    "    p['y2sh'].value = result.params['y2sh'].value\n",
    "    p['xysh'].value = result.params['xysh'].value\n",
    "    p['comm'].value = result.params['comm'].value\n",
    "    # Update Fit Spectra arrays\n",
    "    wsd[bin] = np.mean(wsdata)/1E4\n",
    "    werr[bin] = (wsdata.max()-wsdata.min())/2E4\n",
    "    rprs[bin] = result.params['rprs'].value\n",
    "    rerr[bin] = result.params['rprs'].stderr\n",
    "\n",
    "    # Calculate Bestfit Model\n",
    "    final_model = y-result.residual*err\n",
    "    final_model_fine = model_fine(p)\n",
    "\n",
    "    # More Stats\n",
    "    resid = (y-final_model)/p['f0'].value\n",
    "    residppm = 1E6*(y-final_model)/p['f0'].value\n",
    "    residerr = err/p['f0'].value\n",
    "    sigma = np.std((y-final_model)/p['f0'].value)*1E6\n",
    "    print(\"Residual standard deviation  (ppm) : \", 1E6*np.std((y-final_model)/p['f0'].value))\n",
    "    print(\"Photon noise                 (ppm) : \", (1/np.sqrt(p['f0'].value))*1E6)\n",
    "    print(\"Photon noise performance       (%) : \", (1/np.sqrt(p['f0'].value))*1E6 / (sigma) * 100)\n",
    "    n = text_file.write(\"\\nResidual standard deviation  (ppm) : \"+str(1E6*np.std((y-final_model)/p['f0'].value)))\n",
    "    n = text_file.write(\"\\nPhoton noise                 (ppm) : \"+str((1/np.sqrt(p['f0'].value))*1E6))\n",
    "    n = text_file.write(\"\\nPhoton noise performance       (%) : \"+str((1/np.sqrt(p['f0'].value))*1E6 / (sigma) * 100))\n",
    " \n",
    "    # Measure Rednoise with Binning Technique\n",
    "    sig0 = np.std(resid)\n",
    "    bins = number_of_images / binmeasure\n",
    "    wsb, wsb_bin_edges, binnumber = stats.binned_statistic(bjd, resid, bins=bins)\n",
    "    sig_binned = np.std(wsb)\n",
    "    sigrednoise = np.sqrt(sig_binned**2-sig0**2/binmeasure)\n",
    "    if np.isnan(sigrednoise):\n",
    "        sigrednoise = 0   # if no rednoise detected, set to zero\n",
    "    sigwhite = np.sqrt(sig0**2-sigrednoise**2)\n",
    "    sigrednoise = np.sqrt(sig_binned**2-sigwhite**2/binmeasure)\n",
    "    if np.isnan(sigrednoise): \n",
    "        sigrednoise = 0   # if no rednoise detected, set to zero\n",
    "    beta[bin] = np.sqrt(sig0**2+binmeasure*sigrednoise**2)/sig0\n",
    "    \n",
    "    print(\"White noise                  (ppm) : \", 1E6*sigwhite)\n",
    "    print(\"Red noise                    (ppm) : \", 1E6*sigrednoise)\n",
    "    print(\"Transit depth measured error (ppm) : \", 2E6*result.params['rprs'].value*result.params['rprs'].stderr)\n",
    "    \n",
    "    n = text_file.write(\"\\nWhite noise                  (ppm) : \"+str(1E6*sigwhite))\n",
    "    n = text_file.write(\"\\nRed noise                    (ppm) : \"+str(1E6*sigrednoise))\n",
    "    n = text_file.write(\"\\nTransit depth measured error (ppm) : \"+str(2E6*result.params['rprs'].value*result.params['rprs'].stderr))\n",
    "    text_file.close()\n",
    "    depth[bin] = 1E6*result.params['rprs'].value**2\n",
    "    depth_err[bin] = 2E6*result.params['rprs'].value*result.params['rprs'].stderr\n",
    "\n",
    "    sig_r[bin] = sigrednoise*1E6\n",
    "    sig_w[bin] = sigwhite*1E6\n",
    "    # --------------------------------------------------------------------------\n",
    "    # ---------------------------------------------------------\n",
    "    # Write Fit Spectra to ascii file\n",
    "    ascii_data = Table([wsd, werr, rprs, rerr, depth, depth_err, sig_w, sig_r, beta], names=['Wavelength Center (um)', 'Wavelength half-width (um)', 'Rp/Rs', 'Rp/Rs 1-sigma error', 'Transit Depth (ppm)', 'Transit Depth error', 'Sigma_white (ppm)', 'Sigma_red (ppm)', 'Beta Rednoise Inflation factor'])\n",
    "    ascii.write(ascii_data, save_directory+'JWST_NIRSpec_Prism_fit_transmission_spectra.csv', format='csv', overwrite=True)\n",
    "    # ---------------------------------------------------------\n",
    "    msize = plt.rcParams['lines.markersize'] ** 2. # default marker size\n",
    "    # Plot data models\n",
    "     \n",
    "    # plot\n",
    "    plt.rcParams['figure.figsize'] = [10.0, 7.0]           # Figure dimensions\n",
    "    msize = plt.rcParams['lines.markersize'] ** 2.           # default marker size\n",
    "    fig = plt.figure(constrained_layout=True)\n",
    "    gs = fig.add_gridspec(3, 1, hspace=0.00, wspace=0.00)\n",
    "    ax1 = fig.add_subplot(gs[0:2, :])\n",
    "    ax1.scatter(x, y/p['f0'].value, s=msize*0.75, linewidth=1, zorder=0, alpha=0.5, marker='+', edgecolors='blue')\n",
    "    ax1.plot(x, final_model/p['f0'].value, linewidth=1, color='orange', alpha=0.8, zorder=15)  # overplot Transit model at data\n",
    "    ax1.xaxis.set_ticklabels([])\n",
    "    plt.ylabel('Relative Flux')\n",
    "    plt.title(r'Time-series Transit Light Curve  $\\lambda=$['+str(wave1)+':'+str(wave2) + r'] $\\mu$m')\n",
    "    ax1.xaxis.set_major_locator(ticker.MultipleLocator(0.01))\n",
    "    ax1.xaxis.set_minor_locator(ticker.MultipleLocator(0.005))\n",
    "    ax1.yaxis.set_major_locator(ticker.MultipleLocator(0.002))\n",
    "    ax1.yaxis.set_minor_locator(ticker.MultipleLocator(0.001))\n",
    "    yplot = y/np.mean(y[outtransit])\n",
    "    plt.ylim(yplot.min()*0.999, yplot.max()*1.001)\n",
    "    plt.xlim(bjd.min()-0.001, bjd.max()+0.001)\n",
    "    fig.add_subplot(ax1)\n",
    "    # Residual\n",
    "    ax2 = fig.add_subplot(gs[2, :])\n",
    "    ax2.scatter(x, residppm, s=msize*0.75, linewidth=1, alpha=0.5, marker='+', edgecolors='blue', zorder=0)  # overplot Transit model at data\n",
    "    wsb, wsb_bin_edges, binnumber = stats.binned_statistic(bjd, residppm, bins=256)\n",
    "    plt.scatter(wsb_bin_edges[1:], wsb, linewidth=2, alpha=0.75, facecolors='orange', edgecolors='none', marker='o', zorder=25)\n",
    "    plt.xlabel('Barycentric Julian Date (days)')\n",
    "    plt.ylabel('Residual (ppm)')\n",
    "    plt.plot([bjd.min(), bjd.max()], [0, 0], color='black', zorder=10)\n",
    "    plt.plot([bjd.min(), bjd.max()], [sigma, sigma], linestyle='--', color='black', zorder=15)\n",
    "    plt.plot([bjd.min(), bjd.max()], [-sigma, -sigma], linestyle='--', color='black', zorder=20)\n",
    "    ax2.xaxis.set_major_locator(ticker.MultipleLocator(0.01))\n",
    "    ax2.xaxis.set_minor_locator(ticker.MultipleLocator(0.005))\n",
    "    yplot = y/np.mean(y[outtransit])\n",
    "    plt.xlim(bjd.min()-0.001, bjd.max()+0.001)\n",
    "    fig.add_subplot(ax2)\n",
    "    # save\n",
    "    pp = PdfPages(save_directory+'JWST_NIRSpec_Prism_fit_light_curve_bin'+str(bin)+'_lightcurve.pdf')\n",
    "    plt.savefig(pp, format='pdf')\n",
    "    pp.close()\n",
    "    plt.clf()   \n",
    "    \n",
    "    # plot systematic corrected light curve\n",
    "    b0 = p['a_Rs'].value * np.sqrt((np.sin(phase * 2 * np.pi)) ** 2 + (p['cosinc'].value * np.cos(phase * 2 * np.pi)) ** 2)\n",
    "    intransit = (b0-p['rprs'].value < 1.0E0).nonzero()\n",
    "    light_curve = b0/b0\n",
    "    mulimb0, mulimbf = occultnl(p['rprs'].value, c1, c2, c3, c4, b0[intransit])  # Madel and Agol\n",
    "    light_curve[intransit] = mulimb0\n",
    "    fig, axs = plt.subplots()\n",
    "    plt.scatter(x, light_curve+resid, s=msize*0.75, linewidth=1, zorder=0, alpha=0.5, marker='+', edgecolors='blue')\n",
    "    plt.xlabel('BJD')\n",
    "    plt.ylabel('Relative Flux')\n",
    "    plt.plot(x, light_curve, linewidth=2, color='orange', alpha=0.8, zorder=15)  # overplot Transit model at data\n",
    "    pp = PdfPages(save_directory+'JWST_NIRSpec_Prism_fit_light_curve_bin'+str(bin)+'_corrected.pdf')\n",
    "    plt.savefig(pp, format='pdf')\n",
    "    pp.close()\n",
    "    plt.clf()\n",
    "    plt.close('all') # close all figures\n",
    "    # --------------------------------------------------------------------------\n",
    "    k = k + wk  # step wavelength index to next bin\n",
    "    \n",
    "    print('** Can Now View Output PDFs in ', save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Measured Exoplanet Transmission Spectrum vs Injected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Load Injected Transmission spectra to compare with recovered value\n",
    "\n",
    "# Download Injected Spectra\n",
    "fn_tm = download_file(r'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/transit_spectroscopy_notebook/trans-iso_GJ-0436_0669.0_+0.0_0.56_0001_0.00_model.NIRSpec_PRISM.txt')\n",
    "destld = shutil.move(fn_tm, save_directory+'trans-iso_GJ-0436_0669.0_+0.0_0.56_0001_0.00_model.NIRSpec_PRISM.txt')        \n",
    "\n",
    "f = open(save_directory+'trans-iso_GJ-0436_0669.0_+0.0_0.56_0001_0.00_model.NIRSpec_PRISM.txt', 'r')\n",
    "data = np.genfromtxt(f, delimiter='   ')\n",
    "model_ws = data[:, 0]\n",
    "model_spec = data[:, 1]\n",
    "\n",
    "# Read fit transit depths\n",
    "data = ascii.read(save_directory+'JWST_NIRSpec_Prism_fit_transmission_spectra.csv', format='csv')\n",
    "wsd = data['Wavelength Center (um)']\n",
    "werr = data['Wavelength half-width (um)']\n",
    "rprs = data['Rp/Rs']\n",
    "rerr = data['Rp/Rs 1-sigma error']\n",
    "beta = data['Beta Rednoise Inflation factor']\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots()\n",
    "plt.plot(model_ws, model_spec**2*1E6, linewidth=2, zorder=0, color='blue', label='Injected Spectra')  # overplot Transit model at data\n",
    "plt.errorbar(wsd, rprs**2*1E6, xerr=werr, yerr=2*rerr*rprs*1E6*beta, fmt='o', zorder=5, alpha=0.4, color='orange', label=r'Recovered Spectra with $\\sigma_r$')\n",
    "plt.errorbar(wsd, rprs**2*1E6, xerr=werr, yerr=2*rerr*rprs*1E6, fmt='o', zorder=10, color='orange', label='Recovered Spectra')\n",
    "plt.xlabel(r'Wavelength ($\\mu$m)')\n",
    "plt.ylabel('Transit Depth ($R_p/R_s$)$^2$ (ppm)')\n",
    "axs.yaxis.set_major_locator(ticker.MultipleLocator(200))\n",
    "axs.yaxis.set_minor_locator(ticker.MultipleLocator(100))\n",
    "axs.xaxis.set_major_locator(ticker.MultipleLocator(0.5))\n",
    "axs.xaxis.set_minor_locator(ticker.MultipleLocator(0.1))\n",
    "axs.text(3.3, 6850, 'CH$_4$')\n",
    "axs.text(4.25, 6700, 'CO')\n",
    "axs.text(2.3, 6750, 'CH$_4$')\n",
    "axs.text(2.75, 6550, 'H$_2$O')\n",
    "plt.ylim(5700, 7000)\n",
    "plt.xlim(0.9, 5.25)\n",
    "plt.legend(loc='lower right')    \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By and large, the injected transit depths are well recovered across the spectra, with features such as H$_2$O and CH$_4$ easily detected. There is a bit of an offset in data-points long-ward of 3.5 $\\mu$m that could perhaps be due to changes in CO$_2$ or H$_2$O absorption features from ice built up on the cryogenic window during the CV3 test. These wavelengths show some increases in time correlated noise ($\\sigma_r$), which has been measured here, and the errors in the plot also show the transit depths with this error included. \n",
    "\n",
    "The precisions from the ground-based test are very encouraging, with the best measured bin (which occurs in a clean part of the spectrum with high count rates) achieving near-photon limited transit depths measured to about 30 ppm in only 2 hours of data, and with minimal time correlated noise ($\\sigma_r$).\n",
    "\n",
    "For more robust error estimates, in practice the least-squares minimization performed here would be replaced by an MCMC routine.  In addition, with actual transit data, the transit fit parameters (e.g. $i$, $a/R_{star}$, T$_0$) would also have to be first fit as well, as they can/will differ from literature estimates in high precision transit light curves as JWST will provide.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
