{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"top\"></a>\n",
    "# MIRI MRS Spectroscopy of a Late M Star 1 - Running the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use case:** Extract spatial-spectral features from IFU cube and measure their attributes.<br>\n",
    "**Data:** Simulated [MIRI MRS](https://jwst-docs.stsci.edu/mid-infrared-instrument/miri-observing-modes/miri-medium-resolution-spectroscopy) spectrum of AGB star.<br>\n",
    "**Source of Simulations:** [MIRISim](https://www.stsci.edu/jwst/science-planning/proposal-planning-toolbox/mirisim)<br>\n",
    "**Pipeline Version:** [JWST Pipeline](https://jwst-docs.stsci.edu/jwst-data-reduction-pipeline)<br>\n",
    "**Tools:** specutils, jwst, photutils, astropy, aplpy, scipy.<br>\n",
    "**Cross-intrument:** NIRSpec, MIRI.<br>\n",
    "**Documentation:** This notebook is part of a STScI's larger [post-pipeline Data Analysis Tools Ecosystem](https://jwst-docs.stsci.edu/jwst-post-pipeline-data-analysis) and can be [downloaded](https://github.com/spacetelescope/dat_pyinthesky/tree/main/jdat_notebooks/MRS_Mstar_analysis) directly from the [JDAT Notebook Github directory](https://github.com/spacetelescope/jdat_notebooks).<br>\n",
    "\n",
    "**Note**: This notebook includes MIRI simulated data cubes obtained using MIRISim (https://wiki.miricle.org//bin/view/Public/MIRISim_Public)\n",
    "and run through the JWST pipeline (https://jwst-pipeline.readthedocs.io/en/latest/) of\n",
    "point sources with spectra representative of late M type stars.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook analyzes one star represented by a dusty SED corresponding to the ISO SWS spectrum of\n",
    "W Per from Kraemer et al. (2002) and Sloan et al. (2003) to cover the MRS spectral range 5-28 microns.  Analysis of JWST spectral cubes requires extracting spatial-spectral features of interest and measuring their attributes.\n",
    "\n",
    "This is the first notebook, which will process the data and automatically detect and extract spectra for the point source.  The workflow will use `photutils` to automatically detect sources in the cube to extract the spectrum of the point source. Then it will read in the spectra generated at Stage 3 of the JWST pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import useful python packages\n",
    "import numpy as np\n",
    "\n",
    "# Import packages to display images inline in the notebook\n",
    "import matplotlib.pyplot as plt    \n",
    "%matplotlib inline   \n",
    "\n",
    "# Set general plotting options\n",
    "params = {'legend.fontsize': '18', 'axes.labelsize': '18', \n",
    "         'axes.titlesize': '18', 'xtick.labelsize': '18', \n",
    "         'ytick.labelsize': '18', 'lines.linewidth': 2, 'axes.linewidth': 2, 'animation.html': 'html5'}\n",
    "plt.rcParams.update(params)\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import astropy packages \n",
    "from astropy import units as u\n",
    "from astropy.io import ascii\n",
    "from astropy.wcs import WCS\n",
    "from astropy.table import Table, vstack\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from astropy.nddata import StdDevUncertainty\n",
    "from astropy.io import fits # added by BAS on 8 April 2021\n",
    "from astropy.utils.data import get_pkg_data_filename\n",
    "\n",
    "# To find stars in the MRS spectralcubes and do aperture photometry\n",
    "from photutils import DAOStarFinder, CircularAperture\n",
    "\n",
    "# To deal with 1D spectrum\n",
    "from specutils import Spectrum1D\n",
    "from specutils.fitting import fit_generic_continuum\n",
    "from specutils.manipulation import box_smooth, extract_region, SplineInterpolatedResampler\n",
    "from specutils.analysis import line_flux, centroid, equivalent_width\n",
    "from specutils.spectra import SpectralRegion\n",
    "from specutils import SpectrumList\n",
    "\n",
    "# To make nice plots with WCS axis\n",
    "import aplpy\n",
    "\n",
    "# To fit a curve to the data\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths to the Data and Outputs\n",
    "\n",
    "Use MIRISim JWST pipeline processed data in future iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pipeline\n",
    "\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "from jwst.extract_1d import Extract1dStep\n",
    "import json\n",
    "import glob\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base\n",
    "from jwst.associations import asn_from_list\n",
    "import crds\n",
    "from jdaviz.app import Application\n",
    "import asdf\n",
    "from photutils import aperture_photometry\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data if you don't already have it.\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "if os.path.exists(\"20210413_120546_mirisim.tar.gz\"):\n",
    "    print(\"20210413_120546_mirisim.tar.gz Exists\")\n",
    "else:\n",
    "    url = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/MRS_Mstar_analysis/20210413_120546_mirisim.tar.gz'\n",
    "    urllib.request.urlretrieve(url, './20210413_120546_mirisim.tar.gz')\n",
    "    \n",
    "if os.path.exists(\"20210413_123047_mirisim.tar.gz\"):\n",
    "    print(\"20210413_123047_mirisim.tar.gz Exists\")\n",
    "else:\n",
    "    url = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/MRS_Mstar_analysis/20210413_123047_mirisim.tar.gz'\n",
    "    urllib.request.urlretrieve(url, './20210413_123047_mirisim.tar.gz')\n",
    "    \n",
    "if os.path.exists(\"20210413_125354_mirisim.tar.gz\"):\n",
    "    print(\"20210413_125354_mirisim.tar.gz Exists\")\n",
    "else:\n",
    "    url = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/MRS_Mstar_analysis/20210413_125354_mirisim.tar.gz'\n",
    "    urllib.request.urlretrieve(url, './20210413_125354_mirisim.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip Tar Files\n",
    "\n",
    "import tarfile\n",
    "\n",
    "# Unzip files if they haven't already been unzipped\n",
    "if os.path.exists(\"20210413_120546_mirisim/\"):\n",
    "    print(\"20210413_120546_mirisim Exists\")\n",
    "else:\n",
    "    tar = tarfile.open('./20210413_120546_mirisim.tar.gz', \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    \n",
    "if os.path.exists(\"20210413_123047_mirisim/\"):\n",
    "    print(\"20210413_123047_mirisim Exists\")\n",
    "else:\n",
    "    tar = tarfile.open('./20210413_123047_mirisim.tar.gz', \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    \n",
    "if os.path.exists(\"20210413_125354_mirisim/\"):\n",
    "    print(\"20210413_125354_mirisim Exists\")\n",
    "else:\n",
    "    tar = tarfile.open('./20210413_125354_mirisim.tar.gz', \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline\n",
    "\n",
    "The various [stages of the pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/main.html#pipelines) can be [run within Python](https://jwst-pipeline.readthedocs.io/en/latest/jwst/introduction.html#running-from-within-python).  For a more in depth tutorial on running the pipelines, check out the [JWebbinars](https://www.stsci.edu/jwst/science-execution/jwebbinars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute calwebb_detector1 pipeline on raw simulation output.  This will overwrite previous reductions.\n",
    "\n",
    "allshortfiles = glob.glob('20210413_*_mirisim/det_images/*MIRIFUSHORT*fits')\n",
    "alllongfiles = glob.glob('20210413_*_mirisim/det_images/*MIRIFULONG*fits')\n",
    "      \n",
    "pipe1short = Detector1Pipeline()\n",
    "\n",
    "# run calwebb_detector1 on the MIRIFUSHORT data separate from MIRIFULONG data, as it saves time this way\n",
    "for shortfile in allshortfiles:\n",
    "    print(shortfile)\n",
    "    baseshort, remaindershort = shortfile.split('.')\n",
    "    \n",
    "    # If you run your own simulations, you will need to update these hardcoded files.\n",
    "    beforestuffshort, dateafterstuffshort = shortfile.split('20210413_')    \n",
    "    datestringshort, afterstuffshort = dateafterstuffshort.split('_mirisim')\n",
    "    \n",
    "    pipe1short.refpix.skip = True\n",
    "    pipe1short.output_file = baseshort + datestringshort\n",
    "    \n",
    "    pipe1short.run(shortfile)\n",
    "\n",
    "pipe1long = Detector1Pipeline()\n",
    "\n",
    "for longfile in alllongfiles:\n",
    "    print(longfile)\n",
    "    baselong, remainderlong = longfile.split('.')\n",
    "    \n",
    "    # If you run your own simulations, you will need to update these hardcoded files.\n",
    "    beforestufflong, dateafterstufflong = longfile.split('20210413_')\n",
    "    datestringlong, afterstufflong = dateafterstufflong.split('_mirisim')\n",
    "    \n",
    "    pipe1long.refpix.skip = True\n",
    "    pipe1long.output_file = baselong + datestringlong\n",
    "    \n",
    "    pipe1long.run(longfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute calwebb_spec2 pipeline. This will overwrite previous reductions.\n",
    "\n",
    "# All the local calwebb_detector1 files\n",
    "allshortfiles2 = glob.glob('det_image_*_MIRIFUSHORT_*_rate.fits')\n",
    "alllongfiles2 = glob.glob('det_image_*_MIRIFULONG_*_rate.fits')\n",
    "\n",
    "for short2file in allshortfiles2:\n",
    "    print(short2file)\n",
    "    pipe2short = Spec2Pipeline()\n",
    "    base2short, remainder2short = short2file.split('.')\n",
    "    \n",
    "    pipe2short.straylight.skip = True\n",
    "    \n",
    "    # If you run your own simulations, you will need to update these hardcoded files.\n",
    "    if (short2file == 'det_image_seq1_MIRIFUSHORT_12LONGexp1125354_rate.fits'):\n",
    "        print('this one will have the level 2b cube built')\n",
    "    else:\n",
    "        pipe2short.cube_build.skip = True\n",
    "    pipe2short.extract_1d.skip = True\n",
    "    pipe2short.output_file = base2short\n",
    "        \n",
    "    pipe2short.run(short2file)\n",
    "\n",
    "for long2file in alllongfiles2:\n",
    "    print(long2file)\n",
    "    pipe2long = Spec2Pipeline()\n",
    "    base2long, remainder2long = long2file.split('.')\n",
    "    \n",
    "    pipe2long.straylight.skip = True\n",
    "    # If you run your own simulations, you will need to update these hardcoded files.\n",
    "    if (long2file == 'det_image_seq1_MIRIFULONG_34SHORTexp1120546_rate.fits'):\n",
    "        print('this one will have the level 2b cube built')\n",
    "    else:\n",
    "        pipe2long.cube_build.skip = True\n",
    "    pipe2long.extract_1d.skip = True\n",
    "    pipe2long.output_file = base2long\n",
    "    \n",
    "    pipe2long.run(long2file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to detect the point source in the datacube and extract and plot the spectra for each source\n",
    "\n",
    "For data cubes like the JWST/MIRI MRS information on the point sources in the FOV and also obtaining a source subtracted\n",
    " data cube will be necessary (See the `PampelMuse` software for an example on how spectral extraction is implemented for\n",
    "  near-IR data cubes like MUSE).\n",
    "\n",
    "Note these backgrounds of diffuse emission can be quite complex.\n",
    "\n",
    "On these source extracted data cubes (see `SUBTRES` in `PampelMuse`) I would like to produce moment maps\n",
    "(https://casa.nrao.edu/Release3.4.0/docs/UserMan/UserManse41.html) and Position-Velocity (PV) diagrams\n",
    "(https://casa.nrao.edu/Release4.1.0/doc/UserMan/UserManse42.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Use `Photutils` to detect stars/point sources in the continuum image\n",
    "\n",
    "The first step of the analysis is to identify those sources for which it is feasible to extract spectra from the IFU\n",
    "data. Ideally we can estimate the signal-to-noise ratio (S/N) for all sources in the cube, do a number of checks to\n",
    "determine the status of every source and loop through these (brightest first) to extract the spectra.  Open up the Level 2 Cubes and use photutils to search for point sources for Level 3 extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run your own simulations, you will need to update these hardcoded files.\n",
    "l_cube_file = 'det_image_seq1_MIRIFULONG_34SHORTexp1120546_s3d.fits'\n",
    "s_cube_file = 'det_image_seq1_MIRIFUSHORT_12LONGexp1125354_s3d.fits'\n",
    "\n",
    "with fits.open(s_cube_file) as hdu_s_cube:\n",
    "    s_cube = hdu_s_cube['SCI'].data\n",
    "    s_med_cube = np.zeros((s_cube.shape[1], s_cube.shape[2]))\n",
    "    for a in range(s_cube.shape[1]):\n",
    "        for b in range(s_cube.shape[2]):\n",
    "            s_med_cube[a,b] = np.median(s_cube[:,a,b])\n",
    "\n",
    "mean, median, std = sigma_clipped_stats(s_med_cube, sigma = 2.0)\n",
    "\n",
    "# Get a list of sources using a dedicated source detection algorithm\n",
    "# Find sources at least 3* background (typically)\n",
    "\n",
    "daofind = DAOStarFinder(fwhm = 2.0, threshold = 3. * std)\n",
    "sources = daofind(s_med_cube - median) \n",
    "print(\"\\n Number of sources in field: \", len(sources))\n",
    "\n",
    "# Positions in pixels\n",
    "positions = Table([sources['xcentroid'], sources['ycentroid']])\n",
    "\n",
    "# Convert to RA & Dec (ICRS)\n",
    "peakpixval = np.zeros(len(sources['xcentroid']))\n",
    "for count_s, _ in enumerate(sources):\n",
    "    peakpixval[count_s] = s_med_cube[int(np.round(sources['xcentroid'][count_s])), int(np.round(sources['ycentroid'][count_s]))]\n",
    "print('peak pixel x =')\n",
    "print(sources['xcentroid'][np.argmax(peakpixval)])\n",
    "print('peak pixel y =')\n",
    "print(sources['ycentroid'][np.argmax(peakpixval)])\n",
    "\n",
    "plt.imshow(s_med_cube, vmin=0, vmax=100)#.value)\n",
    "plt.tight_layout()\n",
    "plt.scatter(sources['xcentroid'], sources['ycentroid'], c = \"red\", marker = \"+\", s=50)\n",
    "plt.scatter(sources['xcentroid'][np.argmax(peakpixval)], sources['ycentroid'][np.argmax(peakpixval)], c = 'black', marker='+', s=50)\n",
    "plt.show()\n",
    "\n",
    "f0 = fits.open(s_cube_file)\n",
    "w0 = WCS(f0[('sci',1)].header, f0)\n",
    "f0.close()\n",
    "\n",
    "radec = w0.all_pix2world([sources['xcentroid'][np.argmax(peakpixval)]], [sources['ycentroid'][np.argmax(peakpixval)]], [1], 1)\n",
    "\n",
    "# Take the brightest source flux and take that to be your primary point source for extraction\n",
    "ra_ptsrc = radec[0][0]\n",
    "dec_ptsrc = radec[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the way the pipeline currently extracts Level3 data, you must update the headers to be centered on the point source of your choosing from the step above.\n",
    "all_files = glob.glob('det_image_*_cal.fits')\n",
    "targra = ra_ptsrc\n",
    "targdec = dec_ptsrc\n",
    "for thisfile in all_files:\n",
    "    base, remainder = thisfile.split('.')\n",
    "    outfilename = base + '_fix.' + remainder\n",
    "    print(outfilename)\n",
    "    \n",
    "    with fits.open(thisfile) as hduthis:\n",
    "        hduthis['SCI'].header['SRCTYPE'] = 'POINT'\n",
    "        hduthis[0].header['TARG_RA'] = targra\n",
    "        hduthis[0].header['TARG_DEC'] = targdec\n",
    "        hduthis.writeto(outfilename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up needed reference file(s) for spec3\n",
    "\n",
    "file_all_list = glob.glob('det_image_*_cal_fix.fits')\n",
    "\n",
    "asnall = asn_from_list.asn_from_list(file_all_list, rule=DMS_Level3_Base, product_name='combine_dithers_all_exposures')\n",
    "\n",
    "asnallfile = 'for_spec3_all.json'\n",
    "with open(asnallfile, 'w') as fpall:\n",
    "    fpall.write(asnall.dump()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute calwebb_spec3 pipeline.  This will overwrite previous reductions.\n",
    "\n",
    "pipe3ss = Spec3Pipeline()\n",
    "pipe3ss.master_background.skip = True\n",
    "pipe3ss.mrs_imatch.skip = True\n",
    "pipe3ss.outlier_detection.skip = True\n",
    "pipe3ss.resample_spec.skip = True\n",
    "pipe3ss.combine_1d.skip = True\n",
    "pipe3ss.use_source_posn = 'True'\n",
    "pipe3ss.subtract_background = 'True'\n",
    "pipe3ss.output_file = 'allspec3'\n",
    "pipe3ss.run(asnallfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Proceed to Notebook 2 for visualization and data anlysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [PampelMuse](https://gitlab.gwdg.de/skamann/pampelmuse)\n",
    "- [CASA](https://casa.nrao.edu/Release3.4.0/docs/UserMan/UserManse41.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this notebook\n",
    "**Author:** Olivia Jones, Project Scientist, UK ATC.\n",
    "**Updated On:** 2020-08-11\n",
    "**Later Updated On:** 2021-09-06 by B. Sargent, STScI Scientist, Space Telescope Science Institute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top of Page](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
