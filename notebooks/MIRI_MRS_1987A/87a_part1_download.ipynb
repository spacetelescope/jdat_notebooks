{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298b8519-c695-4562-a96c-96208063c1c3",
   "metadata": {},
   "source": [
    "# MIRI MRS IFU Spectroscopy Part 1: \n",
    "# Downloading Data\n",
    "\n",
    "Aug 2023\n",
    "\n",
    "**Use case:** Reduce MRS Data With User Defined Master Background Step. This is particularly relevant if you did not obtain a Dedicated Background with your observations. While the pipeline will subtract a sky background derived from an annulus, the underlying background may be prohibitively complicated and the user may wish to measure their own background from elsewhere in the cube.<br>\n",
    "**Data:** Publicly available science data for SN 1987A (Program 1232). For this notebook, we will follow the science workflow outlined by [Jones et al. 2023](https://ui.adsabs.harvard.edu/abs/2023arXiv230706692J/abstract).<br>\n",
    "**Tools:** jwst, jdaviz, matplotlib, astropy.<br>\n",
    "**Cross-intrument:** NIRSpec, MIRI.<br>\n",
    "**Documentation:** This notebook is part of a STScI's larger [post-pipeline Data Analysis Tools Ecosystem](https://jwst-docs.stsci.edu/jwst-post-pipeline-data-analysis) and can be [downloaded](https://github.com/spacetelescope/dat_pyinthesky/tree/main/jdat_notebooks/MRS_Mstar_analysis) directly from the [JDAT Notebook Github directory](https://github.com/spacetelescope/jdat_notebooks).<br>\n",
    "\n",
    "### Introduction: Spectral extraction in the JWST calibration pipeline\n",
    "\n",
    "The JWST calibration pipeline performs spectrac extraction for all spectroscopic data using basic default assumptions that are tuned to produce accurately calibrated spectra for the majority of science cases. This default method is a simple fixed-width boxcar extraction, where the spectrum is summed over a number of pixels along the cross-dispersion axis, over the valid wavelength range. An aperture correction is applied at each pixel along the spectrum to account for flux lost from the finite-width aperture. \n",
    "\n",
    "The ``extract_1d`` step uses the following inputs for its algorithm:\n",
    "- the spectral extraction reference file: this is a json-formatted file, available as a reference file from the [JWST CRDS system](https://jwst-crds.stsci.edu)\n",
    "- the bounding box: the ``assign_wcs`` step attaches a bounding box definition to the data, which defines the region over which a valid calibration is available. We will demonstrate below how to visualize this region. \n",
    "\n",
    "However the ``extract_1d`` step has the capability to perform more complex spectral extractions, requiring some manual editing of parameters and re-running of the pipeline step. \n",
    "\n",
    "\n",
    "### Aims\n",
    "\n",
    "This notebook will demonstrate how to re-run the spectral extraction step with different settings to illustrate the capabilities of the JWST calibration pipeline. \n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "We will demonstrate the spectral extraction methods on resampled, calibrated spectral images. The basic demo and two examples run on Level 3 data, in which the nod exposures have been combined into a single spectral image. Two examples will use the Level 2b data - one of the nodded exposures. \n",
    "\n",
    "\n",
    "### Test data\n",
    "\n",
    "The data used in this notebook is an observation of the Type Ia supernova SN2021aefx, observed by Jha et al in PID 2072 (Obs 1). These data were taken with zero exclusive access period, and published in [Kwok et al 2023](https://ui.adsabs.harvard.edu/abs/2023ApJ...944L...3K/abstract). You can retrieve the data from [this Box folder](https://stsci.box.com/s/i2xi18jziu1iawpkom0z2r94kvf9n9kb), and we recommend you place the files in the ``data/`` folder of this repository, or change the directory settings in the notebook prior to running. \n",
    "\n",
    "You can of course use your own data instead of the demo data. \n",
    "\n",
    "\n",
    "### JWST pipeline version and CRDS context\n",
    "\n",
    "This notebook was written using the calibration pipeline version 1.10.2. We set the CRDS context explicitly to 1089 to match the current latest version in MAST. If you use different pipeline versions or CRDS context, please read the relevant release notes ([here for pipeline](https://github.com/spacetelescope/jwst), [here for CRDS](https://jwst-crds.stsci.edu)) for possibly relevant changes.\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. [The Level 3 data products](#l3data)\n",
    "2. [The spectral extraction reference file](#x1dref)\n",
    "3. [Example 1: Changing the aperture width](#ex1)\n",
    "4. [Example 2: Changing the aperture location](#ex2)\n",
    "5. [Example 3: Extraction with background subtraction](#ex3)\n",
    "6. [Example 4: Tapered column extraction](#ex4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ddb4af",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06416a11",
   "metadata": {},
   "source": [
    "- `astropy.io` fits for accessing FITS files\n",
    "- `os` for managing system paths\n",
    "- `matplotlib` for plotting data\n",
    "- `urllib` for downloading data\n",
    "- `tarfile` for unpacking data\n",
    "- `numpy` for basic array manipulation\n",
    "- `jwst` for running JWST pipeline and handling data products\n",
    "- `json` for working with json files\n",
    "- `crds` for working with JWST reference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b96e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CRDS variables first\n",
    "import os\n",
    "\n",
    "os.environ['CRDS_CONTEXT'] = 'jwst_1089.pmap'\n",
    "os.environ['CRDS_PATH'] = os.environ['HOME']+'/crds_cache'\n",
    "os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "print(f'CRDS cache location: {os.environ[\"CRDS_PATH\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21efc012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys,os, pdb\n",
    "# Basic system utilities for interacting with files\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "# Astropy utilities for opening FITS and ASCII files\n",
    "from astropy.io import fits\n",
    "from astropy.io import ascii\n",
    "from astropy.utils.data import download_file\n",
    "from regions import Regions\n",
    "from astropy import units as u\n",
    "\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# Astropy utilities for making plots\n",
    "from astropy.visualization import (LinearStretch, LogStretch, ImageNormalize, ZScaleInterval)\n",
    "\n",
    "# Numpy for doing calculations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for making plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "# Import the base JWST package\n",
    "import jwst\n",
    "\n",
    "# JWST pipelines (encompassing many steps)\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels # JWST datamodels\n",
    "from jwst.associations import asn_from_list as afl # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base # Definition of a Lvl3 association file\n",
    "\n",
    "from stcal import dqflags # Utilities for working with the data quality (DQ) arrays\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Import packages for multiprocessing.  These won't be used on the online demo, but can be\n",
    "# very useful for local data processing unless/until they get integrated natively into\n",
    "# the cube building code.  These need to be imported before anything else.\n",
    "\n",
    "import multiprocessing\n",
    "#multiprocessing.set_start_method('fork')\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "# Set the maximum number of processes to spawn based on available cores\n",
    "usage = 'all' # Either 'none' (single thread), 'quarter', 'half', or 'all' available cores\n",
    "\n",
    "from specutils import Spectrum1D\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from jdaviz import Cubeviz\n",
    "\n",
    "#shutil.copytree('/astro/armin/data/mshahbandeh/aefx/input_dir/', '/astro/armin/data/mshahbandeh/aefx/input_dir_sc/')\n",
    "#shutil.copytree('/astro/armin/data/mshahbandeh/aefx/input_dir/', '/astro/armin/data/mshahbandeh/aefx/input_dir_bkg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37350f4d-4dbc-445d-b743-f26e7898e73e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set parameters to be changed here.\n",
    "# It should not be necessary to edit cells below this in general unless modifying pipeline processing steps.\n",
    "\n",
    "import sys,os, pdb\n",
    "\n",
    "# CRDS context (if overriding)\n",
    "#%env CRDS_CONTEXT jwst_0771.pmap\n",
    "\n",
    "# Point to where the uncalibrated FITS files are from the science observation\n",
    "input_dir = './mastDownload/1232/uncal/'\n",
    "\n",
    "# Point to where you want the output science results to go\n",
    "output_dir = './output/87A/'\n",
    "\n",
    "# Point to where the uncalibrated FITS files are from the background observation\n",
    "# If no background observation, leave this blank\n",
    "input_bgdir = ' '\n",
    "\n",
    "# Point to where the output background observations should go\n",
    "# If no background observation, leave this blank\n",
    "output_bgdir = ' '\n",
    "\n",
    "# Whether or not to run a given pipeline stage\n",
    "# Science and background are processed independently through det1+spec2, and jointly in spec3\n",
    "\n",
    "# Science processing\n",
    "dodet1=True\n",
    "dospec2=True\n",
    "dospec3=True\n",
    "\n",
    "# Background processing\n",
    "dodet1bg=True\n",
    "dospec2bg=True\n",
    "\n",
    "# If there is no background folder, ensure we don't try to process it\n",
    "if (input_bgdir == ''):\n",
    "    dodet1bg=False\n",
    "    dospec2bg=False"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1c5254b-6e38-4b43-82c5-44fa67a4f4b0",
   "metadata": {},
   "source": [
    "## Point to where the uncalibrated FITS files are from the science observation\n",
    "input_dir = '/Users/ofox/data/1860/mast/01860/obsnum03/'\n",
    "#\n",
    "## Point to where you want the output science results to go\n",
    "output_dir = '/Users/ofox/data/1860/output/05ip_3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10e734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Output subdirectories to keep science data products organized\n",
    "## Note that the pipeline might complain about this as it is intended to work with everything in a single\n",
    "## directory, but it nonetheless works fine for the examples given here.\n",
    "det1_dir = os.path.join(output_dir, 'stage1/') # Detector1 pipeline outputs will go here\n",
    "#spec2_dir = os.path.join(output_dir, 'stage2/') # Spec2 pipeline outputs will go here\n",
    "spec2_dir = os.path.join(output_dir, 'stage2/') # Spec2 pipeline outputs will go here\n",
    "spec2_bgdir = ' '\n",
    "#spec3_dir = os.path.join(output_dir, 'stage3/') # Spec3 pipeline outputs will go here\n",
    "spec3_dir = os.path.join(output_dir, 'stage3/') # Spec3 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "if not os.path.exists(det1_dir):\n",
    "    os.makedirs(det1_dir)\n",
    "if not os.path.exists(spec2_dir):\n",
    "    os.makedirs(spec2_dir)\n",
    "if not os.path.exists(spec3_dir):\n",
    "    os.makedirs(spec3_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5341f5-08f7-409b-a764-c3afc160faa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output subdirectories to keep background data products organized\n",
    "det1_bgdir = os.path.join(output_bgdir, 'stage1/') # Detector1 pipeline outputs will go here\n",
    "spec2_bgdir = os.path.join(output_bgdir, 'stage2/') # Spec2 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "if (output_bgdir != ''):\n",
    "    if not os.path.exists(det1_bgdir):\n",
    "        os.makedirs(det1_bgdir)\n",
    "    if not os.path.exists(spec2_bgdir):\n",
    "        os.makedirs(spec2_bgdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a62ae9",
   "metadata": {},
   "source": [
    "# 2. Download all MRS data from SN 1987A PID 1232 (Public)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8798f",
   "metadata": {},
   "source": [
    "#### If you want to run the entire MRS pipeline from start to finish, you will need to download nearly 100 GB of data. The vast majority of these data are the Level0 raw ramp (uncal.fits) and Level1 ramp (rate.fits and rateints.fits) files. For our purposes, we encourage you to simply download the Level2 calibrated data (cal.fits), which totals only 3 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a list of all observations associated with this proposal\n",
    "obs_list = Observations.query_criteria(proposal_id=1232)\n",
    "\n",
    "# We can chooose the columns we want to display in our table\n",
    "disp_col = ['dataproduct_type','instrument_name','calib_level','obs_id',\n",
    "            'target_name','filters','proposal_pi', 'obs_collection']\n",
    "obs_list[disp_col].show_in_notebook()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e378fe92",
   "metadata": {},
   "source": [
    "# Level 0 uncal.fits downloads. Can skip for this workflow.\n",
    "\n",
    "mask = (obs_list['instrument_name'] == 'MIRI/IFU')\n",
    "data_products = Observations.get_product_list(obs_list[mask])\n",
    "\n",
    "filtered_prod = Observations.filter_products(data_products, calib_level=[1], productType=\"SCIENCE\")\n",
    "\n",
    "# Again, we choose columns of interest for convenience\n",
    "disp_col = ['obsID','dataproduct_type','productFilename','size','calib_level']\n",
    "filtered_prod.show_in_notebook(display_length=10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee113760",
   "metadata": {},
   "source": [
    "total = sum(filtered_prod['size'])\n",
    "print('{:.2f} GB'.format(total/10**9))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e6988f7",
   "metadata": {},
   "source": [
    "# Don't forget to login, if accessing non-public data! You can un-comment the line below:\n",
    "# Observations.login()\n",
    "\n",
    "# You can download all of the products by removing the '[:5]' from the line below:\n",
    "manifest = Observations.download_products(filtered_prod)\n",
    "print(manifest['Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c6d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 2b cal.fits\n",
    "\n",
    "mask = (obs_list['instrument_name'] == 'MIRI/IFU')\n",
    "data_products = Observations.get_product_list(obs_list[mask])\n",
    "\n",
    "filtered_prod = Observations.filter_products(data_products, calib_level=[2], productType=\"SCIENCE\", productSubGroupDescription=\"CAL\")\n",
    "\n",
    "# Again, we choose columns of interest for convenience\n",
    "disp_col = ['obsID','dataproduct_type','productFilename','size','calib_level']\n",
    "filtered_prod.show_in_notebook(display_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ecb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(filtered_prod['size'])\n",
    "print('{:.2f} GB'.format(total/10**9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebdd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to login, if accessing non-public data! You can un-comment the line below:\n",
    "# Observations.login()\n",
    "\n",
    "# You can download all of the products by removing the '[:5]' from the line below:\n",
    "manifest = Observations.download_products(filtered_prod)\n",
    "print(manifest['Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the input directory exists. If not, create it. Move all _cal.fits files into that directory.\n",
    "\n",
    "if os.path.exists(input_dir):\n",
    "    print(input_dir+\" already exists\")\n",
    "else:\n",
    "    print(\"Creating Directory \"+input_dir)\n",
    "    os.mkdir(input_dir)\n",
    "    \n",
    "if os.path.exists(\"./output\"):\n",
    "    print(\"./output already exists\")\n",
    "else:\n",
    "    print(\"Creating Directory ./output\")\n",
    "    os.mkdir(\"./output\")\n",
    "    \n",
    "if os.path.exists(output_dir):\n",
    "    print(output_dir+\" already exists\")\n",
    "else:\n",
    "    print(\"Creating Directory \"+output_dir)\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "if os.path.exists(det1_dir):\n",
    "    print(det1_dir+\" already exists\")\n",
    "else:\n",
    "    print(\"Creating Directory \"+det1_dir)\n",
    "    os.mkdir(det1_dir)\n",
    "    \n",
    "if os.path.exists(spec2_dir):\n",
    "    print(spec2_dir+\" already exists\")\n",
    "else:\n",
    "    print(\"Creating Directory \"+spec2_dir)\n",
    "    os.mkdir(spec2_dir)\n",
    "    \n",
    "if os.path.exists(spec3_dir):\n",
    "    print(spec3_dir+\" already exists\")\n",
    "else:\n",
    "    print(\"Creating Directory \"+spec3_dir)\n",
    "    os.mkdir(spec3_dir)\n",
    "    \n",
    "print(\"Moving All Uncal Files To Input Directory\")\n",
    "for file in glob.glob('./mastDownload/JWST/*/*_uncal.fits'):\n",
    "    root = file.split('/')\n",
    "    print(root[-1])\n",
    "    if os.path.isfile(input_dir+'/'+root[-1]):\n",
    "        print('Deleting '+input_dir+'/'+root[-1])\n",
    "        os.remove(input_dir+'/'+root[-1])\n",
    "    print('Moving '+input_dir+'/'+root[-1])\n",
    "    shutil.move(file, input_dir)\n",
    "    \n",
    "print(\"Moving All Cal Files To Input Directory\")\n",
    "for file in glob.glob('./mastDownload/JWST/*/*_cal.fits'):\n",
    "    root = file.split('/')\n",
    "    print(root[-1])\n",
    "    if os.path.isfile(spec2_dir+'/'+root[-1]):\n",
    "        print('Deleting '+spec2_dir+'/'+root[-1])\n",
    "        os.remove(spec2_dir+'/'+root[-1])\n",
    "    print('Moving '+spec2_dir+'/'+root[-1])\n",
    "    shutil.move(file, spec2_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e74e6-52cb-4c56-b150-2945f3a00230",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Detector1 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4591e04",
   "metadata": {},
   "source": [
    "#### Not necessary to run the Detector1 stage of the pipeline for this notebook. But here is sample code in case you do."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f9cbdd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# If you did want to run the entire pipeline, here are the steps.\n",
    "\n",
    "# First we'll define a function that will call the detector1 pipeline with our desired set of parameters\n",
    "# We won't enumerate the individual steps\n",
    "def rundet1(filename, outdir):\n",
    "    print(filename)\n",
    "    det1 = Detector1Pipeline() # Instantiate the pipeline\n",
    "    det1.output_dir = outdir # Specify where the output should go\n",
    "    \n",
    "    # The jump and ramp fitting steps can benefit from multi-core processing, but this is off by default\n",
    "    # Turn them on here if desired by choosing how many cores to use (quarter, half, or all)\n",
    "    det1.jump.maximum_cores='half'\n",
    "    det1.ramp_fit.maximum_cores='half'\n",
    "    \n",
    "    det1.jump.find_showers=True\n",
    "    det1.jump.only_use_ints = False\n",
    "    det1.minimum_sigclip_groups = 75\n",
    "        \n",
    "    det1.save_results = True # Save the final resulting _rate.fits files\n",
    "    det1(filename) # Run the pipeline on an input list of files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f6a0ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now let's look for input files of the form *uncal.fits from the science observation\n",
    "sstring = input_dir + 'jw*mirifu*uncal.fits'\n",
    "lvl1b_files = sorted(glob.glob(sstring))\n",
    "print('Found ' + str(len(lvl1b_files)) + ' science input files to process')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33ecb948",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "if dodet1:\n",
    "    for file in lvl1b_files:\n",
    "        rundet1(file, det1_dir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ea41b-0b77-4a76-9de2-5f237c551226",
   "metadata": {},
   "source": [
    "# 3. Spec2 Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0012b1",
   "metadata": {},
   "source": [
    "#### Not necessary to run the Spec2 stage of the pipeline for this notebook. But here is sample code in case you do."
   ]
  },
  {
   "cell_type": "raw",
   "id": "032bf7cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define a function that will call the spec2 pipeline with our desired set of parameters\n",
    "# We'll list the individual steps just to make it clear what's running\n",
    "def runspec2(filename, outdir, nocubes=False):\n",
    "    spec2 = Spec2Pipeline()\n",
    "    spec2.output_dir = outdir\n",
    "      \n",
    "    spec2.save_results = True\n",
    "    spec2(filename)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14d23946",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Look for uncalibrated science slope files from the Detector1 pipeline\n",
    "sstring = det1_dir + 'jw*mirifu*rate.fits'\n",
    "ratefiles = sorted(glob.glob(sstring))\n",
    "ratefiles=np.array(ratefiles)\n",
    "print('Found ' + str(len(ratefiles)) + ' input files to process')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d6b2d1f",
   "metadata": {},
   "source": [
    "if dospec2:\n",
    "    for file in ratefiles:\n",
    "        runspec2(file, spec2_dir, nocubes=True)\n",
    "else:\n",
    "    print('Skipping Spec2 processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a59fe",
   "metadata": {},
   "source": [
    "# 4. Spec3 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a useful function to write out a Lvl3 association file from an input list\n",
    "# Note that any background exposures have to be of type x1d.\n",
    "def writel3asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "        \n",
    "    # Add background files to the association\n",
    "    nbg=len(bgfiles)\n",
    "    for ii in range(0,nbg):\n",
    "        asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d12e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find and sort all of the input files\n",
    "\n",
    "# Science Files need the cal.fits files\n",
    "sstring = spec2_dir + 'jw*mirifu*cal.fits'\n",
    "calfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# Background Files need the x1d.fits files\n",
    "sstring = spec2_bgdir + 'jw*mirifu*x1d.fits'\n",
    "bgfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "print('Found ' + str(len(calfiles)) + ' science files to process')\n",
    "print('Found ' + str(len(bgfiles)) + ' background files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e0e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make an association file that includes all of the different exposures\n",
    "#asnfile=os.path.join(output_dir, 'spec2_l3asn.json')\n",
    "asnfile='spec2_l3asn.json'\n",
    "dospec3 = 1.\n",
    "if dospec3:\n",
    "    writel3asn(calfiles, bgfiles, asnfile, 'Level3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284cfcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "asnfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965c445",
   "metadata": {},
   "source": [
    "#### Running spec3 in 'multi' output mode to create a single cube with all sub-bands stitched together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50de18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function that will call the spec3 pipeline with our desired set of parameters\n",
    "# This is designed to run on an association file\n",
    "def runspec3(filename):\n",
    "    crds_config = Spec3Pipeline.get_config_from_reference(filename)\n",
    "    spec3 = Spec3Pipeline.from_config_section(crds_config)\n",
    "\n",
    "    spec3.output_dir = spec3_dir\n",
    "    spec3.save_results = True\n",
    "\n",
    "    # Cube building configuration options\n",
    "    spec3.cube_build.output_type = 'multi' # 'band', 'channel', or 'multi' type cube output\n",
    "\n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    spec3.master_background.skip = True\n",
    "    spec3.subtract_background = False\n",
    "    spec3.extract_1d.subtract_background=False\n",
    "    spec3(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96a3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec3 = 1.\n",
    "if dospec3:\n",
    "    runspec3(asnfile)\n",
    "else:\n",
    "    print('Skipping Spec3 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403777ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
