{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592da8da",
   "metadata": {},
   "source": [
    "# Download Wide Field Slittless Spectroscopy (WFSS) Data\n",
    "This notebook uses the python [astroquery.mast Observations](https://astroquery.readthedocs.io/en/latest/mast/mast_obsquery.html) class of the [MAST API](https://mast.stsci.edu/api/v0/) to query specific data products of a specific program. We are looking for NIRISS imaging and WFSS files of the [NGDEEP program](https://www.stsci.edu/jwst/phase2-public/2079.pdf) (ID 2079). The observations are in three [NIRISS filters](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-instrumentation/niriss-pupil-and-filter-wheels): F115W, F150W, and F200W using both GR150R and GR150C [grisms](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-instrumentation/niriss-gr150-grisms). A [WFSS observation sequence](https://jwst-docs.stsci.edu/jwst-near-infrared-imager-and-slitless-spectrograph/niriss-observing-strategies/niriss-wfss-recommended-strategies) typically consists of a direct image followed by a grism observation in the same blocking filter to help identify the sources in the field. In program 2079, the exposure sequence follows the pattern: direct image -> GR150R -> direct image -> GR150C -> direct image.\n",
    "\n",
    "**Use case**: use MAST to download data products.<br>\n",
    "**Data**: JWST/NIRISS images and spectra from program 2079 observation 004.<br>\n",
    "**Tools**: astropy, astroquery, glob, matplotlib, numpy, os, pandas, (yaml)<br>\n",
    "**Cross-instrument**: all<br>\n",
    "\n",
    "**Content**\n",
    "- [Imports](#imports)\n",
    "- [Querying for Observations](#setup)\n",
    "  - [Search with Proposal ID](#propid)\n",
    "  - [Search with Observation ID](#obsid)\n",
    "- [Filter and Download Products](#filter)\n",
    "  - [Filtering Data Before Downloading](#filter_data)\n",
    "  - [Downloading Data](#downloading)\n",
    "- [Inspect Downloaded Data](#inspect)\n",
    "\n",
    "\n",
    "**Author**: Camilla Pacifici (cpacifici@stsci.edu) & Rachel Plesha (rplesha@stsci.edu) & Jo Taylor (jotaylor@stsci.edu)<br>\n",
    "**First Published**: May 2024\n",
    "\n",
    "This notebook was inspired by the [JWebbinar session about MAST](https://github.com/spacetelescope/jwebbinar_prep/blob/main/mast_session/Crowded_Field/Crowded_Field.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff93c82c",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astroquery.mast import Observations\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008fcb1e",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Querying for Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562f354",
   "metadata": {},
   "source": [
    "The observations class in ``astroquery.mast`` is used to download JWST data. Use the metadata function to see the available search options and their descriptions.\n",
    "\n",
    "Note that for JWST, the instrument names have a specific format. More information about that can be found at: https://outerspace.stsci.edu/display/MASTDOCS/JWST+Instrument+Names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3185204",
   "metadata": {},
   "outputs": [],
   "source": [
    "Observations.get_metadata(\"observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447551a",
   "metadata": {},
   "source": [
    "The two most common ways to download specific datasets are by using the [proposal ID](https://www.stsci.edu/jwst/science-execution/program-information) or by using the [observation ID](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47efd71",
   "metadata": {},
   "source": [
    "<a id='propid'></a>\n",
    "#### Search with Proposal ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569efa6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the proposal ID, instrument, and some useful keywords (filters in this case).\n",
    "obs_table = Observations.query_criteria(obs_collection=[\"JWST\"], \n",
    "                                        instrument_name=[\"NIRISS/IMAGE\", \"NIRISS/WFSS\"],\n",
    "                                        provenance_name=[\"CALJWST\"], # Executed observations\n",
    "                                        filters=['F115W', 'F150W', 'F200W'],\n",
    "                                        proposal_id=[2079],\n",
    "                                        )\n",
    "\n",
    "print(len(obs_table), 'files found')\n",
    "# look at what was obtained in this query for a select number of column names of interest\n",
    "obs_table[['obs_collection', 'instrument_name', 'filters', 'target_name', 'obs_id', 's_ra', 's_dec', 't_exptime', 'proposal_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d358734",
   "metadata": {},
   "source": [
    "<a id='obsid'></a>\n",
    "#### Search with Observation ID\n",
    "The observation ID (obs_id) allows for flexibility of searching by the proposal ID and the observation ID because of how the JWST filenames are structured. More information about the JWST file naming conventions can be found at: https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/file_naming.html. For the purposes of this notebook series, we will use only one of the two observations (004) in program 2079.\n",
    "\n",
    "Additionally, there is flexibility using wildcards inside of the search criteria. For example, instead of specifying both \"NIRISS/IMAGE\" and \"NIRISS/WFSS\", we can specify \"NIRISS*\", which picks up both file modes. The wildcard also works within the obs_id, so we do not have to list all of the different IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddce5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a list to download from a specific list of observation IDs instead\n",
    "obs_id_table = Observations.query_criteria(instrument_name=[\"NIRISS*\"],\n",
    "                                           provenance_name=[\"CALJWST\"], # Executed observations\n",
    "                                           obs_id=['jw02079-o004*'], # Searching for PID 2079 observation 004\n",
    "                                           ) \n",
    "\n",
    "# this number will change with JWST pipeline and reference file updates\n",
    "print(len(obs_id_table), 'files found') # ~613 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57c3c0",
   "metadata": {},
   "source": [
    "<a id='filter'></a>\n",
    "## Filter and Download Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daaeaf9",
   "metadata": {},
   "source": [
    "If there are too many files to download, the API will time out. Instead, it is better to divide the observations in batches to download one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a653a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 # 5 files at a time maximizes the download speed.\n",
    "\n",
    "# Let's split up our list of files, ``obs_id_table``, into batches according to our batch size.\n",
    "obs_batches = [obs_id_table[i:i+batch_size] for i in range(0, len(obs_id_table), batch_size)]\n",
    "print(\"How many batches?\", len(obs_batches))\n",
    "\n",
    "single_group = obs_batches[0] # Useful to inspect the files obtained from one group\n",
    "print(\"Inspect the first batch to ensure that it matches expectations of what you want downloaded:\") \n",
    "single_group['obs_collection', 'instrument_name', 'filters', 'target_name', 'obs_id', 's_ra', 's_dec', 't_exptime', 'proposal_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7d144",
   "metadata": {},
   "source": [
    "Select the type of products needed. The various levels are:\n",
    "- uncalibrated files\n",
    "    - productType=[\"SCIENCE\"]\n",
    "    - productSubGroupDescription=['UNCAL']\n",
    "    - calib_level=[1]\n",
    "- rate images\n",
    "    - productType=[\"SCIENCE\"]\n",
    "    - productSubGroupDescription=['RATE']\n",
    "    - calib_level=[2]\n",
    "- level 2 associations for both spectroscopy and imaging\n",
    "    - productType=[\"INFO\"]\n",
    "    - productSubGroupDescription=['ASN']\n",
    "    - calib_level=[2]\n",
    "- level 3 associations for imaging\n",
    "    - productType=[\"INFO\"]\n",
    "    - productSubGroupDescription=['ASN']\n",
    "    - dataproduct_type=[\"image\"]\n",
    "    - calib_level=[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f01e4",
   "metadata": {},
   "source": [
    "<a id='filter_data'></a>\n",
    "#### Filtering Data Before Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of the above information to use for inspection of the filtering function\n",
    "file_dict = {'uncal': {'product_type': 'SCIENCE',\n",
    "                       'productSubGroupDescription': 'UNCAL',\n",
    "                       'calib_level': [1]},\n",
    "             'rate': {'product_type': 'SCIENCE',\n",
    "                      'productSubGroupDescription': 'RATE',\n",
    "                      'calib_level': [2]},\n",
    "             'level2_association': {'product_type': 'INFO',\n",
    "                                    'productSubGroupDescription': 'ASN',\n",
    "                                    'calib_level': [2]},\n",
    "             'level3_association': {'product_type': 'INFO',\n",
    "                                    'productSubGroupDescription': 'ASN',\n",
    "                                    'calib_level': [3]},\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7ca65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the files existing for each of these different levels\n",
    "files_to_download = []\n",
    "for index, batch_exposure in enumerate(single_group):\n",
    "    \n",
    "    print('*'*50)\n",
    "    print(f\"Exposure #{index+1} ({batch_exposure['obs_id']})\")\n",
    "    # pull out the product names from the list to filter\n",
    "    products = Observations.get_product_list(batch_exposure)\n",
    "    \n",
    "    for filetype, query_dict in file_dict.items():\n",
    "        print('File type:', filetype)\n",
    "        filtered_products = Observations.filter_products(products,\n",
    "                                                         productType=query_dict['product_type'],\n",
    "                                                         productSubGroupDescription=query_dict['productSubGroupDescription'],\n",
    "                                                         calib_level=query_dict['calib_level'],\n",
    "                                                         )\n",
    "        print(filtered_products['productFilename'])\n",
    "        files_to_download.extend(filtered_products['productFilename'])\n",
    "        print()\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98933e6",
   "metadata": {},
   "source": [
    "From above, we can see that for each exposure name in the observation list (`obs_id_table`), there are many associated files in the background that need to be downloaded as well. This is why we need to work in batches to download.\n",
    "\n",
    "<a id='downloading'></a>\n",
    "#### Downloading Data\n",
    "To actually download the products, provide ``Observations.download_products()`` with a list of the filtered products. \n",
    "\n",
    "Typically, adjustments aren't needed to the [detector1 pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html), so we can start with the outputs from detector1, the rate files, rather than the uncal files. Because of this, we only need to download the rate and association files. If you need to rerun the detector1 pipeline, `productSubGroupDescription` and `calib_level` will need to be adjusted in the `Observations.filter_products` call to download the uncal files instead.\n",
    "\n",
    "If the data are proprietary, you may also need to set up your API token. **NEVER** commit your token to a public repository. An alternative is to create a separate configuration file (config_file.yaml) that is readable only to you and has a key: 'mast_token' : _API token_\n",
    "\n",
    "To make create a new API token visit to following link: \n",
    "    https://auth.mast.stsci.edu/token?suggested_name=Astroquery&suggested_scope=mast:exclusive_access\n",
    "\n",
    "*Note that a version of astroquery >= 0.4.7 is required to have the call `flat=True` when downloading the data. If you prefer to use an earlier version, remove that line from the call, download the data, and move all of the files in all downloaded subfolders into the single directory as defined by the `download_dir` variable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca7055-81bd-4a94-a766-45bb463bc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the version is above 0.4.7. See above note for more information\n",
    "import astroquery\n",
    "astroquery.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc1897a3",
   "metadata": {},
   "source": [
    "# if needed, create a separate configuration file and replace Observations.download_products() with the following:\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open(config_file) as f:\n",
    "    mast_config = yaml.safe_load(f)\n",
    "        \n",
    "mysession = Observations(mast_config['mast_token'])\n",
    "mysession.download_products(filtered_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c50976-a56a-4cd2-9cd6-11f9266b61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = 'data'\n",
    "\n",
    "# make sure the download directory exists; if not, write a new directory\n",
    "if not os.path.exists(download_dir):\n",
    "    os.mkdir(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ff15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get the products for each batch of observations, and filter down to only the products of interest.\n",
    "for index, batch in enumerate(obs_batches):\n",
    "    \n",
    "    # Progress indicator...\n",
    "    print('\\n'+f'Batch #{index+1} / {len(obs_batches)}')\n",
    "    \n",
    "    # Make a list of the `obsid` identifiers from our Astropy table of observations to get products for.\n",
    "    obsids = batch['obsid']\n",
    "    print('Working with the following obsids:')\n",
    "    for number, obs_text in zip(obsids, batch['obs_id']):\n",
    "        print(f\"{number} : {obs_text}\")\n",
    "    \n",
    "    # Get list of products \n",
    "    products = Observations.get_product_list(obsids)\n",
    "    \n",
    "    # Filter the products to only get only the products of interest\n",
    "    filtered_products = Observations.filter_products(products, \n",
    "                                                     productType=[\"SCIENCE\", \"INFO\"],\n",
    "                                                     productSubGroupDescription=[\"RATE\", \"ASN\"], # Not using \"UNCAL\" here since we can start with the rate files\n",
    "                                                     calib_level=[2, 3] # not using 1 here since not getting the UNCAL files\n",
    "                                                     )\n",
    "    \n",
    "    # Download products for these records.\n",
    "    # The `flat=True` option stores all files in a single directory specified by `download_dir`.\n",
    "    manifest = Observations.download_products(filtered_products,\n",
    "                                              download_dir=download_dir,\n",
    "                                              flat=True, # astroquery v0.4.7 or later only\n",
    "                                              ) \n",
    "    print('Products downloaded:\\n', filtered_products['productFilename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff8769-1240-452d-b84c-eca69d53a13f",
   "metadata": {},
   "source": [
    "If continuing on with the WFSS notebooks, let's double check that we've downloaded all of the files that we need for the remaining notebooks. There should be 149 files downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39310ec6-f58c-4dd6-96ba-b5cda55ee7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_files = glob.glob(os.path.join(download_dir, '*.fits')) + glob.glob(os.path.join(download_dir, '*.json'))\n",
    "print(len(downloaded_files), 'files downloaded to:', download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840d7c4-3643-4152-bd04-bfdcc150cb3e",
   "metadata": {},
   "source": [
    "<a id='inspect'></a>\n",
    "## Inspect Downloaded Data\n",
    "\n",
    "The purpose of this function is to have a better idea of what data are available to you. Additionally, you will be able to use this dataframe to select specific files that match the mode you would like to take a closer look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949795c1-5ab9-45a5-aaf2-ba30593a1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratefile_datadir = 'data/'\n",
    "\n",
    "# first look for all of the rate files you have downloaded\n",
    "rate_files = np.sort(glob.glob(os.path.join(ratefile_datadir, \"*rate.fits\")))\n",
    "\n",
    "for file_num, ratefile in enumerate(rate_files):\n",
    "\n",
    "    rate_hdr = fits.getheader(ratefile) # Primary header for each rate file\n",
    "\n",
    "    # information we want to store that might be useful to us later for evaluating the data\n",
    "    temp_hdr_dict = {\"FILENAME\": ratefile,\n",
    "                     \"TARG_RA\": [rate_hdr[\"TARG_RA\"]],\n",
    "                     \"TARG_DEC\": [rate_hdr[\"TARG_DEC\"]],\n",
    "                     \"FILTER\": [rate_hdr[\"FILTER\"]], # Grism; GR150R/GR150C\n",
    "                     \"PUPIL\": [rate_hdr[\"PUPIL\"]], # Filter used; F090W, F115W, F140M, F150W F158M, F200W\n",
    "                     \"EXPSTART\": [rate_hdr['EXPSTART']], # Exposure start time (MJD)\n",
    "                     \"PATT_NUM\": [rate_hdr[\"PATT_NUM\"]], # Position number within dither pattern for WFSS\n",
    "                     \"NUMDTHPT\": [rate_hdr[\"NUMDTHPT\"]], # Total number of points in entire dither pattern\n",
    "                     \"XOFFSET\": [rate_hdr[\"XOFFSET\"]], # X offset from pattern starting position for NIRISS (arcsec)\n",
    "                     \"YOFFSET\": [rate_hdr[\"YOFFSET\"]], # Y offset from pattern starting position for NIRISS (arcsec)\n",
    "                     }\n",
    "\n",
    "    # Turn the dictionary into a pandas dataframe\n",
    "    if file_num == 0:\n",
    "        # if this is the first file, make an initial dataframe\n",
    "        rate_df = pd.DataFrame(temp_hdr_dict)\n",
    "    else:\n",
    "        # otherwise, append to the dataframe for each file\n",
    "        new_data_df = pd.DataFrame(temp_hdr_dict)\n",
    "\n",
    "        # merge the two dataframes together to create a dataframe with all \n",
    "        rate_df = pd.concat([rate_df, new_data_df], ignore_index=True, axis=0)\n",
    "\n",
    "rate_dfsort = rate_df.sort_values('EXPSTART', ignore_index=False)\n",
    "\n",
    "# Save the dataframe to a file to read in later, if desired\n",
    "outfile = './list_ngdeep_rate.csv'\n",
    "rate_dfsort.to_csv(outfile, sep=',', index=False)\n",
    "print('Saved:', outfile)\n",
    "\n",
    "# Look at the resulting dataframe\n",
    "rate_dfsort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49059b-cc92-448f-aca1-b5af87b0c208",
   "metadata": {},
   "source": [
    "In particular, let's look at the observation sequence these rate files follow. We have sorted the files above by exposure time, so they should already be in time order in the dataframe. \n",
    "\n",
    "`FILTER = CLEAR` indicates a direct image while `FILTER=GR150R` or `FILTER=GR150C` indicates a dispersed image. PUPIL is the blocking filter used. The first 14 exposures make up the first sequence set of direct image -> grism -> direct image -> grism. There are also multiple dither positions for the dispersed images and the direct images. The multiple direct image dithers will be combined in image3, while the multiple dispersed images can be combined in spec3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd23e56-4441-4ad0-ac90-68358802133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_df[['EXPSTART', 'FILTER', 'PUPIL', 'PATT_NUM', 'XOFFSET', 'YOFFSET']].head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38503db6-e37b-4237-b518-6b1a15cefd66",
   "metadata": {},
   "source": [
    "Shown below are the first 14 rate files to give an idea of the above sequence visually. Grid lines are shown as a visual guide for any dithers that are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a285064-b1ee-4ead-8890-4d20f1ba7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot set up\n",
    "fig = plt.figure(figsize=(20, 35))\n",
    "cols = 3\n",
    "rows = int(np.ceil(14 / cols))\n",
    "\n",
    "# loop over the first 14 rate files and plot them\n",
    "for plt_num, rf in enumerate(rate_dfsort[0:14]['FILENAME']):\n",
    "\n",
    "    # determine where the subplot should be\n",
    "    xpos = (plt_num % 40) % cols\n",
    "    ypos = ((plt_num % 40) // cols) # // to make it an int.\n",
    "\n",
    "    # make the subplot\n",
    "    ax = plt.subplot2grid((rows, cols), (ypos, xpos))\n",
    "\n",
    "    # open the data and plot it\n",
    "    with fits.open(rf) as hdu:\n",
    "        data = hdu[1].data\n",
    "        data[np.isnan(data)] = 0 # filling in nan data with 0s to help with the matplotlib color scale.\n",
    "        \n",
    "        ax.imshow(data, vmin=0, vmax=1.5, origin='lower')\n",
    "\n",
    "        # adding in grid lines as a visual aid\n",
    "        for gridline in [500, 1000, 1500]:\n",
    "            ax.axhline(gridline, color='black', alpha=0.5)\n",
    "            ax.axvline(gridline, color='black', alpha=0.5)\n",
    "\n",
    "        ax.set_title(f\"#{plt_num+1}: {hdu[0].header['FILTER']} {hdu[0].header['PUPIL']} Dither{hdu[0].header['PATT_NUM']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e1d71",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
