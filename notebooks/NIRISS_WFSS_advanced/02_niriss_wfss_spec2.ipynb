{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d674dc1",
   "metadata": {},
   "source": [
    "# Run spec2 pipeline\n",
    "\n",
    "The dispersed images for WFSS will be run through the [spec2 pipeline](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html) after the image2 and image3 pipelines have been run on the corresponding direct images. As mentioned in the previous notebook, it is extremely helpful for this step if you have a source catalog that only includes the sources that you would like to look at. Any additional sources will take extra time to calibrate.\n",
    "\n",
    "**Use case**: After creating a custom source catalog, spec2 should be run on the dispersed WFSS images.<br>\n",
    "**Data**: JWST/NIRISS images and spectra from program 2079 obs 004. This should be stored in a single directory `data`, and can be downloaded from the notebook 00_niriss_mast_query_data_setup.ipynb.<br>\n",
    "**Tools**: astropy, crds, glob, jdaviz, json, jwst, matplotlib, numpy, os, pandas, shutil, urllib, zipfile<br>\n",
    "**Cross-instrument**: NIRISS<br>\n",
    "\n",
    "**Content**\n",
    "- [Imports & Data Setup](#imports)\n",
    "- [Custom Spec2 Pipeline Run](#custom_spec2)\n",
    "  - [Spec2 Association Files](#spec2_asn)\n",
    "  - [Run Spec2](#spec2_run)\n",
    "  - [Examine the Outputs of Spec2](#spec2_examine)\n",
    "- [Explore Data Further](#explore)\n",
    "  - [Find a Source](#source)\n",
    "  - [Limit the Number of Extracted Sources](#limit_source)\n",
    "  - [Final Visualization](#final_visualize)\n",
    "\n",
    "**Author**: Rachel Plesha (rplesha@stsci.edu), Camilla Pacifici (cpacifici@stsci.edu), JWebbinar notebooks.<br>\n",
    "**First Published**: May 2024<br>\n",
    "**Last tested**: This notebook was last tested with JWST pipeline version 1.12.5 and the CRDS context jwst_1225.pmap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e541b",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "## Imports & Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab693165-555f-41b3-8135-1155da31393f",
   "metadata": {},
   "source": [
    "[CRDS Documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/user_documentation/reference_files_crds.html#crds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ab7ce-d1dd-4bcc-b405-25c6dd683a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the CRDS path to your local directory\n",
    "%env CRDS_PATH=crds_cache\n",
    "%env CRDS_SERVER_URL=https://jwst-crds.stsci.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import urllib\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "from astropy import constants as const\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "%matplotlib widget\n",
    "# %matplotlib inline\n",
    "\n",
    "from jwst.pipeline import Spec2Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6e8d7-64a0-4a3b-9776-29b181452ff9",
   "metadata": {},
   "source": [
    "Check what version of the JWST pipeline you are using. To see what the latest version of the pipeline is available or install a previous version, check [GitHub](https://github.com/spacetelescope/jwst#software-vs-dms-build-version-map\"). Also verify what [CRDS version](https://jwst-crds.stsci.edu/) you are using. [CRDS documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/user_documentation/reference_files_crds.html) explains how to set a specific context to use in the JWST pipeline. If either of these values are different from the last tested note above there may be differences in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jwst\n",
    "import crds\n",
    "print('JWST Pipeliene Version:', jwst.__version__)\n",
    "print('CRDS Context:', crds.get_context_name('jwst'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ed7e4-f642-41d0-83dd-565c24f78d5c",
   "metadata": {},
   "source": [
    "#### Data Setup\n",
    "\n",
    "The data directory, `data_dir` should contain all of the association and rate files in a single, flat directory. `custom_run_image3` should match the output directory for the custom image3 calibration in the notebook 01_niriss_wfss_image2_image3.ipynb.\n",
    "\n",
    "For spec2, we need the rate files that we downloaded as well as the segmentation maps and source catalogs from image3. Because of that, we will create a new directory (defined by `custom_run_spec2`), change into it, and copy the relevant data over. \n",
    "\n",
    "*For a regular workflow, it is likely easier to leave all files in a single directory compared to the multiple directories we make here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218c8e0-7a25-4688-b904-c4225a66a8ca",
   "metadata": {},
   "source": [
    "Open up our data information file which will make parsing the rate files to copy over easier as we only need the dispersed images, i.e. those observed with GR150R or GR150C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "custom_run_spec2 = 'custom_spec2' # saving files here in this notebook\n",
    "custom_run_image3 = 'custom_image3_calibrated' # results of custom image3 calibration\n",
    "\n",
    "# if the directories dont't exist yet, make it\n",
    "for custom_dir in [custom_run_spec2, custom_run_image3]:\n",
    "    if not os.path.exists(os.path.join(data_dir, custom_dir)):\n",
    "        os.mkdir(os.path.join(data_dir, custom_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e2f5f-8a49-4eea-83f6-850456436635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have not downloaded the data from notebook 00 or have not run notebook 01, run this cell. Otherwise, feel free to skip it.\n",
    "\n",
    "# Download uncalibrated data from Box into the data directory:\n",
    "boxlink = 'https://data.science.stsci.edu/redirect/JWST/jwst-data_analysis_tools/niriss_wfss_advanced/niriss_wfss_advanced_02_input.zip'\n",
    "boxfile = os.path.basename(boxlink)\n",
    "urllib.request.urlretrieve(boxlink, boxfile)\n",
    "\n",
    "zf = zipfile.ZipFile(boxfile, 'r')\n",
    "zf.extractall(path=data_dir)\n",
    "\n",
    "# move the files downloaded from the box file into the top level data directory\n",
    "box_download_dir = os.path.join(data_dir, boxfile.split('.zip')[0])\n",
    "\n",
    "for filename in glob.glob(os.path.join(box_download_dir, '*')):\n",
    "    if '.csv' in filename:\n",
    "        # move to the current directory\n",
    "        os.rename(filename, os.path.basename(filename))\n",
    "    elif '_segm.fits' in filename or '_cat.ecsv' in filename:\n",
    "        # move the image2 products to the appropriate directory\n",
    "        os.rename(filename, os.path.join(data_dir, custom_run_spec2, os.path.basename(filename)))\n",
    "    elif '_i2d.fits' in filename:\n",
    "        # move image3 products to their directory, too\n",
    "        os.rename(filename, os.path.join(data_dir, custom_run_image3, os.path.basename(filename)))\n",
    "    else:\n",
    "        # move to the data directory \n",
    "        os.rename(filename, os.path.join(data_dir, os.path.basename(filename)))\n",
    "        \n",
    "# remove unnecessary files now\n",
    "os.remove(boxfile)\n",
    "os.rmdir(box_download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f8222a-92f1-476a-9dee-85604e8415d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the csv file we created earlier, find a list of all of the grism observations we will want to calibrate with spec2\n",
    "listrate_file = 'list_ngdeep_rate.csv'\n",
    "rate_df = pd.read_csv(listrate_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfd083d-55b5-4610-b11e-276388394e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all of the grism rate files\n",
    "gr150r_files = list(rate_df[rate_df['FILTER'] == 'GR150R']['FILENAME'])\n",
    "gr150c_files = list(rate_df[rate_df['FILTER'] == 'GR150C']['FILENAME'])\n",
    "\n",
    "for grism_rate in gr150r_files + gr150c_files:\n",
    "    if os.path.exists(grism_rate):\n",
    "        shutil.copy(grism_rate, os.path.join(data_dir, custom_run_spec2, os.path.basename(grism_rate)))\n",
    "    else:\n",
    "        print(f'{grism_rate} does not exist. Not able to copy file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03967b5-e6d1-4cd6-a0ca-d429b5fa18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all of the spec2 asn files\n",
    "for asn in glob.glob(os.path.join(data_dir, '*spec2*asn*.json')):\n",
    "    if os.path.exists(asn):\n",
    "        shutil.copy(asn, os.path.join(data_dir, custom_run_spec2, os.path.basename(asn)))\n",
    "    else:\n",
    "        print(f'{asn} does not exist. Not able to copy file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5beb0fe-ff62-453c-95b5-046d6f2aec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all of the necessary image3 output files\n",
    "cats = glob.glob(os.path.join(data_dir, custom_run_image3, '*source*_cat.ecsv')) # copy both the source-match and source118 catalogs\n",
    "segm = glob.glob(os.path.join(data_dir, custom_run_image3, '*_segm.fits'))\n",
    "\n",
    "for image3_file in cats + segm:\n",
    "    if os.path.exists(image3_file):\n",
    "        shutil.copy(image3_file, os.path.join(data_dir, custom_run_spec2, os.path.basename(image3_file)))\n",
    "    else:\n",
    "        print(f'{image3_file} does not exist. Not able to copy file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d9212-7f9a-4f8d-a769-6765bc5bf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # get the current working directory \n",
    "if cwd != data_dir: # if you are not already in the location of the data, change into it\n",
    "    try:\n",
    "        os.chdir(data_dir)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Not able to change into: {data_dir}.\\nRemaining in: {cwd}\")\n",
    "        pass\n",
    "\n",
    "if not os.path.exists(custom_run_spec2):\n",
    "    os.mkdir(custom_run_spec2)\n",
    "\n",
    "os.chdir(custom_run_spec2)\n",
    "\n",
    "new_cwd = os.getcwd()\n",
    "print('Now in:', new_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84915ddb-15a0-491b-ada1-965112ac9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories for calibrating later\n",
    "source_outdir = 'new_catalog_calibrated'\n",
    "if not os.path.exists(source_outdir):\n",
    "    os.mkdir(source_outdir)\n",
    "\n",
    "\n",
    "param_outdir = 'parameter_input_calibrated'\n",
    "if not os.path.exists(param_outdir):\n",
    "    os.mkdir(param_outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a176fd-094d-44e5-b860-5b9f730e0898",
   "metadata": {},
   "source": [
    "<a id=\"custom_spec2\"></a>\n",
    "## Custom Spec2 Pipeline Run\n",
    "\n",
    "Because this is a custom spec2 pipeline run, the first step is to make sure the correct source catalog is being used. This will define the spectral trace cutouts, and therefore inform the pipeline on where to extract the spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8bd1b-1012-47ea-8ce4-fece4c2866a0",
   "metadata": {},
   "source": [
    "<a id=\"spec2_asn\"></a>\n",
    "### Spec2 Association Files\n",
    "\n",
    "As with the imaging part of the pipeline, there are association files for spec2. These are a bit more complex in that they need to have the science (WFSS) data, direct image, source catalog, and segmentation map included as members. For the science data, the rate files are used as inputs, similarly to image2. Also like image2, there should be one asn file for each dispersed image dither position in an observing sequence. In this case, that should match the number of rate files where `FILTER=GR150R` or `FILTER=GR150C`. For this program and observation, there are three dithers per grism, and both GR150R and GR150C are used, totaling six exposures per observing sequence with five observing sequences in the observation using the blocking filters F115W -> F115W -> F150W -> F150W -> F200W."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228f9ea-5e57-41d3-b236-69d7de008423",
   "metadata": {},
   "source": [
    "#### Looking in a Spec2 Association File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6515ea-2b60-4cb1-92ab-3b8dd72a8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec2_asns = np.sort(glob.glob('*spec2*asn*.json'))\n",
    "print(len(spec2_asns), 'Spec2 ASN files')\n",
    "# number of asn files should match the number of dispersed images -- 30 for obs 004\n",
    "print(len(rate_df[(rate_df['FILTER'] == 'GR150R') | (rate_df['FILTER'] == 'GR150C')]), 'Dispersed image rate files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4b9de-b857-4c2b-a58d-a90ef2aadf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at one of the association files\n",
    "asn_data = json.load(open(spec2_asns[0]))\n",
    "for key, data in asn_data.items():\n",
    "    print(f\"{key} : {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5c5a0-0641-4b7d-bf0c-f1ce688c48e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in particular, take a closer look at the product filenames with the association file:\n",
    "for product in asn_data['products']:\n",
    "    for key, value in product.items():\n",
    "        if key == 'members':\n",
    "            print(f\"{key}:\")\n",
    "            for member in value:\n",
    "                print(f\"    {member['expname']} : {member['exptype']}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caee78b-b2e4-432d-9838-836495f390ac",
   "metadata": {},
   "source": [
    "#### Modify the Association File to use Custom Source Catalog\n",
    "\n",
    "What is currently in the association files uses the pipeline source catalog. In the previous notebook, we created a custom source catalog that we want to use with the extension `source-match_cat.ecsv`, so we need to point to that catalog instead in the association files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4074a86-fd34-42e7-b09f-bf6e66c4b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_source_ext = 'source-match_cat.ecsv'\n",
    "\n",
    "# loop through all of the spec2 association files in the current directory\n",
    "for asn in spec2_asns:\n",
    "    asn_data = json.load(open(asn))\n",
    "    for product in asn_data['products']: # for every product, check the members        \n",
    "        for member in product['members']: # there are multiple members per product\n",
    "            if member['exptype'] == 'sourcecat':\n",
    "                cat_in_asn = member['expname']\n",
    "                # check that we haven't already updated the source catalog name\n",
    "                if new_source_ext not in cat_in_asn:\n",
    "                    new_cat = cat_in_asn.replace('cat.ecsv', new_source_ext)\n",
    "                    # actually update the association file member\n",
    "                    if os.path.exists(new_cat):\n",
    "                        member['expname'] = new_cat\n",
    "                    else:\n",
    "                        print(f\"{new_cat} does not exist in the currenty working directory\")\n",
    "\n",
    "    # write out the new association file\n",
    "    with open(asn, 'w', encoding='utf-8') as f:\n",
    "        json.dump(asn_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413eeb50-7e0a-4190-b999-f870b02fdc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check that things still look ok:\n",
    "asn_check = json.load(open(spec2_asns[0]))\n",
    "for product in asn_check['products']:\n",
    "    for key, value in product.items():\n",
    "        if key == 'members':\n",
    "            print(f\"{key}:\")\n",
    "            for member in value:\n",
    "                print(f\"    {member['expname']} : {member['exptype']}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584e673-2907-4483-bb80-28bc16c11911",
   "metadata": {},
   "source": [
    "Alternatively, open the .json file in your favorite text editor and manually edit each of the catalog names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ccaa3",
   "metadata": {},
   "source": [
    "<a id=\"spec2_run\"></a>\n",
    "### Run spec2\n",
    "\n",
    "Like the image pipeline, we can supply custom parameters for steps in the pipeline.\n",
    "\n",
    "More information about everything that is run during the spec2 stage of the pipeline can be found [here](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html).\n",
    "\n",
    "To start, we are only going to run spec2 on one association file because spec2 can take a while to run if there are many sources. We are saving the outputs to the directory we are currently in, which should be the `custom_spec2` directory made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e686d1-51b9-4548-8117-57a8815c9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_asn = spec2_asns[0]\n",
    "print(f'Calibrating: {test_asn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d5b653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if the calibrated file already exists\n",
    "asn_data = json.load(open(test_asn))\n",
    "x1d_file = f\"{asn_data['products'][0]['name']}_x1d.fits\"\n",
    "\n",
    "if os.path.exists(x1d_file):\n",
    "    print(x1d_file, ': x1d file already exists.')\n",
    "else:\n",
    "    spec2 = Spec2Pipeline.call(test_asn, save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc3a2d-e33e-4f4d-88cb-c335e38bdcc8",
   "metadata": {},
   "source": [
    "<a id=\"spec2_examine\"></a>\n",
    "### Examining the Outputs of Spec2\n",
    "\n",
    "The outputs of spec2 are `cal.fits` and `x1d.fits` files. Here we do a quick look into some important parts of these files.\n",
    "- [cal further reading](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html#calibrated-data-cal-and-calints)\n",
    "- [x1d further reading](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html#extracted-1-d-spectroscopic-data-x1d-and-x1dints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fe30e-3737-4886-ae51-28bfe21771e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_example = json.load(open(spec2_asns[0]))\n",
    "rate_file = asn_example['products'][0]['members'][0]['expname']\n",
    "source_cat = asn_example['products'][0]['members'][2]['expname']\n",
    "cal_file = rate_file.replace('rate', 'cal')\n",
    "x1d_file = rate_file.replace('rate', 'x1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b56449-c61d-4330-9698-17818b95f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open all of the files to look at\n",
    "rate_hdu = fits.open(rate_file)\n",
    "cal_hdu = fits.open(cal_file)\n",
    "x1d_hdu = fits.open(x1d_file)\n",
    "cat = Table.read(source_cat)\n",
    "\n",
    "# first look at how many sources we expect from the catalog\n",
    "print(f'There are {len(cat)} sources identified in the current catalog.\\n')\n",
    "\n",
    "# then look at how long the cal and x1d files are for comparison\n",
    "print(f'The x1d file has {len(x1d_hdu)} extensions & the cal file has {len(cal_hdu)} extensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6c961-dfba-4b12-a7ee-cb61623308dd",
   "metadata": {},
   "source": [
    "Note that the 0th and final extension in each file do not contain science data, but the remaining extensions correspond to each source. The `x1d` file contains the extracted spectrum for each source, while the `cal` file contains the 2D cutout information in seven extensions for each source (SCI, DQ, ERR, WAVELENGTH, VAR_POISSON, VAR_RNOISE, VAR_FLAT).\n",
    "\n",
    "This in part is why it is so important to have a refined source catalog. If there are sources that are not useful for your research, there is no reason to create a cutout and extract them.\n",
    "\n",
    "Notice that there are more sources than there are extensions in the files. This is because the pipeline defaults to only extracting the 100 brightest sources. To change this behavior, supply the pipeline with the paramter `wfss_nbright`, which we do [below](#limit_source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae6ca2-b804-4158-824f-0ed859d6a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x1d_hdu.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0773e93-9baa-4d66-b181-a9623446159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_hdu.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52473e1d-c5b4-490d-b2e9-e0d049897c7b",
   "metadata": {},
   "source": [
    "The `x1d` file is a BinTable, so there are additional columns contained inside each of the data extensions. [x1d further reading](https://jwst-pipeline.readthedocs.io/en/latest/jwst/data_products/science_products.html#extracted-1-d-spectroscopic-data-x1d-and-x1dints) goes through what is contained in each column, but we can also take a quick look by looking at one of the data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8d0c6-ea17-475a-ab74-2a40fba25670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick look at the columns also available in the x1d file\n",
    "print(x1d_hdu[1].data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a31a6-fbba-4ca5-920a-ac59014c8495",
   "metadata": {},
   "source": [
    "<a id=\"explore\"></a>\n",
    "## Explore Spec2 Data Further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ce4c1-64c0-4fcb-a274-b1003fade978",
   "metadata": {},
   "source": [
    "<a id=\"source\"></a>\n",
    "### Find a Source in the Spec2 Data\n",
    "\n",
    "Each extension of the cal and x1d files has a source ID in the header. These values should match with the values in the source catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7504a2f-74b9-4f02-a760-79a99a1b80be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_source_ext(x1d_hdu, cal_hdu, source_id, info=True):\n",
    "    # x1d extension first\n",
    "    x1d_source_ids = np.array([x1d_hdu[ext].header['SOURCEID'] for ext in range(len(x1d_hdu))[1:-1]]) # cut off the 0th and last data extensions\n",
    "    wh_x1d = np.where(x1d_source_ids == source_id)[0][0] + 1 # need to add 1 for the primary header\n",
    "    \n",
    "    # look for cal extension, too, but only in the SCI extension; \n",
    "    # fill in with a source ID of -999 for all other extensions to get the right extension value\n",
    "    cal_source_ids = np.array([cal_hdu[ext].header['SOURCEID'] if cal_hdu[ext].header['EXTNAME'] == 'SCI'\n",
    "                               else -999 for ext in range(len(cal_hdu))[1:-1]]) \n",
    "    wh_cal = np.where(cal_source_ids == source_id)[0][0] + 1 # need to add 1 for the primary header\n",
    "\n",
    "    if info:\n",
    "        print(f\"All source IDs in x1d file:\\n{x1d_source_ids}\\n\")\n",
    "        print(f\"Extension {wh_x1d} in {x1d_hdu[0].header['FILENAME']} contains the data for source {source_id} from our catalog\")\n",
    "        print(f\"Extension {wh_cal} in {cal_hdu[0].header['FILENAME']} contains the data for source {source_id} from our catalog\")\n",
    "\n",
    "    return wh_x1d, wh_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df474283-d660-4b5a-a4f8-9e5a6b187c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look for source 118 that we identified in the previous notebook.\n",
    "source_id = 118\n",
    "wh_x1d_118, wh_cal_118 = find_source_ext(x1d_hdu, cal_hdu, source_id)\n",
    "\n",
    "x1d_data_118 = x1d_hdu[wh_x1d_118].data \n",
    "cal_data_118 = cal_hdu[wh_cal_118].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a362fa9f-ae33-4dcc-954c-471a80b41d37",
   "metadata": {},
   "source": [
    "First, let's look at the extraction box as it appears on the rate image. This will help us get a global feel for how much we can trust the pipeline's extraction. \n",
    "\n",
    "*Note: In this example, the extraction box isn't fully cenetered around the spectrum. There is an ongoing calibration effort to better account for difference in the spectral trace shape across the NIRISS detector. Updates about the status of this calibration can be found on the [NIRISS jdox](https://jwst-docs.stsci.edu/jwst-calibration-pipeline-caveats/jwst-wide-field-slitless-spectroscopy-pipeline-caveats#JWSTWideFieldSlitlessSpectroscopyPipelineCaveats-NIRISSWFSS)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc84d02-8b67-440b-9380-53d5a27a5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the nan values from the bad pixels with zero to make it easier to look at\n",
    "rate_data = rate_hdu[1].data\n",
    "rate_data[np.isnan(rate_data)] = 0\n",
    "\n",
    "# extraction box parameters from the header of the cal data:\n",
    "cal_header = cal_hdu[wh_cal_118].header\n",
    "sx_left = cal_header['SLTSTRT1']\n",
    "swidth = cal_header['SLTSIZE1']\n",
    "sx_right = cal_header['SLTSTRT1'] + swidth\n",
    "sy_bottom = cal_header['SLTSTRT2']\n",
    "sheight = cal_header['SLTSIZE2']\n",
    "sy_top = cal_header['SLTSTRT2'] + sheight\n",
    "\n",
    "# plot the rate file and the extraction box\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1.imshow(rate_data, origin='lower', vmin=0.2, vmax=1, aspect='auto') # the scaling may need some adjustment\n",
    "ax2.imshow(rate_data, origin='lower', vmin=0.2, vmax=1, aspect='auto') # the scaling may need some adjustment\n",
    "\n",
    "rectangle = patches.Rectangle((sx_left, sy_bottom), swidth, sheight, edgecolor='darkorange', facecolor=\"None\", linewidth=1)\n",
    "ax1.add_patch(rectangle)\n",
    "ax1.set_title(rate_file)\n",
    "\n",
    "rectangle2 = patches.Rectangle((sx_left, sy_bottom), swidth, sheight, edgecolor='darkorange', facecolor=\"None\", linewidth=2)\n",
    "ax2.add_patch(rectangle2)\n",
    "ax2.set_xlim(sx_left-30, sx_right+30)\n",
    "ax2.set_ylim(sy_bottom-30, sy_top+30)\n",
    "ax2.set_title(f'Source {source_id} Zoom in')\n",
    "\n",
    "plt.suptitle(f\"{cal_hdu[0].header['FILTER']} {cal_hdu[0].header['PUPIL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7c98b-0a1b-4d1f-9323-25eb08696e4d",
   "metadata": {},
   "source": [
    "We can then take a look at the extracted spectrum in this box both in the cal file and the x1d file. In the extracted spectrum below you can see the [OII] and H$\\beta$ emission lines from the galaxy.\n",
    "\n",
    "*Note: The upturned edge effects seen in the 1-D spectrum are due to interpolation at the edges of the extraction box for the current flux calibration. This is also part of an ongoing calibration effort.*\n",
    "\n",
    "*Additional note: The default units of flux from the pipeline are in Jansky. However, in these extracted spectra we show units of erg/s/cm^2/Angstrom. To turn this off, set `convert=False` in `plot_cutout_and_spectrum`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6bfa92-3ba5-4863-b36f-8cd3dbb4d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fnu_to_Flam(wave_micron, flux_jansky):\n",
    "    # convert Jansky flux units to erg/s/cm^2/Angstrom with an input wavelength in microns\n",
    "    f_lambda = 1E-19 * flux_jansky * (const.c.value) / (wave_micron**2) # erg/s/cm^2/Angstom\n",
    "    return f_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f80f3f-b2c9-4496-a1e9-0b8005b49f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cutout_and_spectrum(cal_data, x1d_data, cal_file, x1d_file, source_id, convert=True):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5))\n",
    "    \n",
    "    # plot the cal image\n",
    "    ax1.imshow(cal_data, origin='lower', vmin=0, vmax=np.nanmax(cal_data)*.01, aspect='auto')\n",
    "    ax1.set_title(os.path.basename(cal_file))\n",
    "    \n",
    "    # plot the spectrum\n",
    "    wave = x1d_data['WAVELENGTH'].ravel()\n",
    "    flux = x1d_data['FLUX'].ravel()\n",
    "\n",
    "    if convert:\n",
    "        flux = Fnu_to_Flam(wave, flux)\n",
    "        fluxunit = 'egs/s/cm^2/Angstrom'\n",
    "    else:\n",
    "        fluxunit = 'Jy'\n",
    "\n",
    "    ax2.plot(wave, flux)\n",
    "    ax2.set_title(os.path.basename(x1d_file))\n",
    "\n",
    "    edge_buffer = int(len(flux) * .25)\n",
    "    max_flux = np.nanmax(flux[edge_buffer:edge_buffer*-1])\n",
    "    ax2.set_ylim(0,  max_flux+(max_flux*0.1)) # cutting the flux of the edges & adding 10% buffer to the limits\n",
    "    ax2.set_xlabel('Wavelength (Microns)')\n",
    "    ax2.set_ylabel(f'Flux ({fluxunit})')\n",
    "\n",
    "    if fits.getval(cal_file, 'FILTER') == 'GR150C':\n",
    "        ax1.set_xlabel('X Pixels (<--- dispersion direction)')\n",
    "        ax1.set_ylabel('Y Pixels')\n",
    "    else:\n",
    "        ax1.set_xlabel('X Pixels')\n",
    "        ax1.set_ylabel('Y Pixels (<--- dispersion direction)')        \n",
    "    \n",
    "    plt.suptitle(f\"{fits.getval(cal_file, 'FILTER')} {fits.getval(cal_file, 'PUPIL')} Source {source_id}\", x=0.5, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35719a-8eb7-4ba3-87c8-82cb067da4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cutout_and_spectrum(cal_data_118, x1d_data_118, cal_file, x1d_file, source_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0c7f9-2400-4324-a831-d614bb51c22f",
   "metadata": {},
   "source": [
    "<a id=\"limit_source\"></a>\n",
    "### Limit the Number of Extracted Sources\n",
    "\n",
    "Because it takes so long to extract so many sources, let's see if we can pair down the number of sources being extracted. We'll do this with parameter inputs (for bright point sources) and with a further refined source catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815381d-b50b-404a-9941-e21696e20542",
   "metadata": {},
   "source": [
    "#### Limit Extraction Using Parameter Inputs\n",
    "For this calibration, we are explicitly calling out parameters the following step:\n",
    "- `extract_2d` where we are setting `wfss_nbright` which limits the number of bright sources that are extracted and `wfss_mmag_extract` which sets a limit on the faintest magnitude we want extracted. [Further Reading](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_2d/main.html#nircam-and-niriss-wfss)\n",
    "\n",
    "In this case, we'll limit the extractions to only the 10 brightest objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2d1af-e529-497d-92db-390d17905eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the calibrated file already exists\n",
    "asn_data = json.load(open(spec2_asns[0]))\n",
    "x1d_file = os.path.join(param_outdir, f\"{asn_data['products'][0]['name']}_x1d.fits\")\n",
    "\n",
    "if os.path.exists(x1d_file):\n",
    "    print(x1d_file, ': x1d file already exists.')\n",
    "else:\n",
    "    spec2 = Spec2Pipeline.call(spec2_asns[0],\n",
    "                               steps={'extract_2d': {'wfss_nbright': 10}, },\n",
    "                               save_results=True,\n",
    "                               output_dir=param_outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e9d47-0e79-4005-8aa6-d3eaf1264a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the x1d to examine how many sources were extracted\n",
    "x1ds = glob.glob(os.path.join(param_outdir, '*x1d.fits*'))\n",
    "with fits.open(x1ds[0]) as temp_x1d:\n",
    "    print(f'The x1d file has {len(temp_x1d)-2} extracted sources')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e624bc-ea7b-41e5-a923-8c62374c9b13",
   "metadata": {},
   "source": [
    "#### Limit Extraction Using the Source Catalog\n",
    "In the last notebook we limited the catalog a couple of different ways. Here, let's limit the catalog to a specific magnitude range, and then use that new catalog to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cbc550-6f54-4f2b-b887-617661fabd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_match_cats = np.sort(glob.glob('*source-match_cat.ecsv'))\n",
    "source_match_cat = Table.read(source_cat)\n",
    "\n",
    "# look at the possible magnitude ranges to look at\n",
    "mags = cat['isophotal_vegamag']\n",
    "min_vegmag = mags.min()\n",
    "max_vegmag = mags.max()\n",
    "print(f\"Magnitude range: {min_vegmag} -  {max_vegmag}\")\n",
    "\n",
    "# source 118 should have a Vega mag of ~21.68\n",
    "source_id = 118\n",
    "source_mag = source_match_cat[source_match_cat['label'] == source_id]['isophotal_vegamag'][0]\n",
    "print(f\"Magnitude for source in previous notebook (source {source_id}) : {source_mag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd470d5-f0f7-4f81-adf9-52e05f7dadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the catalog for how many sources are between a specific magnitude (21.18 to make sure to include our example source)\n",
    "min_mag = 21.1\n",
    "max_mag = 21.2\n",
    "mag_cat = source_match_cat[(source_match_cat['isophotal_vegamag'] >= min_mag) & (source_match_cat['isophotal_vegamag'] <= max_mag)]\n",
    "\n",
    "mag_cat['label', 'xcentroid', 'ycentroid', 'sky_centroid', 'is_extended', 'isophotal_abmag', 'isophotal_vegamag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670bd3b-00c7-40cb-906d-b63e44c044d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_source_ext = 'mag-limit_cat.ecsv'\n",
    "\n",
    "new_cat = Table(mag_cat) # turn the row instance into a dataframe again\n",
    "\n",
    "# save the new catalog with a unique name\n",
    "new_cat_name = source_cat.replace('cat.ecsv', mag_source_ext)\n",
    "new_cat.write(new_cat_name, overwrite=True)\n",
    "print('Saved:', new_cat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f2072-b19d-4aab-92d6-2e0570bda9d8",
   "metadata": {},
   "source": [
    "Once we have a source catalog that we've limited to specific sources, let's match the remaining catalogs to those sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc40e15-a182-4440-8d6a-920c0ea8f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_name in source_match_cats:\n",
    "\n",
    "    if cat_name == source_cat:\n",
    "        # the base one has already been saved\n",
    "        continue\n",
    "\n",
    "    # match the source IDs between the current catalog and the base catalog above\n",
    "    cat = Table.read(cat_name)\n",
    "    cat.add_index('label')\n",
    "    new_cat = cat.loc[list(mag_cat['label'])]\n",
    "\n",
    "    # check to ensure the sources are all there\n",
    "    print(repr(new_cat['label', 'xcentroid', 'ycentroid', 'sky_centroid', 'is_extended', 'isophotal_abmag', 'isophotal_vegamag']))\n",
    "    \n",
    "    # save the new catalog with a unique name\n",
    "    new_cat_name = cat_name.replace('cat.ecsv', mag_source_ext)\n",
    "    new_cat.write(new_cat_name, overwrite=True)\n",
    "    print('Saved:', new_cat_name)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570bc05e-f84f-42ab-9067-ea9dd072de75",
   "metadata": {},
   "source": [
    "Once the new catalogs have been made, we have to update the association files to use the new catalogs. **Note: the association files will need to be updated again if you want to calibrate again with the previous source catalogs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9ad72-78d6-4718-8de4-1e5e108db6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_source_ext = mag_source_ext\n",
    "\n",
    "# loop through all of the spec2 association files in the current directory\n",
    "for asn in spec2_asns:\n",
    "    asn_data = json.load(open(asn))\n",
    "    for product in asn_data['products']: # for every product, check the members        \n",
    "        for member in product['members']: # there are multiple members per product\n",
    "            if member['exptype'] == 'sourcecat':\n",
    "                cat_in_asn = member['expname']\n",
    "                # check that we haven't already updated the source catalog name\n",
    "                if new_source_ext not in cat_in_asn:\n",
    "                    new_cat = cat_in_asn.replace('cat.ecsv', new_source_ext)\n",
    "                    # actually update the association file member\n",
    "                    if os.path.exists(new_cat):\n",
    "                        member['expname'] = new_cat\n",
    "                    else:\n",
    "                        print(f\"{new_cat} does not exist in the currenty working directory\")\n",
    "\n",
    "    # write out the new association file\n",
    "    with open(asn, 'w', encoding='utf-8') as f:\n",
    "        json.dump(asn_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e839a1a-5c23-4185-8033-e43d0b367037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# double check that the source catalog has been changed\n",
    "for spec2_asn in spec2_asns:\n",
    "    asn_check = json.load(open(spec2_asn))\n",
    "    for product in asn_check['products']:\n",
    "        for key, value in product.items():\n",
    "            if key == 'members':\n",
    "                for member in value:\n",
    "                    if member['exptype'] == 'sourcecat':\n",
    "                        print(f\"    {member['exptype']}: {member['expname']}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd5d4c-5ef0-4690-a4da-a795c295a313",
   "metadata": {},
   "source": [
    "Now when we calibrate everything, for a single file it should take a lot less time because there are a limited number of sources. However, we will calibrate all of the files in this visit, so this cell might take a bit of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7dad07-7d44-4ebe-841d-3f686b9df016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate with the new source catalog\n",
    "for spec2_asn in spec2_asns:\n",
    "    # check if the calibrated file already exists\n",
    "    asn_data = json.load(open(spec2_asn))\n",
    "    x1d_file = os.path.join(source_outdir, f\"{asn_data['products'][0]['name']}_x1d.fits\")\n",
    "    \n",
    "    if os.path.exists(x1d_file):\n",
    "        print(x1d_file, ': x1d file already exists.')\n",
    "    else:\n",
    "        spec2 = Spec2Pipeline.call(spec2_asn, save_results=True, output_dir=source_outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03794fa-eee5-402a-af65-96378351275d",
   "metadata": {},
   "source": [
    "<a id=\"final_visualize\"></a>\n",
    "### Final Visualization\n",
    "\n",
    "Now that everything has been calibrated, it's useful to look at all of the extracted files. The cal and x1d files from spec2 are extracted at each dither step, which is shown below. Spec3 then turns the individual dither x1d files into a single combined spectrum per grism and filter for each source.\n",
    "\n",
    "Note that for GR150R data, the dispersion direction is in the -Y direction, and for GR150C data, the dispersion direction is in the -X direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7c35f-cf6e-4d30-be38-5b16c80c1487",
   "metadata": {},
   "source": [
    "##### Look at all of the Files for a Single Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aec550-ea1f-4215-8965-bcd2ac408bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the new Data\n",
    "x1ds = np.sort(glob.glob(os.path.join(source_outdir, \"*x1d.fits\")))\n",
    "\n",
    "# get a list of all of the source IDs from the first file to look at for this example\n",
    "sources = [fits.getval(x1ds[0], 'SOURCEID', ext=ext) for ext in range(len(fits.open(x1ds[0])))[1:-1]]\n",
    "source_id = 118\n",
    "\n",
    "# plot each x1d/cal file\n",
    "for i, x1d_file in enumerate(x1ds):\n",
    "    cal_file = x1d_file.replace('x1d.fits', 'cal.fits')\n",
    "    with fits.open(x1d_file) as x1d_hdu, fits.open(cal_file) as cal_hdu:\n",
    "\n",
    "        try:\n",
    "            wh_x1d, wh_cal = find_source_ext(x1d_hdu, cal_hdu, source_id, info=False)\n",
    "        except IndexError:\n",
    "            # this means the source isn't in this observation\n",
    "            continue\n",
    "\n",
    "        x1d_data = x1d_hdu[wh_x1d].data \n",
    "        cal_data = cal_hdu[wh_cal].data\n",
    "\n",
    "    plot_cutout_and_spectrum(cal_data, x1d_data, cal_file, x1d_file, source_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08135225-8662-4b89-8b2d-e422ae087e73",
   "metadata": {},
   "source": [
    "Overplot these files on top of each other to compare. The two grisms will be different line styles to draw attention to any differences that could be due to the calibration, including contamination, and each blocking filter will be a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc37eb-a7c8-4083-82a6-80e329f1fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overplot the different grisms on top of each other for the same source\n",
    "x1ds = np.sort(glob.glob(os.path.join(source_outdir, \"*x1d.fits\")))\n",
    "\n",
    "# get a list of all of the source IDs from the first file to look at for this example\n",
    "sources = [fits.getval(x1ds[0], 'SOURCEID', ext=ext) for ext in range(len(fits.open(x1ds[0])))[1:-1]]\n",
    "source_id = 118\n",
    "\n",
    "# create a figure with three panels\n",
    "src118_fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(11, 9), sharex=True, sharey=True)\n",
    "\n",
    "ls_dict = {'GR150R': '--',\n",
    "           'GR150C': '-',\n",
    "           }\n",
    "\n",
    "color_dict = {'F115W': '#e1cb00',\n",
    "              'F150W': '#32b45c',\n",
    "              'F200W': '#099ab4',\n",
    "              }\n",
    "\n",
    "max_fluxes = []\n",
    "all_waves = []\n",
    "# plot each x1d file\n",
    "for i, x1d_file in enumerate(x1ds):\n",
    "    with fits.open(x1d_file) as x1d_hdu:\n",
    "        try:\n",
    "            wh_x1d, wh_cal = find_source_ext(x1d_hdu, cal_hdu, source_id, info=False)\n",
    "        except IndexError:\n",
    "            # this means the source isn't in this observation\n",
    "            continue\n",
    "\n",
    "        x1d_data = x1d_hdu[wh_x1d].data \n",
    "        grism = x1d_hdu[0].header['FILTER']\n",
    "        filter = x1d_hdu[0].header['PUPIL']\n",
    "\n",
    "    wave = x1d_data['WAVELENGTH'].ravel()\n",
    "    flux = x1d_data['FLUX'].ravel()\n",
    "\n",
    "    flux = Fnu_to_Flam(wave, flux)\n",
    "    fluxunits = 'erg/s/cm^2/Angstrom'\n",
    "\n",
    "    ax1.plot(wave, flux, color=color_dict[filter], ls=ls_dict[grism], alpha=0.7)\n",
    "    if grism == 'GR150C':\n",
    "        ax2.plot(wave, flux, color=color_dict[filter], ls=ls_dict[grism], alpha=0.7)\n",
    "    else:\n",
    "        ax3.plot(wave, flux, color=color_dict[filter], ls=ls_dict[grism], alpha=0.7)\n",
    "\n",
    "    # save the maximum fluxes for plotting, removing any edge effects\n",
    "    edge_buffer = int(len(flux) * .25)\n",
    "    max_fluxes.append(np.nanmax(flux[edge_buffer:edge_buffer*-1]))\n",
    "    all_waves.extend(wave)\n",
    "\n",
    "# plot limits & labels\n",
    "ax1.set_ylim(0, np.max(max_fluxes))\n",
    "ax1.set_xlim(np.min(all_waves), np.max(all_waves))\n",
    "\n",
    "src118_fig.suptitle(f'Source {source_id}')\n",
    "ax1.set_title('GR150R & GR150C')\n",
    "ax2.set_title('GR150C')\n",
    "ax3.set_title('GR150R')\n",
    "\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.set_xlabel('Wavelength (Microns)')\n",
    "    ax.set_ylabel(f'Flux ({fluxunits})')\n",
    "\n",
    "# label for each of the filters\n",
    "for filt, color in color_dict.items():\n",
    "    ax1.plot(0, 0, color=color, label=filt)\n",
    "\n",
    "ax1.legend(bbox_to_anchor=(1, 1.05))\n",
    "src118_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123375a9-4660-4205-9128-4b077bfc06dc",
   "metadata": {},
   "source": [
    "##### Look at all of the sources for a single file\n",
    "\n",
    "Note that some sources might not actually be extracting anything interesting. If this is the case, go back to your source catalog and images to ensure that you have the correct source identified and the target is centered in the cutout. This notebook uses simple examples to create and limit the source catalog, so it might not always show the most scientifically interesting sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b5777-4368-41fa-942f-c3a6f003d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1d_file = os.path.join(source_outdir, 'jw02079004002_11101_00002_nis_x1d.fits') # this is a nice spectrum of 118 above\n",
    "cal_file = x1d_file.replace('x1d.fits', 'cal.fits')\n",
    "with fits.open(x1d_file) as x1d_hdu, fits.open(cal_file) as cal_hdu:\n",
    "\n",
    "    for ext in range(len(x1d_hdu))[1:-1]:\n",
    "\n",
    "        source_id = x1d_hdu[ext].header['SOURCEID']\n",
    "\n",
    "        try:\n",
    "            wh_x1d, wh_cal = find_source_ext(x1d_hdu, cal_hdu, source_id, info=False)\n",
    "        except IndexError:\n",
    "            # this means the source isn't in this observation\n",
    "            continue\n",
    "    \n",
    "        x1d_data = x1d_hdu[wh_x1d].data \n",
    "        cal_data = cal_hdu[wh_cal].data\n",
    "    \n",
    "        plot_cutout_and_spectrum(cal_data, x1d_data, cal_file, x1d_file, source_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0285f-edb3-4796-8377-774a65d02ecb",
   "metadata": {},
   "source": [
    "##### Visualize where the extracted sources are on the dispersed image, and how that compares to the direct image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87810b9-efc5-4554-a2c0-11b5338c7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 5), sharex=True, sharey=True)\n",
    "\n",
    "# **grism data\n",
    "\n",
    "# for the dispersed image plot\n",
    "# x1d_file and cal_file use the same root as we are looking at for a single source\n",
    "rate_file = os.path.basename(x1d_file.replace('x1d.fits', 'rate.fits'))\n",
    "\n",
    "# fill in the nan values from the bad pixels with zero to make it easier to look at\n",
    "with fits.open(rate_file) as rate_hdu:\n",
    "    rate_data = rate_hdu[1].data\n",
    "rate_data[np.isnan(rate_data)] = 0\n",
    "\n",
    "# plot the rate file and the extraction box\n",
    "ax1.imshow(rate_data, origin='lower', vmin=0, vmax=np.nanmax(rate_data)*0.01, aspect='auto')\n",
    "\n",
    "with fits.open(x1d_file) as x1d_hdu, fits.open(cal_file) as cal_hdu:\n",
    "\n",
    "    for ext in range(len(x1d_hdu))[1:-1]:\n",
    "\n",
    "        source_id = x1d_hdu[ext].header['SOURCEID']\n",
    "\n",
    "        try:\n",
    "            wh_x1d, wh_cal = find_source_ext(x1d_hdu, cal_hdu, source_id, info=False)\n",
    "        except IndexError:\n",
    "            # this means the source isn't in this observation\n",
    "            continue\n",
    "    \n",
    "        x1d_data = x1d_hdu[wh_x1d].data \n",
    "        cal_data = cal_hdu[wh_cal].data\n",
    "    \n",
    "        # extraction box parameters from the header of the cal data:\n",
    "        cal_header = cal_hdu[wh_cal].header\n",
    "        sx_left = cal_header['SLTSTRT1']\n",
    "        swidth = cal_header['SLTSIZE1']\n",
    "        sx_right = cal_header['SLTSTRT1'] + swidth\n",
    "        sy_bottom = cal_header['SLTSTRT2']\n",
    "        sheight = cal_header['SLTSIZE2']\n",
    "        sy_top = cal_header['SLTSTRT2'] + sheight\n",
    "        \n",
    "        rectangle = patches.Rectangle((sx_left, sy_bottom), swidth, sheight, edgecolor='darkred', facecolor=\"None\", linewidth=1)\n",
    "        ax1.add_patch(rectangle)\n",
    "        ax1.text(sx_left, sy_top+10, source_id, fontsize=12, color='darkred')\n",
    "        \n",
    "    ax1.set_title(f\"{os.path.basename(x1d_file).split('_nis')[0]}: {cal_hdu[0].header['FILTER']} {cal_hdu[0].header['PUPIL']}\")\n",
    "\n",
    "# **imaging data\n",
    "asn_data = json.load(open(fits.getval(x1d_file, 'ASNTABLE')))\n",
    "i2d_name = asn_data['products'][0]['members'][1]['expname']\n",
    "cat_name = asn_data['products'][0]['members'][2]['expname']\n",
    "with fits.open(os.path.join('../../', data_dir, custom_run_image3, i2d_name)) as i2d:\n",
    "    ax2.imshow(i2d[1].data, origin='lower', aspect='auto', vmin=0, vmax=np.nanmax(i2d[1].data)*0.01)\n",
    "    ax2.set_title(f\"{os.path.basename(i2d_name).split('_i2d')[0]}\")\n",
    "\n",
    "# also plot the associated catalog\n",
    "cat = Table.read(cat_name)\n",
    "extended_sources = cat[cat['is_extended'] == 1] # 1 is True; i.e. is extended\n",
    "point_sources = cat[cat['is_extended'] == 0] # 0 is False; i.e. is a point source\n",
    "\n",
    "for color, sources in zip(['darkred', 'black'], [extended_sources, point_sources]):\n",
    "    # plotting the sources\n",
    "    ax2.scatter(sources['xcentroid'], sources['ycentroid'], s=40, facecolors='None', edgecolors=color, alpha=0.9, lw=2)\n",
    "\n",
    "    # adding source labels \n",
    "    for i, source_num in enumerate(sources['label']):\n",
    "        ax2.annotate(source_num, \n",
    "                     (sources['xcentroid'][i]+0.5, sources['ycentroid'][i]+0.5), \n",
    "                     fontsize=12,\n",
    "                     color=color)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115e0b7-459b-4688-a27e-b8a0afb22da3",
   "metadata": {},
   "source": [
    "Continue to explore further, including using the [spec3 stage](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html) of the pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252a65a-1be8-4e0b-ae67-80cac8fc4770",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
